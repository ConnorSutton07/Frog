Just a reminder that /r/AskScience aims to provide in-depth answers that are accurate, up to date, and on topic.

In particular **anecdotes are not permitted**, especially as a top level comment. This is not the right subreddit to discuss what is your local weather is. So far we have had to remove more than 80% of the comments because most of them are along the lines of "Funny. In Alabama it's 74°f atm", "I am from North Canton, Ohio. I have no idea what you are talking about" or "..You’ve obviously never been to England".
Hmm, lots of incomplete answers here.

Take a look at [this graph](https://www.e-education.psu.edu/earth103/sites/www.e-education.psu.edu.earth103/files/module03/fig8sol.png) of solar intensity vs latitude.  As you can see, the summer pole receives about as much sunlight as the equator (actually, slightly more because of the 24-hour days in the arctic circle!).  On the other hand, there's a huge difference in energy provided to the equator vs the completely dark winter pole.   As a result, there's a huge difference in temperature between the winter pole and the equator, and not so much between the summer pole and equator.

But that just explains changes in temperature vs *latitude*, not changes over time, so let's think about weather systems.  Storms are created by the north-south temperature gradient: as heat moves from equator to pole, a small amount of it is converted into kinetic energy.  Heat flow causes motion, just like a very inefficient steam or gasoline engine.  When the temperature difference is large, the "engine" runs faster and more efficiently, creating more kinetic energy -- which is to say the storm winds blow stronger when the pole-equator temperature difference is large.

So.  In the wintertime, the difference in solar heating creates a much larger equator-pole temperature difference than in the summer.  This creates much stronger wind patterns.  So you've got stronger winds, blowing a bigger temperature gradient around across the surface of the Earth ... so you see much bigger temperature shifts from day to day.

Edited to add some examples.  Here are views of temperatures and winds at the 500 mbar level [in northern hemisphere summer](https://earth.nullschool.net/#2017/08/19/1800Z/wind/isobaric/500hPa/overlay=temp/orthographic=-66.65,42.29,316) (last August 19) and [northern hemisphere winter](https://earth.nullschool.net/#2018/02/19/1800Z/wind/isobaric/500hPa/overlay=temp/orthographic=-66.65,42.29,316) (yesterday).  Colors indicate temperature contrast (purple is colder), streaks indicate wind direction and speed.
In Australia - where there's ocean to the south of us all the way to Antarctica - we can get cold fronts during the middle of summer that can bring freezing conditions at altitude, e.g. [like this event in 2006.](https://www.theage.com.au/news/national/victorias-white-christmas/2006/12/25/1166895228377.html) Like /u/TheRealNooth and /u/seasaltandpepper explained, to the north of the US is a landmass which transfers heat more quickly than water, thus it doesn't have the chance to remain cool in summer or warm in winter like the Southern Ocean does.
[removed]
The summer equivalent to a warm winter day would be a cold night.  Summer in many climates is quite humid.  On a cool night, the temperature will often drop to the dew point or a little below, and dew will drop out of the air.  A phase change from water vapor to liquid water entails removing a larger amount of energy than just changing the temperature, and so the temperature tends to stop dropping fast when it hits the dew point.  

If the temperature had been able to drop lower overnight, the next day wouldn't get nearly as warm and could be like a winter day.

Deserts typically get colder at night than similarly warm humid places.  Part of that is the lack of cloud cover but part of of it also the lack of this dew point clamping effect.
In the U.S. Midwest you have the cold Arctic to the North of you and the warm Tropics to the South of you.
In the higher latitudes (north), you have a winter where it is pretty much always cold. Arctic winters don't have warm winter days like in the Midwest. In the tropics it is pretty much always summer. It is hot year-round.

During a Midwest winter sometimes the air will blow down from the north and be very cold, but sometimes it will blow from the south and be warmer. There might be a 100F difference or more between northern Canada and the equator. Cold northern winter air and warm tropical air meet in the middle and we experience some of that large temperature difference.

During a Midwest summer, the same thing happens except that the arctic is now also experiencing summer. Northern Canada is now warmer so there might only be a 40F difference between northern Canada and the equator. The midwest still experiences some cooler air traveling down from the north, but it isn't nearly as cool, so we don't experience nearly as large a difference in temperature.

The other factor involved is exactly where the cold arctic air and the warm tropical air meet. In the winter, they meet much further south while in the summer they meet farther north. This boundary is basically the polar front jet stream, which you can see from this [diagram](https://climate.ncsu.edu/images/edu/polar_front_summer_winter.jpg) is located further south in the winter. You can view the current jet stream maps [here](http://squall.sfsu.edu/gif/jetstream_init_00.gif). You can compare that to this [map](http://www.intellicast.com/National/Temperature/Current.aspx) which shows generally warmer temperatures south of the jet stream and colder temperatures north of the jet stream. 
This is a really great question, and it would take me several hours that I don't have to put together a complete and understandable response. But I'll put some basic points below:

Firs, it's hard to quantify something that is kind of abstract like the phenomenon you describe, but it does have a basis in truth. [Using Chicago as an example](https://en.wikipedia.org/wiki/Climate_of_Chicago#Data): In January, the average temperature (not the average high or low, the average overall temperature) is 20.8F (-6.2C). The record high in January is 67F (19C) and the record low is -33F (-36C). That's a departure of +46F and -54F between the average temperature and the record highs and lows. In contrast: in July the average temperature is 71.7F (22.1C). The record July high is 111F (44C) and the record low is 40F (4C). That's a similar departure for the record high (+39F) but a much lower departure for the record low (-32F).

So what is the cause of this discrepancy? This is due to many factors, most of which are hard to explain at anything less than a college level, but I will aim for high-school level and see how I do.


**1. Thermal radiation**

The major contributing cause has to do with the different phenomena that cause air to cool down vs warm up. Ignoring small effects, air cools down almost solely due to [thermal radiation](https://en.wikipedia.org/wiki/Thermal_radiation) escaping into space. Thermal radiation is a very important phenomenon that is sadly misunderstood by many (because it is often poorly explained, even by physics teachers!), but in a nutshell, all matter, everywhere in the universe, emits [electromagnetic radiation](https://www.livescience.com/38169-electromagnetism.html), and the amount of energy it releases via this radiation goes up as the temperature goes up, and down as the temperature goes down. The earth and air are no exception, they are both constantly releasing thermal radiation, and without external sources of heat they would be constantly cooling off.

Ignoring effects from clouds and the atmosphere absorbing and re-emitting the thermal radiation (these effects are not negligible, but not important to this explanation) it gets cooler at night because the ground and air are emitting thermal radiation out to space. This is a slow process, that is made even slower by the fact that as it cools down, it emits less energetic thermal radiation, so the rate of cooling slows down as the temperature drops.

This is all in contrast to the way the earth heats up: via light from the sun. Sunlight is (roughly) constant and [very energetic compared to the weak infrared radiation released by the earth and its air](http://faculty.icc.edu/easc111lab/labs/labi/waveradi.jpg). Even a very small amount of sun is enough to offset the cooling by earth's thermal radiation, so in the summer, with long sunny days and short nights, there just isn't much time for the temperatures to drop significantly.

**2. Polar day vs. polar night (summer vs. winter)**

*Note that the following explanation is specific to the [mid-latitudes](http://isen.northwestern.edu/sites/default/files/images/articles/2016/oct/Horton_globe_widgetLARGE2.png), which are areas which are not in the tropics and not near the north or south pole. It is also northern-hemisphere-specific, you need to flip the directions if you're in the southern hemisphere.*

Going along with the above point, it's worth pointing out that, when it's very cold out, the air that you're feeling didn't get that way just because of nighttime cooling: it's almost always because that air came from a much colder region. Cool/cold snaps, both in winter and summer, result from winds blowing from the north. Likewise, heat waves are usually due to air which originated further south, just because areas further south have the sun closer to overhead, and so the [solar heating is stronger](http://shrinkthatfootprint.com/wp-content/uploads/2015/07/irradiance1.jpg).

In the winter, the polar regions are in a constant night, meaning that unless warmer air is pushed up from the south, the air will just continually get colder, and colder. So even though radiational cooling as explained above gets slower and slower as you get colder, this can result in **very** cold temperatures over the course of weeks and months of constant night! 

On the other hand, in the summer, the northern arctic is in constant daylight. The sun is at a low angle for sure, but even this feeble heating influence is enough to counteract most of the cooling due to thermal radiation, and it means that any air pushing south to create a cold snap will start at a much higher temperature, and even if it doesn't warm up at all as it moves south (in reality it will be warming up at least a little) it just won't be the extreme, below-zero cold that is possible in the winter.

**3. Humidity's restriction on cooling air**

The existence of water vapor means that heating-vs-cooling is not an even process: for all intents and purposes, you can heat air indefinitely without hitting any barriers, but if you cool off air that has some water vapor in it, eventually you will reach the dew point: the temperature at which the humidity is 100%. When you cool down to that temperature, water vapor will start to condense into a liquid (or solid), which is an exothermic (heat-releasing) process. This means that any further cooling will be much slower, because of this extra energy that is constantly being released by the condensing water.

Because average temperatures are higher in the summer, average humidities are also higher (outside of desert areas that have very little liquid water available to evaporate). This means that it is very hard to get the right circumstances in the summer to get very cold temperatures.

**tl;dr: Unfortunately I really don't have one. It's a complicated subject with complicated causes. I tried to keep the above as simple as possible, go ahead and give it a try!**

Well, this answer got way longer than I thought it would, and it doesn't even really come close to the full explanation I wish I could type up. But let me know if you have any follow-up questions, and I'll try to clarify things as much as possible!
[removed]
It actually does happen. Ok, the first one that came in my mind wasn’t in summer, it was in May. But same idea. 

“Temperatures have plummeted in the past two days from 28 degrees Celsius (82 degrees Fahrenheit) on Saturday to 0.3 degrees Celsius on Monday.  (.....) However forecasts for the next few days predict temperatures soaring back above 20 degrees Celsius. “ 

https://www.sott.net/article/245456-First-Time-in-50-Years-Snow-Hits-Bosnian-Capital
[removed]
2 basic reasons.


TL:DR
1.  Artic air is colder in the winter than it is in the summer so you don't get real cold swings in summer, Tropical air masses are basically the same temperature in winter and summer, so if it does make it to the Midwest in winter it will still be warm leading to a larger temperature swing.
2.  The Jetstream is stronger and closer to the Midwest in the winter.


1.  The area around the north pole goes through temperature changes.  In the winter it is really cold(about -30F), so when that cold air gets pulled down into the Midwest it is really cold and we get to deal with all the wonderful things that cold weather brings.  In the summer that artic air is much warmer(37-54F).  So even if you get air coming south from the artic in summer it is not as cold to begin with and with much more energy being added along the way by the sun in summer it will not be nearly as cold as it is in the winter when it arrives.  Now for the opposite.  The temperature in the tropics is almost constant.  So no matter what time of year it is the same temperature air will be coming up.  So in winter, that tropical air is the same temperature as it is in summer and we notice a spring like warmup in the Midwest.
2.  The jet stream position is such that it is just south of the Midwest in winter but still close enough that a small shift can bring up warm air.  In the summer the jet stream is much farther North meaning a large shift, much larger than in winter would need to occur to pull artic air down to the Midwest.  Add to this that it is weaker in the summer and stronger in the winter and you are more likely to get large temperature swings in winter than you are in the summer.
[Chinook wind](http://https://en.wikipedia.org/wiki/Chinook_wind) is one cause of the warm air during winter. Essentially, you get warm air coming over a mountain that can displace the cooler air. I can't think of a similar weather phenomenon for summer to winter-like weather. 
I run a site that alerts on watches and warnings issued by Environment Canada and I've stored them for a few years.  They issue a Frost Warning when "during the growing season" ... "widespread frost formation is expected over an extensive area"  (https://www.canada.ca/en/environment-climate-change/services/types-weather-forecasts-use/public/criteria-alerts.html#frost). 

I used frost as an indicator of non-summer like weather "during the growing season" and I looked back at Regina (middle of the Canadian prairies more or less) and here's what I found:



Year | Frost Warnings
----|--------------
2014 | 4
2015 | 6
2016 | 1
2017 | 2

Now that's hardly a definitive list of what may constitute "winter weather" in summer, but it should show that it certainly does happen .
Its because the longwave pattern during the summer is zonal and the long wave pattern in the winter in meridonal. The longwave pattern in the typical of the polar front jet which is the boundary between cold and warm air. It moves north during the summer and becomes zonal (flattened and straighter). In the Winter cold air forces its way south which causes the PFJ more meridonal (which is more like a steep sine wave. Cold air is +N of the jet and Warm air is -N of the jet. So, long story short, summer is more static in temperature due to the PFJ where winter is more erratic due to the variations in the PFJ. I taught meteorology for a few years for the USAF.  
Changes in air mass. High pressure keeps low pressure out and during summer the influx of solar radiation strengthens high press regions so low pressure is restricted to higher latitudes. During winter there is less solar radiation so high pressure systems weaken/retrograde and that allows low pressure to encroach closer toward the equator. 
[removed]
It DOES happen, but it is rare.

But why do we have more warm days in winter than cold days in summer?

Thermodynamics.  In the summer, everything collects and holds heat.  The ground, the air, buildings, roads, cars, all of it.  The sun, even on a cloudy day, is constantly adding heat to every lighted surface.

There are no mechanisms other than convection (heating gas) conduction (heating solids or liquids) or radiation (pure energy via infrared) to move heat.  If there is no point source of low temperature, temperatures cannot fall.

A cold front, or storm can bring in cold air from far away, or high in the air, but other than that, there are few viable 'sources' of cold to provide low temperatures in summer.

But in the winter, there is always a source of heat, at least part of the day.  The sun.  Add some warm air or a storm that formed over warm water, and you can get a warm day in winter.

TLDR: There are few 'sources' of cold (low heat) in summer, but there is always a source of heat in winter.
[Here](https://weather.com/sports-recreation/ski/news/5-extreme-temperature-drops-20130118#/1) is an article on Weather.com that describes multiple such instances. As others have alluded to it’s all to do with air currents and in these particular instances the specific geography around them comes into play.
[removed]
There is thing called the polar front jet and during the summer time but tends to be zonal which is sort of a long flat wave pattern. During the winter time this jet moves south and becomes move wavey and when it becomes wavey it brings with it frontal systems. In the frot of these waved is warm air and behind are cold air. In the summer time weather changed tend to be gradual.
[removed]
Because heat is created by the sun that does exist. But cold is a result of the lack of a heat source. The sun makes its appearance felt when conditions make it possible during the winter months. On the other hand in the summer when the sun is  in its closest position for higher heat.  It ( the heat source) does not disappear or move further from the earth. And when it’s real cloudy or a storm arises it is not enough of a shield to stop the heat sources impact on the environment. 
[removed]
[removed]
[removed]
Hello everyone,

We're getting lots of great follow-up questions from this thread, so I just want to point out that we have an [FAQ section](https://www.reddit.com/r/askscience/wiki/physics#wiki_light_and_special_relativity) related to light speed and Einstein's special relativity. Check it out, if you're interested.

Cheers.
The [time dilation](https://en.wikipedia.org/wiki/Time_dilation) factor is (1 - f^2 )^0.5 , where f is the fraction of light speed.

(This means that the ~~experienced travel time~~ **time that passes inside the ship** approaches 0 as f approaches 1. If you could travel arbitrarily close to light speed, you would experience the journey as effectively instantaneous.)

Here's a calculator for it: http://www.emc2-explained.info/Dilation-Calc/

At .999c (99.9% light speed) it calculates that time is slowed down to 4.47% of the outside value. That means four years are experienced as approximately 65 days.

Edit: fixed the wording. This is literally the time that passes for the ship. It affects everything, from atomic clocks to metabolisms to brain activity.
Nope, time dilation is a cool part of special relativity. At 99.9% the speed of light the trip would be 0.17 ish years to the occupants of the spaceship. The closer you get to 100%, ie. add more 9’s to the end of your percentage, the faster the trip would feel to the occupants. Here’s a cool calculator site you can play with to see these effects. http://www.emc2-explained.info/Dilation-Calc/#.XESiMRZlCEc

-edit This seems to be a frequent question and it’s a good one. How would you age? The traveler would age relative to the time they experience. So on this trip while everyone on earth experienced 4 years, you’d only age a few months! Take a long enough trip and you might just outlive your own grandchildren!


-edit2 Time dilation has been proven experimentally using very precise measurements. Here's a link to an article for some further reading. https://www.scientificamerican.com/article/einsteins-time-dilation-prediction-verified/

Also, as some have pointed out, it would take a long time to accelerate to these speeds at 1g and would require a HUGE amount of energy to do so. Definitely not practical with current propulsion technologies. I have heard that large solar sails might be able to accelerate small probes to high speeds 20-30% c which would be a cool thing to see. 

-edit3 For those curious about the solar sail probe, if you google StarShot you’ll find a lot of info about it. https://breakthroughinitiatives.org/initiative/3

-edit4 As many have pointed out, time dilation is a direct result of the length contraction observed by the traveling body. Also, thanks for for the gifts kind strangers.

-edit5 Another fun link thanks to /u/konstantinua00 talking about relativistic effects on mission duration http://convertalot.com/relativistic_star_ship_calculator.html
Another good fictional example of time dilation can be found in the Ender Quintet (the first five books of the [series](https://en.m.wikipedia.org/wiki/Ender%27s_Game_(series))  that began with Ender's Game). Basically, boy wins war and leaves on a colony ship to meet the descendents of the soldiers he commanded to win the war, with only like two years having passed for him.
Yes, time would pass slower aboard the ship than it would appear to from our perspective.

For a fixed duration 4 year journey at a constant speed (no acceleration or deceleration)

At 99.9% C only 65 days would pass from the perspective of the crew

At 99.9999999999% of C only 3 minutes would pass from the perspective of the crew

A 4ly distance traveling at 100%, 99.9999999999% and 99.9% C from earths perspective would take 4 years, 4 years ~1 day, 4years ~2 days respectively.

Edit: It doesn't *just* "feel" like 65 days / 3 minutes passes. That's how much time actually passes from that frame of reference. Time (and space) is literally dilated. You would only age by that amount, you would only need to eat that amount of food, because that is the amount of time that has elapsed.
Interesting fact I read somewhere, astronauts who have spent any substantial time on the ISS are actually a few seconds? (Can't remember exactly how much) younger than if they had stayed on earth. All relative of course.
Every few months I read something about time dilation then end up going down a rabbit hole until I finally have a very basic understanding of how it works. Then I spend a few days contemplating existence. Then, because I'm not very smart, I forget exactly how it all works, and forget about it at all. Then, a few months later, I read something about time dilation.

Thanks, Reddit. I was looking forward to the next week of my life.
The time dilation factor (Lorentz Factor) can be calculated by the equation: 1 / sqrt(1 - (v/c)^2 ).  Note that v/c is your speed relative to the speed of light (.999 in this case).  For a speed of .999c, we get a Lorentz factor of 22.37
  
This means that for every 22.37 years in the reference frame of Earth, one year occurs in the reference frame of the spaceship.  

This is not a mere illusion or change in how time is experienced, but an actual dilation of space-time.  Relative velocity causes dilation in spacetime, leading to many counter intuitive phenomena.  One of my favourites is that events that are simultaneous in one reference frame are not necessarily simultaneous in another.
Interesting you ask, there is a fantastic novel called "time for the stars" by Robert Heinlein that illustrates the point. The main character is on a "torchship" headed first for alpha centauri and the trip only takes 8 or 9 months for the people on board, whereas to the outside observer on Earth it's been closer to a decade or so. 

Another plot point is the fact that the character is a twin and has a psychic link to his twin back on Earth. Thus allowing them to communicate in real time even when traditional communications would break down at such speeds. His brother grows older and they eventually lose their link because they change too much as people. 
Nope, it'd feel much shorter, by a factor of ~22.4. Their overall trip would feel like 2 months, but the distance they'd appear to travel would also be scrunched in the direction of their travel, by the same factor. 

There's this browser game called [Velocity Raptor](https://testtubegames.com/velocityraptor.html)  where the villian of the game has reduced the speed of light to just 3m/s (about double human walking speed). I highly recommend it, and I think it won an award or two. The physics engine of the game accounts for the pedestrian speed of light accurately (it uses all of the actual equations), and gives you an intuitive feel for weird relativistic effects that you then have to use to progress through the levels.
[removed]
OP, you should read The Forever War. It's a novel set throughout our future, and a lot of it deals with the consequences of such fast space travel. You follow a character who ends up experiencing many eras of humanity in the universe over time as each time they really their destination it's been so many years for humanity that things are drastically different from when and why they set out.
Since speed is relative (we’re traveling x mph around the sun, the sun is traveling y mph around the Milky Way and the Milky Way is traveling z mph etc etc) how do all of those different speeds interact together? Is the whole Milky Way galaxy’s “speed” different to other galaxies? What about a galaxy going the same speed, but in the opposite direction? Isn’t it’s speed 2x as fast relative to is than vs a fixed point? 
Particles moving at the speed of light but having mass always bothered me.

Doesn't this imply that for mass less particles, they do not age?  And since time is cause and events, doesn't this also imply that mass less particles (electrons, photons, etc.) do not interact with our universe because we live in a place of cause and effect?  How does physics take this into account when describing the photoelectric effect?
It would feel shorter due to the effects of special relativity, where if you are traveling close to the speed of light time moves slower for you relative to an external observer.  


According to this calculation: [if you travel 99.9% the speed of light to go 4 light years, it would take 0.17 years in the perspective of the astronaut while the people on Earth would see the spacecraft take 4 years.](http://www.emc2-explained.info/Dilation-Calc/#.XES91S2ZMUs)
I'm legitimately confused by these responses. If AC is 4 light years away, that means it takes four years for light to travel there from here or vice-versa. This means that traveling at 99.9% the speed of light, it would *still take* four years to get there. How would four years only be 65 days for the crew of that ship? Time dilates the faster you go, but does that mean the light emanating from AC only "feels like" 65 days have passed since leaving that part of space?
It would appear to be nearly instant from their point of view. Light traveling at the speed of light arrives at the same time that it left from its perspective. There would be far more time spent accelerating and accelerating at a rate a human could handle than there would be at .999 c. If accelerating at 322 ft/s2 (10 times earth gravity), it would take 35 days (to an outside observer) to accelerate to 980,000,000 ft/s \~c and the same to decelerate.

&#x200B;

Edit: Maths is hard
If you really want your mind blown consider that from the perspective of a photon traveling at the speed light it gets absorbed at it's destination (no matter how distant) the instant it's created. 

As a bonus consider that as you approach the speed light your mass approaches infinite. This is not true of photons because they have no mass, yet some how still have momentum which can be exploited using a light sail.
Yes and no.

The occupants of the spacecraft would have time pass at their normal rate, the clocks on their ship would tick normally and they would age normally.

However:
The time of the objects they are zooming passt would be massively slowed down due to its enormous relative speed and anyone looking into the ship passing them would see time pass incredibly slowly on the ship, again due to its relative speed.

However 2 - relativistic boogaloo:
When travelling at relativistic speeds the space around you contracts if the speed difference is relativistic, thus the trip to alpha centauri would seem like they barely travelled any distance at all and would in fact take a rather short time.

So no they would not be cooped up in there for 4 years.

Special relativity is actually very simple once you wrap your head around the inertial reference frames (who's viewing what) which is the difficult part conceptually.

For people in the ship it would seem that they got to their destination almost instantly because the more you approach the speed of light, the more time approaches a zero value (t=0) which would mean zero time has passed. For people observing from earth it would look like the trip lasted for 4 years.
I finally find a question that I have the answer for! I’m an AstroPhysics Major and we’re just going over Relativity this week. The closer you get to the speed of light, the more the forces of time dilation affect you. So at nearly the entire speed of light, the actual time (aka “proper time”) they would feel on board would be closer to about 0.178* years.

Edit: Math. Changed the number due to comment, changed back because I checked and saw I was right.
So if I got into a space ship and flew at nearly light speed in circles around the earth, it would take only a few minutes for me, but it would be years for those on earth? Can this be used to effectively time travel into the future? I could take off, fly around for 10 minutes and when I land, 100 could have passed?
It gets even better than this:

If you get in your spaceship and accelerate at 1g for 12.5 years in one direction. Turn around and slowdown at 1g for another 12.5 years. 

Then to return "home" you head back at 1g acceleration for 12.5 years, and then slow down for 12.5 years to arrive "home" 

This is all ship time, so for you 50 years have gone by. Also, we need to ignore the expansion of the universe during this time. But when you look around you might find things have changed in the 50 years since you left seeing that 70 Billion (yes billion) years have gone by on earth (or whatever it has become).

From the perspective of the Earth, the trip would take four years, but the crew would seem to have experienced time dilation: their years take longer than one Earth year, so they believe the trip was less than four years.

From the perspective of the ship, however, it seems like the distance between the planets itself shrinks, so of course their trip took less than four years. They would also think that time ran forwards for the people on Earth.
A lot of curious people here on time dilation and spatial contraction. You guys should check out:

[http://gamelab.mit.edu/games/a-slower-speed-of-light/](http://gamelab.mit.edu/games/a-slower-speed-of-light/) , a game by MIT in which you collect orbs that gradually reduce the speed limit of light, so that you experience relativistic effects even at walking or running speed. It even illustrates redshift/blueshift effects! Epilepsy warning though
Imagine for a moment that the experienced passage of time is basically an automatic pendulum experiencing itself swinging back and forth in a liquid. Let's say 1 back and forth is 1 hour.

That liquid is space.

Time dilation happens when you move that pendulum through the liquid at any significant speed. It's still trying to swing but the back swing takes a lot longer now because of the resistance from the liquid. Eventually you reach a point where the pendulum can no longer swing back because it's moving too fast through the liquid. So the pendulum stops experiencing time and becomes suspended in animation until it slows down.

Time still passes normally for everyone else not moving through the liquid, in fact time never dilated at all and has been flowing at a constant rate. It's the "experienced" time that is different.

Similarly, take two pieces of meat. Put one in the freezer and put another outside in the sun. One spoils much faster than the other. Though both pieces of meat goes through the exact same process to become spoiled, they do so at different rates. This is not because of "time dilation", time dilation doesn't exist, except in the sense that we experience the process at different rates, but the process itself remains the same.

It helps to think of the passage of time not as a massive flowing river with everything trapped inside, but rather as just the individual process of change for every individual thing. And that process can be sped up and slowed down.
I've a question. If a spaceship departed for Alpha Centauri under the parameters set forth by the OP, but assume they are coming right back. You could technically be watching their departure after they have already returned, right? So my question is, if you have a Faster-Than-Light method of communication (so the messages can catch up to the vessel), can you communicate with the departing vessel in real time after they have already returned, with the passengers also standing right next to us? In other words, two instances of the same people.

Or do we only exist in each-others present?
If you were to travel at 0.707ish the speed of light, from your point of view it would only take 1 year to travel 1 light-year.  Or 4.3 years to travel the 4.3 light-years to Alpha Centauri.  An observer would record that the journey took longer than that, however.   (The speed 0.707ish, is in fact the square root of 1/2)
So this thought just came into my mind.

 Let's say there was a constant live radio feed that didn't stop playing that was being transmitted to the ship from earth. 

I take it to the occupants on board the ship would hear the radio transmission slow down in speed the faster the ship got?
An interesting way of seeing this is if you travel to Alpha Centauri at light speed without acceleration and deceleration at all, you will feel it only takes a instant for you while it other people on Earth waited for 4 years. 

Does that mean photons never age since they always travel at speed of light?
Does this mean that if I'm the flash and continue to move at speeds of light, that time would slow down for me, but for everyone else time would not change? So then does it mean that in 1 light year people on earth would experience a full year and I would experience much less time? Meaning that people on earth would have more time and even have kids, but I would not even have time to have a kid? That makes it seem like earth is moving faster. I must be missing something.
[removed]
If you had a ship capable of constant 1g acceleration, you could get to the Andromeda galaxy in just 28 years, including deceleration so that when you get there you'll be stationary.

You could do a round trip to Andromeda and back, a 5 million light year trip all up, in a single human lifetime. Of course, 5 million years will have passed on Earth, even though you'll have aged less than 60.
Your question assumes instantaneous acceleration/deceleration. Even at 99.9% of c, assuming you still have to account for mass and momentum, you would start accelerating at the initial point, reach your top speed at around the halfway point, and then start decelerating until you reach the terminus. This would increase the overall time required substantially, many times over. It would also limit the effects of time dilation as you would only be traveling at relativistic speeds (> 99.9% of c) for a small fraction of the journey.
To get to alpha centauri (4.3 LY at 1g) it would take 3.5 years for the passengers, and 5.9 years on earth, with a maximum relative velocity of 95% of the speed of light.

To have a maximum relative velocity of 99.9% the speed of light would require 9.63 g acceleration, and the time on board would be 0.76 years with 4.5 years elapsed on earth. However, for the passengers, it would feel like they died within the first few minutes.

[here's a calculator someone made to solve this problem](http://convertalot.com/relativistic_star_ship_calculator.html)
Thanks for all of the info, guys. I posted this after waking up from a dream in which giraffes were being struck by lightning, of all things.
So this is not my area of expertise and I hope other more qualified people chime in but I have found some informations (some of it even peer reviewed!). 

[This article [1]](https://link.springer.com/article/10.1007/s00484-011-0515-5) talks about how four legged animals are more vulnerable to lightning strikes because ground current has high chances to go through vital organs. It also describes how large animals such as giraffes and elephants have higher chances of deadly strikes. There is the obvious thing about the fact that they are tall so even under a tree there is a non-negligeable chance that lighting could [jump directly to their head](http://i.imgur.com/iYzL85H.png). The other less obvious one is that even if they are not directly hit, the large distance their legs span increased the voltage (and potentially the current) going though them (as described in[ this figure](http://i.imgur.com/JkPWM1K.png)). Sadly the article lacks hard numbers on strike frequency depending on the species. 

This [blog post](http://scienceblogs.com/tetrapodzoology/2009/07/15/mammal-deaths-by-lightning/) seems to suggest that giraffes getting killed by lightning is not that uncommon. 

> Between 1996 and 1999 the Rhino and Lion Reserve near Krugersdorp, South Africa, had two of its three giraffes killed by lightning – the third animal (a juvenile) was also struck but survived. Betsy the giraffe was killed by lightning at Walt Disney World in Florida in 2003 (in front of lots of witnesses).

...

>  A juvenile giraffe at Louisiana’s Global Wildlife Center, named Dusty, was killed after lightning struck a nearby tree. 

[1] Gomes, Chandima. "Lightning safety of animals." International journal of biometeorology 56.6 (2012): 1011-1023.
[deleted]
There was a study done at one of the game parks here in South Africa regarding the susceptibility of giraffes to lightning. It was found that a lot of the deaths attributed to lightning were actually caused by snake bites , Black Mamba's hunt birds in tree tops and the giraffes get fatal bites because they browse in the same  area. I live a few kilometers from the lion park mentioned and it has a very high strike rate because of the terrain layout and elevation. Johannesburg has one of the highest lightning strike rates in the world generally.
Giraffes get struck by lightning more often because they are the tallest standing object in an open area...

People will talk all day about biochemistry and whatnot, but I'm an electrical engineer and I can tell you that they are struck by lightning more solely because they are the shortest path to ground.    
Actually Reindeer heards in Russia get nuked by lighting more often than any other animal on earth.  Due to them grazing or traveling in a tight pack on the open Plains,  a single strike has killed over 1500 reindeer in Russia, with a similar event in Norway I believe it was killed around 1000.
[removed]
[removed]
Considering taller objects always have a higher risk of conducting lightning strikes i would say yes although i laughed thinking who is out there compiling data on the amount of lightning strikes on giraffes.
Yup theres a fried one, sixth this month!
do those tufted horns on the top of their head look like tesla thingys to anyone but me?

i've always thought they looked odd... now, maybe i know why?

lighting as an evolutionary pressure on the savanna is a fascinating subject.

how might it have affected human precursors?

is walking UPRIGHT a way to shunt current from vital organs?

anyone know the anatomy of lightning strikes thru animal tissue enough to know the answer?
I didn't see anyone mention this, so I thought I'd bring it up and see if anyone has more information or thoughts. How much does the height of a giraffe really affect the likelihood of being struck by lightning? If lightning starts somewhere between 6500ft and 20000ft ([making an assumption here about where thunderstorms and lightning actually start](http://www.nationalgeographic.com/science/earth/earths-atmosphere/clouds/)), and a giraffe and zebra are 18ft and 4ft respectively, does that 14ft difference really matter?
What you need to take into effect, is that giraffe populations are not in areas that get a lot of lightning.  

Also, the animals behavior plays a role. Giraffes may actually seek shelter in forested areas during a thunderstorm, lessening their risk for a strike. 

I would theorize that either birds (shockwaves from strikes) or tree dwelling animals would actually be the highest risk of lightning fatalities. 
This doesn't directly answer the question but it's related. In my physics class we talked about lightning rods; before electricity was well understood people tried to guide lightning away from buildings with roof-mounted conductive orbs. The orbs actually had higher capacitance with the clouds than the ground does, meaning that the orbs would simply cause larger arcs of lightning to hit them/the building than if there was no orb at all. A lightning rod has a very low capacitance with the sky and can also slowly dissipate charge from the clouds, resulting in less powerful/damaging lightning in the area near the rod. So while giraffes are tall, the shape of their heads and bodies might also impact their odds of getting hit in the same manner shape impacts lightning rods/orbs (although this may be wrong since the giraffe is a poor conductor compared to something like a lightning rod, and this may reduce or prevent charge dissipation).
[Here's a BBC article](http://www.bbc.com/news/magazine-11734228) about Giraffes getting struck. 

I personally have heard of a giraffe being struck in a South African game park that we visit regularly, as well as a giraffe breaking it's neck after running into a fence. Unwieldy animals, those giraffe.

So.. this post has me picturing a giraffe wearing something like a little pointy WWI Kaiser helmet and a wire that travels down along his body.. or implanted.. and comes out and affixes to a contact at the bottom of his hoof.. The Franklinberg Giraffe Lightning Abatement System by Kaiser Wilhelm Inc.. 
In a theoretical example where animals are standing in close proximity to where lightning will strike anyways you could expect a Giraffe to get hit a slightly higher percentage of times than a smaller or shorter animal, but I think something like a hippo who has more surface area touching the ground has a higher chance than a Giraffe.

In the end no amount of height or girth is enough to affect the path of lightning as it's a connection between the upper atmosphere brimming with energy and the earth sinking it:  It's such a massive scale even our most conductive man-made materials have a hard time changing it's path more than a short distance.
You also have to consider the frequency of lightning in certain places. Funnily enough the highest frequency of lightning strikes happen around central Africa so it would stand to reason that animals there would also get struck by lightning more often.
[removed]
[removed]
Your question made me curious and a quick search yielded the study linked below, which looked at exactly this question.^1 The researchers found that the answer depends both on the variant of the exercise as well as the stage of the exercise. For example, in a traditional push-up the number is about 69% in the up position (at the top of the movement) and 75% in the down position (bottom of the movement).

It's also worth mentioning that the study also looked at a "modified push-up." This modification [as shown here](https://i.imgur.com/2PagQIv.png) is essentially just an ~~lazier~~ easier version of the exercise where the knees stay on the floor.  Surprisingly (to me at least), even in this simpler version you still lift quite a bit of your body mass (54% in the up position and 62% in the down position).

edit: I corrected "going up/down" to "up/down position" to reflect the fact the body was kept stationary when the force was recorded in this study.

^(**1**) Suprak, et al. **The effect of position on the percentage of body mass supported during traditional and modified push-up variants.** 2011: 25 (2) pp 497-503 *J. Strength Cond. Res.* [Link](https://www.ncbi.nlm.nih.gov/pubmed/20179649)
To measure yourself: Put a bathroom scale under one of your hands while doing a push up. Double the maximum value the scale lists and divide that by your total weight (and multiple by 100) to calculate the percentage.
So I wonder if it works in reverse? If you want to improve push ups by bench pressing. Say you are 250 lb at 75% that's 187.5 lb. Could you then work on a set with say 190 lb over period Of time and increase your stamina for push ups push up effectively
[removed]
[removed]
I’m curious about this too. What about the different ways of doing push-ups and push-ups that focus on other muscles. Or a wide spread push-up compared to one in line with your shoulders. Would it be the same or would the weight change. 
Just for fun to check the people giving you numbers, you can put your feet on a scale (easier since they're less wide apart) and ask a buddy to read off the number at the top and the bottom. Hold still at the top, get the number, then hold still at the bottom and get the number. The difference between your weight and the weight on your feet is the weight on your hands. 
[removed]
Correct me if I'm wrong but you could do a pushup on a set of scales and get an accurate (to within the tolerances of the scales) measure of it surely?

Then just see what percentage that is of your total body weight.

I know it's not all sciencey but sometimes simple solutions are fun.
[removed]
You can step on a scale and see how much you weigh. Then, you can get into a pushup position and put your hands on the scale. You get the exact number how many kgs you're lifting. Divide those kgs with your weight kgs (also, multiply by 100) and you get the exact percentage.
[removed]
moments / torque

bodyweight(lbs) × centre of gravity(ft) = push-weight(lbs) × arms-from-feet distance(ft).

centre of gravity should be at your bottom rib or just above your belly button. (heimlich maneuver spot).

eg: 6ft tall and 180lbs  
180lbs × 3.75ft (CoG) = X × 5(ft)  
X = 135lbs ≈ 75%
As you have your answer which will depend a bit on your body design but is easy to find with your hands on a scale, I'll add there's some use with that number (actual number, not percentage). You can make it higher by elevating your feet or lower it by using books or chair. However, don't make the angle too extreme as you change the exercise.

Other methods make the exercise more difficult or easy by changing the balance and leverage. Hands closer together is harder than hands further apart (also shifts whether you use the chest or triceps more). When you shift width of your feet, only the balance is affected. Also, there a range of motion to consider. Usually, your max range is from "chest to ground" to "arms locked out". This ground part can be deepened using handles or books. Anyway, making the range of motion shorter (going only parallel, bit of bend of elbows) makes things easier as your going less distance.

I'll also add that while it's tempting to just get higher and higher reps, consider having a cap where after that you start adding weight (placed on your back, weight vest, straps, etc) after you've reached a specific goal of reps/time. For example, if you're doing a "Tabata" method that's 15 seconds of reps/ 15 seconds of rest for 10 rounds (2.5 minutes rest, 2.5 minutes work) and full range of motion and you're doing this every other day. If you can reach 120 reps total, add five pounds total to your back for next exercise. When you reach 120 after so many days, again, add 5 pounds. If you ever get up to 25 pounds, for fun try it with 0 and be amazed at how fast you knock out reps.
Based on male data. Going by an average of center of mass to height ratio of 0.560 according to [this website](https://hypertextbook.com/facts/2006/centerofmass.shtml) and an average height of the male in america being 1.77m [according to google](https://www.google.be/search?client=firefox-b&dcr=0&ei=K3jyWc_tA8S7aq3jutAG&q=average+male+height&oq=average+male+height&gs_l=psy-ab.3..0i67k1j0l9.3651.4137.0.4219.6.6.0.0.0.0.90.335.5.5.0....0...1.1.64.psy-ab..1.5.334...0i20i263k1.0.LdbwVi-Kx9o) and an average arm lenght for the average height of 0.889m according to [this source](http://www.tdj.eg.net/articles/2015/43/1/images/TantaMedJ_2015_43_1_1_154557_t20.jpg) and going by 30 cm shoulder to top of head

And by assuming the starting position is a straight edge triangle. 

∑M_feet = 0.989 * m * cos(30.15°) - 1.47 * F * cos(30.15°) = 0

yields: F = 0.672 * bodymass

this is the force perpendicular to the body, the force on the arms is:

 F = 0.56/cos(30.15°) * bodymass = 0.78 * bodymass


[deleted]
[removed]
If you create a force diagram, depends on your center of mass vs. where your arms push against the ground as a ratio of distance to your feet.  For instance, if your arms are twice the distance from your feet compared to your center of mass, you will be pushing 50% of your weight.

To get your percentage, weigh yourself full weight on the scale, then put your feet on the scale in pushup position at the top and bottom of your pushup and measure the scale weight.  The difference is what you are pushing.
[removed]
This is one I can actually answer!

I was curious as well about that, so to find out I used a typical bathroom scale. 

Place both of your hands on the scale while in a push-up position, while all the way up for stability. 

Then divide that number by your weight on the scale to get your percentage.  
While standing flat-footed:

Measure height to shoulder (Hs).
Measure height to belly button (Hbb)

Push up force = Body Weight x (Hbb/Hs) +/- 10%*

*This is sum of moments around your toes and assumes your CG is around your belly button (it is for most people).
I'd be interested to see also how the weight distribution in your body affects this percentage. Along with this how this would affect the amount of work done such as someone who carries most of their weight in the upper part of their body as opposed to their core area .
Can I chime in with a question? What is the influence of different body proportions on pushups? Longer arms, longer abdomen? More men seem to have relatively shorter arms and legs while women tend to have longer arms and legs I think (based on discussions on a cycle forum about the perfect bike fit). 
F=ma  75kg man x g(9.8) = 735Newtons
735sin(32degrees)=  390 Newton’s  
390/735 = 52% of you body weight 

No clue if I did this correctly. But seem right because you can rep out way more push ups than bench pressing 52% of your body weight because you are using your muscles in your body more advantageously in push up position. Ie: you are able to flex your abdominals harder in a push-up position harder than a bench press position and flex your glutes tighter in push up position. More muscles involved means more reps for the push up.  
[removed]
[removed]
You've got some good answers here already, but they're all leaving out an important aspect, which is how the screw and screwdriver deal with fouling. Dirt, oil, weld slag, multiple layers of paint, whatever. If you're in an environment where you don't have to worry about that, a complex geometry is fine. But on a factory floor, Phillips or torx can get irreversibly fouled. Allen head screws can be relatively easily cleaned, but the master of this is the shittiest of all screw heads, the flat head. The *only* tool you need to clear the slot of a flat head screw is the screwdriver you're going to use to unscrew it. No other screw type has that ability.
The reason for the different styles is cost and torque. The slotted head screws are cheap and easy to make. But they're completely useless for powered screwdrivers and you can't put much torque on the screw without it either slipping out or stripping the head (and maring the surface of whatever you're screwing). Phillips screws are self-centering, making powered screwdrivers possible. They're somewhat more expensive to produce than slotted-head. They tend to 'cam-out' easily under torque, making it hard to apply much torque. I've heard they were designed that way to prevent overtightning. However, it's not good for exposed fasteners to look stripped. Robertson-head and allen-head fasteners can handle more torque than phillips-head fasteners, but are more expensive. Because the bottom of the hole is flat (unlike the pointed end of the phillips), there's more contact area and so it's less likely to cam-out. The robertson-head is cheaper than the allen-head, but the allen-head has six points of contact rather than 4, making it less prone to rounding out the hole. The Torx-head fasteners solve the problem of rounding/stripping by having the flat bottom of the robertson/allen that reduces cam-out, but it has much better contact with the driving bit to prevent stripping the head. The points of the 'star' on the driving bit engage the recesses on the screw at nearly right angles, so it has a very positive contact. Torx is becoming more and more popular because of that, particularly in assembly-line work. Because they're less likely than a phillips to be damaged when tightening, the allen (internal hex) heads are often used for exposed ('decorative') fasteners on 'some assembly required' furniture. It's also very cheap to make the allen keys, so they usually include one with the fasteners.

I think the core of your question has been answered, but I wanted to add one point.  Sometimes various screws are used for security reasons.  For instance on the products my company sells there are some screws we intend for the customer to undo, and we will use hex, slotted, or Phillips for these.  Other screws we don't want them messing with and may use Torx or something more rare.

In fact some companies specialize in making strange screw head shapes purely for security reasons, [as seen in this chart.](http://cdn.clm02.com/ezvivi.com/204050/6359362590222142198612765.jpg)
One thing not mentioned is that some screws (e.g. Robertson) are better for automatic machine use than others. 

For machines, it's best if you can load the screw onto the driver, then move the driver into position. But you need a screw that will hold onto the bit well enough to not fall off.

Others have mentioned Phillips bits and cam-out. This is desirable. For example: Drywall installers use special screw guns (and you can get drywall bits for regular drills) that will detatch from the screw at the right install depth. Handy for that application.

By the way, Robertson screws are really friggin awesome. They are just so great to work with in most applications. It's a global tragedy that they never became more widespread.
To add, universal standards are a hard thing to decide upon. Just look at how imperial units remain, despite the existence of SI units. People will have differing opinions on which type is the best, especially as the different head types have different mechanical and operating properties.  
  
XKCD has relevant comic (as always)  
[Standards](https://xkcd.com/927/)
my 2 cts to this discussion..Note that Pozidriv screws are often mixed-up with Philips. It is definitively worth having one #2 pozidriv driver as well as a philips one and paying attention which is which. Ideally all sizes are better.

http://www.finehomebuilding.com/2015/09/16/what-is-the-difference-screw-bits-phillips-vs-pozidriv
I have learned so much about screws in this post. 

I feel like this is information that I need to store in my brain for when I'm a dad who has to fix everything around the house and answer my kids questions. Also, so I look manlier when I'm standing around the grill with the other dads at a cookout our wives organized and we start talking about manly dad stuff and I lay down some serious screw knowledge like they're ESPN stats.

Thanks everyone!
Being a Canadian most of the screws we use are Robertson, in 40 years I have only stripped two and both were really rusted 

I have had the same hand screw driver for 40 years and still going strong

The nice thing about Robertson is there are really only two sizes and one is by far the most popular 

When you have a Philips screw strip you can use a Robertson to get the job done 


When I was a volunteer at the Buffalo Museum of Science, I worked in the carpentry shop helping to build displays as well as building repairs. The guy who ran the shop was a 40 year tradesman and the best finishing carpenter I have ever met.
When he came on board the first thing he did was replace all our variety of screws for Robertsons. He claimed they were the best screws in the world. I now use nothing else.
There is a book called One Good Turn, about the invention and development of screws, screwdrivers and their various types. It's only sort and is quite an interesting read.

https://www.amazon.co.uk/One-Good-Turn-Natural-Screwdriver/dp/0684867303
[removed]
A combination of:
patent issues
purpose of use

Security screws are obvious -they are simply made so that common screwdrivers people would have laying around won't work, to make it clear that it's not user serviceable - this is sometimes a legal requirement, not just the manufacturer's wish (as it can be argued that , hey, if I wasn't supposed to open it, why did my screwdriver fit?)
Another curious case we are stuck with is the Phillips. You may have noticed your screwdriver often pops out under torque - this was originally by design. The driver cams out when too much torque is applied - this was a safety measure to prevent over-torquing the screws.
Every design has a story. There's gotta be a book.

[removed]
To add to other answer(s): Macs/iPhones use a ton of different screw types, with a new type seemingly being added in each new revision.

They do this not for any technical or logistics reason, but because they want you to take it into the Apple Store for repairs/upgrades, rather than doing it yourself.
[removed]
Lord, you could say that for computer ports, and measurements, and...everything.

People evolve standards at different points, and they tend to like the one they got used to. It's hard to get the world to standardize
I used to think that phillips head screws sucked, then i learned about using quality bits and fasteners. Combine that with learning how to use a drill/impact driver well and phillips fasteners actually work extremely well. Biggest problem is using magnetic bit holders. The phillips head actually grips onto the screw so sell that when you pull out, the bit will pop out and stay on the fastener. Have to buy ones with the retention ring. 

Robertson and torx are great for torque, but they wobble too damn much on the end of the drill/impact driver! I much prefer them for use with hand screwdrivers/ratchets, but hate them for driving new fasteners.
Square drive or Robertson aka decking screws are used a lot now in the states. You can probably attribute it to the trex boards as the screws for them are Robertson. That's probably where a lot of people really were exposed to them. Having to run hundreds of screws will do that. On a side note the bits that come in the bucket are cheapo and suck. Philips are decent and work. They usually strip because people use the wrong size bit. Most common is a #2 think drywall screw.  Or the bit is best up not making good contact or the head is best. If your running new screws with a fresh bit and apply force straight into the head it's a breeze.
A story that was told to me years ago and I didn't verify but thought was interestong regardless. I was told it was because of patents. Early on before patents expired they figured it would be cheaper to manufacturer their own screws with a different design. If I had to guess now, it's just money for having multiple heads. In construction I've never used a flathead in my life except for finishing. It's just too difficult and risky to use a flathead quickly and efficiently. 
Most people in this thread appear to be debating the cute little heads.  Nobody seems to be addressing the big bad boy screws used in critical fastener applications (also called bolts sometimes, which is wrong, but I won't get it I that.. looking at you Europeans!).  I work for one of the largest industrial conglomerates in the world.  We are currently trying to standardize on fastener types to reduce costs.

I'm  investigating the possibility of using one head type for all large critical fastener applications.  I'm leaning strongly towards an external 12 point flange head (bi-hexagonal, IFI 115, or similar), and potentially a hollow head variation of the 12 point flange head.  It has the most advantages bar none. The only disadvantage we've found is the relative unpopularity (for sourcing and services tooling) compared to external  and internal hex.  But when you are a big boy like us, we can make the river flow the way we want and corral the insanity of dozens of head types, when one would be just fine.

On the small side of fasteners, we're seeing a lot of traction in the internal hexalobular (aka torx), Robertson and pozidriv.  Phillips and slotted are going to disappear, and I really wish internal hex would to. But the internal hex is unfortunately still the most commonly used. It's a strong head type, but it's not the best drive type for mechanics working in tight places with small fasteners.

We don't have high speed manufacturing lines like the automotive guys.  That industry will tend to pick the right balance  between cost, speed and reliability.  So keep an eye on them if you want to know where the world is going.
One point i cant see covered is also a security problem if we only had 1 type of screw then that would be used in prisons. Where once you manafactured a tool which could open 1 screw you could open them all. Normally in prisons up to 15 different screw heads are used they also get there own types of screw heads made so there is little chance of having a suitable head available for purchase. 
[removed]
Well, the socket head screw is vastly superior to every other type, but it was invented by a Canadian who refused to sell his patent rights to Henry Ford. Out of spite, Ford came up with other designs that were all junk for the Model T. 
I been asking that for years. The worst is when using/installing a product and there are two different types to deal with.
I think a lot also has to do with industries being used to making/using certain types of screws that they see no​ reason to change anything because of costs ie, retooling.
[removed]
Just like different types of bolt heads, it's all in the application they were meant for. Besides for your standard 3 which were made for things like keeping the heads clear or easy cleaning for removal long way down the line. If your curious the stand 3 our phillips, flathead, and Robertson's. The latter of which I use mainly since it's harder to strip, like a torque head, and common enough that almost everyone has a driver. I don't know where everyone here in the US lives that keep saying their special order in the states but I live in IN US and they're more common here than Phillips. I think torque are even more common here now.
As the name phillips suggest, it was introduced by a company and was not a industrial standard, and since everyone wants to stand out it's sadly very hard for companies to just agree on a standard. 

The biggst problem with philips is the HUGE quantity of bits, changing in depth, thickness and "pointiness" of the tip. We were constructing a wooden shed in the backyard for a client with 100s of philips screws, everyone with the exact same head. We had also the exact correct bit and therefore never stripped a single screw. 

That's why torx is so appealing, there's not a lot you can change (exept the diameter) and therefor not a lot the user can do wrong.

TL;DR Companies want to be unique so they flooded the market
Made a deck with over 1000 hex star screws. Used (good brand) 6 mm hex cat 4 screws. Predrilled all holes same diameter as shank/shaft. Never one screw went wrong. The (good brand) drill included hex point however lasted about 600 screws into the job. 

I dont think hex screws would have gone it so far.

My point is i'll good quality lasts. However for a manufacturer the least costly is either good or bad for cost or reputation. 

Avoid all "cost saving qualities" or no brand screws (and drills, drill bits). You might get lucky but less likely than to get a bad good brand tool.

However i hate it when they make bathroom light (which need replacement) fixtures with 3mm! star screw (when a philips screw would have done the job. That's what you get with fancy mirrors. Only found this screwdriver type in an electronics shop! 

However i dont think star screws would work long for brake's impact screws. And please dont use a nail to hang stuff on plaster.
One additional use I didn't see posted is that in some  assembly line situations in order to prevent the wrong similarly sized bolt or screw from being used different heads can be used to ensure the wrong one can't be used at that station. 
I've got about 10 years of hardware retail experience, and one of the more common reasons we still sell (and consequently, people still use) different head types is restoration work. When someone comes in while restoring an old thingamajig, they generally want new versions of the screws that were originally there. Sell lots of brass flathead slotted screws for that very reason.
I enjoyed this question/answer.  

Thank you.  
Believe it or not, each screw type has a specific use case and some are improvements on others. There is no universal screw because there is no universal use for it.

For example a flathead is the easiest to use and cheapest to manufacture. You don't even need a screwdriver to tighten it but there's the issue of slipping, and a lack of torque.

The Phillips head makes up for the torque and slipping issues of the flathead, there are actually different types of cross screws, such as [JIS](https://youtu.be/ZY7XO5H_6HY?t=6m6s), and fearson heads.

Robertson screws are commonplace in Canada, basically they are the Canadian version of the Phillips head. They do have one advantage: the square head tends to stay on the screw driver without you needing to hold it.

The hex socket provides once again more torque, and a form of basic tamper-resistance. There are whole other types of screws to prevent tampering, and a universal screw type would defeat the purpose of that.

More importantly universal heads do exist: the Phillips' Pozidriv/Supadriv for example, combines a Robertson, Phillips, and flat head all into one. I see them all the time on receptacle terminals.

For a list of screw heads with explanations there's [this wikipedia article](https://en.wikipedia.org/wiki/List_of_screw_drives)
Advancing technology comes up with better solutions, but there is still a lot of historical backpack to drag along.

I have, more or less, whenever I have a choice, dumped everything but Torx. They are just so much better. For a few special cases, other's might have reasons to exist, but, as far as I'm concerned, Torx is the best general screw today.
Because companies (such as Apple) don't want you to open up their products (e.g. iPhone) to do your own repairs.  They'd rather you just buy another new product from them.  So they use very obscure screw types such as the pentalobe, and most people won't have a pentalobe screwdriver lying around.
Proprietary screws are widely used in electronics and the car industry, both for preventing people making dangerous repairs and s they can try charge extra money when you take it into the shop Apple and BMWboth use them for this reason to my knowledge 
Actually I believe its evolution of the tool. Originally the flat head was the most simple and first invented. Later the Phillips head was created for more tork and grip. Later the tool evolved into allens, stars, and torks. I think the reason we havent just come to use one is because of the other reasons posted, cleaning and usefullness.
[removed]
I'm late to the party, and I'm on mobile so I can't tell if this has already been pointed out. (It probably has.)

Yes, hail torx and the bi-hexagonal head; praise be, an end to screw stripping for all time. 

But, we're far more likely to carry around a couple of coins (flat head substitutes, naturally) in our pockets than a six-point star headed screwdriver. Day to day, for those ~~peasants~~ people who don't do repair work or engineering every day, it's just more convenient. 

I'm sure we'll transition to more durable and reliable heads, but Phillips heads won't ever be completely ruled out, at least not for the foreseeable future until we have easily producible consumer grade hardlight technology or something. 
[removed]
Well, in the early days power tools didn't have any built in mechanism to reduce torque. Driving in a ponzi style screw carelessly would result in jarring kickback. Phillips head would simply disengage by jumping out. Basically you could argue phillips head screws are obsolete.
[removed]
Most hand sanitizers use alcohol, which kills indiscriminately.  It would kill us if we didn't have livers to filter it, and in high enough doses will kill anyway.  Some germs survive due to randomly being out of contact, in nooks and crannies and such, not due to any mechanism that might be selected for.
Sanitizers almost always use alcohol, which bacterial cells don’t really have any cellular means of developing resistance against.  You may as well worry about developing resistance to having a nuke dropped directly on your face.  Alcohol essentially saps bacterial cells of all moisture instantaneously, and to combat that they would need to develop characteristics which would essentially make them not even bacteria anymore (like a plant-like cell wall or a eukaryote-like complex cell membrane)

EDIT:  I got a few things wrong, thanks for pointing them out everyone! (no sarcasm intended).

-  Alcohol doesn’t work mainly by sapping moisture, it actually causes the bacterial cell membrane (and eukaryotic cell membranes also) to basically dissolve.  We can put it on our hands because of our epidermal outer layer of already-dead cells which basically doesn’t give a fuck about alcohol.

- Some bacteria actually can develop resistance to low to moderate concentrations of alcohol, by devoting more resources to a thickened cell membrane.

- Look up bacterial endospores.  These can survive highly concentrated alcohol solutions and cause surfaces to be re-colonized under the right conditions.
So it’s kinda like this: there’s a difference between antibiotics and sanitizer.

Let’s think about if you wanted to wreck someone’s car: you could do a small targeted attack (cut a brake line, drain the fuel, ruin the steering). For every strategy you choose, they can improve it (locked fuel door, etc). You could also take the less glamorous approach and just completely destroy the car baseball bat at midnight style. 

That’s what alcohol does, it’s the crude style, it’ll always work, and you can’t really stop it.
[removed]
It depends on what’s in the hand sanitizer. 

The triple antibiotic soaps and hand sanitizers will absolutely cause resistance to develop. This has already been documented and it is discouraged to use those types of soaps and sanitizers. 

For alcohol based sanitizers, the mechanism of killing bacteria is much more intense, for lack of a better word. Antibiotic resistance can be through random mutations of the targeted protein or an enzyme that sequesters or degraded the antibiotic; antibiotics act in a very specific way, so resistance is just a change in the very specific mechanism. Alcohol’s effect is far-reaching and affects nearly all aspects of bacteria. It is very unlikely that all the proper mutations will be present to resist the alcohol’s effect. In fact it’s so unlikely that it hasn’t been documented to any reasonable degree that I know of. 

The 0.01% is most likely due to bacteria forming spores or improper technique over a tiny portion of the skin. 

Please correct me if I’m wrong or have any assumptions I should state. 

Edit: felt that further clarification of why spores don’t develop resistance was necessary.
You can think of spores as dehydrated cells. They have a thick cell wall that resists most extreme environments, e.g. low nutrients, low and high temperatures, low moisture, radiation, etc. so they won’t react to any stimulus at ergo they won’t acquire resistance since they aren’t really living (so to speak)
They are actually a worry when sending space equipment to other planets. How do we know if life found there is native or just a spore that decided to start populating the planet when it fell off the space equipment?
There are two parts to this question.

First, sanitizer probably will kill all germs; the 99.99% is given to err on the side of caution, to prevent people from suing "hey, I found 2 still living germs out of the billions I started with, you are making false promises" (and proving that those 2 germs were due to contaminated sample or the sanitizer was not used properly is difficult).

Second, AFAIK it's impossible to be immune against the alcohols used in sanitizers; there's too much of it so that even a slight immunity would not be enough; all biological processes would probably have to change to be immune.

The same way biological systems cannot develop immunity against fire or strong acids/bases, they cannot develop immunity against sanitizers.
No. Alcohol basically nukes the bacteria. I actually did a research project in high school where I was looking at survival rates of bacteria after hand sanitizer application. That 99.99 percent figure is basically just a way to cover your ass if someone gets sick, because my results showed that it killed basically everything.

I did get a neat side result though. It turns out that bacterial genetic material can survive and be picked up by new bacteria after hand sanitizer use.
"99.99%" is more or less a marketing safety net, as they can't just say 100%. The alcohol in virtually all hands sanitizers will kill any bacteria it comes in contact with, for reasons explained in other comments, but in the case of bacteria lucky enough to be missed when applying the sanitizer, it's just safer to say "99.99%".

Similar with the "99.9%" chance of preventing pregnancy with condoms. Sometimes pregnancy does occur due to user error or manufacturing defect. 
Since I don't think anyone else here has clarified the two words, the difference is between an anti-*biotic*, and an anti-septic.  Bacteria cannot build tolerances to anti-septics, things like bleach or alcohol or even fire.  They destroy the bacteria chemically, they make it physically impossible for the cultures, as well as most other life forms, to survive.

Bacteria *can* build tolerance to anti-*biotics*.  These kill the bacteria biologically - preventing these particular life forms from existing, some by targeting the cell wall, others by targeting the cell membrane, others by the bacterial enzymes.  

Also crucial, is that examples of anti-biotics aren't just limited to prescription pills given by your doctor.  You can find them in hand soaps and even in some hand sanitizers, in the form of "triclosan", which can build triclosan-resistant bacteria.
[removed]
So, the issue with hand sanitizer is not that it isn't effective at killing bacteria, the problem is that it is very good at killing bacteria indiscriminately.

Bacteria live all over your body, inside and out.  Their behavior ranges from beneficial, to neutral to detrimental.  For most people, the vast majority of bacteria making their home reside on the beneficial side.

The reason for this is that even a bacteria with no direct positive (making food more easy to digest or whatever) but also no direct negative (making you sick) still use resources that prevents a directly negative bacteria from taking it's place.  These neutral bacteria provide us the benefit of competing with negative bacteria.

When you rub alcohol all over your hands you kill all positive, negative and neutral bacteria on your hands, which opens up a massive number of new homes for bacteria all over your hands and some of those bacteria might not be friendly.

So what do?

Don't use antibacterial soap for your hands (dishes, w/e).  Water alone removes a significant number of transient bacteria.  Seriously, it's between 50 and 75%.  Handwashing with soap and water is of course better and will get rid of 70-95% of transient bacteria (depending on study and what bacteria they're looking at).  These methods will leave the resident flora (for the most part) of your hands happy to live and compete with all the negative nancy's that try to enter to community.

The only real reason to wash with alcohol or other disinfectants is when you're practicing aseptic technique, either for maintaining pure cultures or treating people medically.
[removed]
I can't believe this isn't on here (from what i saw), its 99.99 because there isn't a scientific statistical analysis test that can promise anything higher than that. Bleach or hand sanitizer in this case will kill anything it comes in contact with but statistically we can't prove anything over 99.99%. (this isn't to say any other answers on here a wrong but its worth mentioning analysis and standard deviations)
[removed]
Another thing to keep in mind here is that unlike anti-biotics, which go after bacteria by poisoning them (in so many words) and to which bacteria populations can evolve resistance; *sanitizer* is more like shooting the bacteria with a little gun. The populations can develop defenses against the 'poisoning' approach, but it is hard to evolve a defense against being shot. The spores (as mentioned) are resistant, but that resistance is unlikely to result in genetic drift for reasons others have mentioned.

Sanitizer dehydrates and/or denatures the bacteria, for lack of a simpler explanation. In so many words, the alcohol causes the outer membrane to 'dissolve'. A few may get too low a dose, or may be in a spot on your hand where the sauce wasn't spread well. Examples might be in the crinkle of a knuckle, or in the fold of your palm, or under your fingernails or something. Having dirt/material on your hands can also cut the effectiveness of hand sanitizer, which is why most bottles say something like "does not replace soap", or "wash hands before using".

These bacteria remain vulnerable when an appropriate dosage is applied to them for a few minutes and they aren't hiding out somewhere in a nook or cranny :).

You would not want to drink/eat enough sanitizer to function as an antibiotic, because it would also attack your body cells. You would be in very bad shape by the time enough sanitizer was in your body to do anything useful against the bacteria (for example: topical rubbing alcohol is not consumed, it is applied to surface wounds). 

tl;dr--*Antibiotics* work because they target either the reproductive and/or metabolic activity of bacteria in some way that does not overlap significantly with the way your body's cells function. They can target bacteria specifically with minimal impact on your own body. *Sanitizer* destroys the cells physically by affecting their construction, but can not discriminate between bacteria and your body--this is why overuse of sanitizer makes your hands dry and itchy. Your body cells are much tougher, and there are more of them, which is the reason you can use it without any significant damage to yourself :).
[removed]
A lot of the responses here are just flat out wrong in saying that bacteria have no means of developing resistance to isopropyl alcohol or that all bacteria that comes in contact with it will die 100%. 

Hand sanitizers kill germs by dissolving the plasma membrane of bacteria and causing the cells to lyse. Resistance to this can be formed by altering the composition of phospholipids that make up the plasma membrane, and this evolutionary trend has been observed in bacteria present on needles that were not sterilized properly. If a sample flora of bacteria present on our hands were isolated and expanded, then periodically exposed to alcohol while other competing selective pressures were minimized, alcohol resistance will occur, that's just evolution.

To answer why we haven't seen a bunch of alcohol resistant bacteria arise from increased use of hand sanitizers, multiple factors are probably at play. It may be that the change required to obtain resistance to alcohol confers some form of selective disadvantage and makes bacteria less competitive than they were originally, and the exposure to alcohol isn't as important. The high alcohol concentration may also create a bottleneck too narrow for rapid transmission of resistance genes. Uneven application may lead to survivors that had no real resistance but survived in greater numbers than resistant strains. All of these things could be affecting the evolutionary trends of the bacterial flora, but you can bet that if a lot more people start using hand sanitizers frequently these resistant strains will surface over time. 
Hey folks. This has nothing to do with the efficacy of the alcohol. It is the claim that the mfr is legally allowed to make on the label.  99.99% is called a [3 log reduction. ](http://microchemlab.com/log_reduction_and_percent_reduction_calculations). I've been out of the business for a while,  but to claim 99.999% is either overkill (heh) or ridiculous to pay to prove given the usage.  
Hi pretty late response but from a microbiology point of view yes you are correct is assuming that the 0.01% would br sanitizer resistant. 

For this to occur the cell has to have spontaneous mutations which provide a way of resisting destruction. 

What science counts on is that by mutating, the cell might also gather disadvantageous mutations which kill the cell in a different way. for example, mutation leads to alcohol resistance but this interferes with DNA replication hence cannot replicate and will die.

Hope it helps and don't hesistate to reply if you have any questions!
Alcohol in hand sanitizers is like stabbing a guy with a knife. Even if some people survive your knife attack, it usually just means they got lucky. It would take a really long term evolutionary approach to develop a natural resistance to knife wounds (tougher skin, wider bones protecting vital organs, etc) and it is likely even tougher to develop a biological protection from alcohol. The .01% that survives likely just didn't come into full contact with the alcohol. 

Non-alcohol based hand sanitizers aren't actually thought to be that effective. Warm water and thorough scrubbing are key in that scenario.
Other answers are technically wrong. 

Bacteria CAN develop resistance to alcohol, just as they theoretically can develop resistance to anything. 

The risk is low, due to how alcohol kills bacteria, basically by physically destroying their membranes. But it could happen. And if it did, you would then likely kill that surviving bacterium with some other method like hand washing before it became a problem. 

Resistance is generally only a problem if you only and consistently use one method to kill bacteria. If you have other effective methods, then you can kill bacteria that become resistant to one method. The odds that a bacterium would become resistant to alcohol, soap, and your immune system all at the same time are statistically 0.


So cleaning your hands is good. Sanitizer is good for most bacteria IF used correctly. And I've never seen anyone without a medical degree use it correctly. 

Fyi the correct way to use sanitizer is to scrub your hands with it while they are absolutely dripping wet with sanitizer for 30 seconds. If you're not dripping, you're not using enough.  


And fun fact: the best sanitizers use a mix of alcohols. For example 30% isopropanol and 30% ethanol in your sanitizer is better than 65% of either alcohol alone. We don't know why.   
3rd year immunology/microbiology PhD candidate. Haven't seen anyone mention viruses... Hand sanitizer doesn't kill viruses that don't have a lipid envelope. Hand sanitizer won't help you against things like norovirus (stomach virus). You are much better off washing your hands than relying on hand sanitizer. 
Biologist here, I haven't read all the top comments yet, but I've seen a lot of replies that are talking about bacteria evolving resistance to alcohol. So I'm going to offer an analogy to help clarify the point. 

Imagine a car. Antibiotics kill by destroying something small and specific, like snipping the wire that connects the starter to the engine. Alcohol kills by lighting the car on fire. For obvious reasons, it's easier to either make an extra wire to the starter, or to find another way to start the car, than it is to make it fire resistant. 

But it's still possible of course, but much like making a fire resistant car would have catches (like say a tank) there are some catches to evolving resistance to alcohol. The methods I'm aware of generally involve senescence (basically hibernation) or developing a barrier (like armoring your car). But an armored car won't drive around as well as a regular car, it gets worse gas mileage. And the bacterial world makes Mad Max look like My Little Pony, so if you aren't well adapted to your environment you die, and quickly. 

And that's basically why we don't worry much about alcohol resistance, some species are, but the trade-off generally makes them less infective, because they don't survive as well under normal conditions. 
Here is the issue. Say there are resistant bacteria on your hands that didn't die from the hand sanitizer due to some sort of advantage. A lot of times the features that make a bacteria resistant require the bacteria to work a lot harder to grow. As an example, maybe it has a thicker membrane which allows it to survive the sanitizer. But this thicker membrane requires more resources to make, and reduces the bacterias ability to absorb nutrients through it's membrane. If your hands were covered in hand sanitizer all the time, this bacteria would thrive because it's the only one that can survive. The disadvantage of the thicker membrane would be balanced out by actually being able to live in this hostile environment. But eventually your hands will get coated in other types of bacteria and the hand sanitizer will evaporate off. Those other bacteria who are also competing for the resources on your hand, will outcompete the resistant bacteria, and multiply faster because they don't have the burden of growing this thicker membrane. This is because the thicker membrane doesn't provide any advantage on unsanitized hands, but still comes with all of the downsides.


There are a lot of different mechanisms that provide a selective advantage in organisms given the right environment, but almost none come at no extra cost. Things like alcohol, bleach, and other harsh cleaning products attack the cell in so many ways it would be difficult to find a cell that existed with all the necessary components to survive such an onslaught. The resource cost to the cell would be enormous and would only exist in the most minute of quantities if at all. 

The reason hand sanitizer says 99.99% is more to cover there butts than anything else. 
Anytime a product says "99.99%" it means 100% for all intents and purposes except legal. If they claimed 100%, and 1 germ or microbe was found under a microscope, they would be sued for false advertising. But rest assured that you are killing everything
The 0.01% (that's not a correct number) refers mostly to bacteria and virus species which don't die to alcohol, rather than a proportion of a species which is normally susceptible. For example the clostridium difficile bacterium cell wall structure protects it from the toxicity of alcohol gel.
So the way this is calculated is by a log reduction in the kill of many, many types of bacteria. The hand sanitizer is diluted to it's MRC, or minimum efficacy concentration, then tested against the bacteria and if they reduce the growth from say 1e8 to 1e2, that's a 6 log reduction, or about a 99% effectiveness. It might be that at full strength there is total kill, but like what was said already, the killing might also be indiscriminate, and not based on mechanism, like penicillin is. Also there are several, intense tests these products must pass, if a product makes it to market, it has been researched, tested, and likely mimics one already available and known to work. Developing sanitizers that don't confer resistance is obviously a goal and is considered by companies. However, not all mechanisms of action are known, so rigorous testing and model systems have to be used, but basically, very few actual new chemicals are used in desiging new hand sanitizers, so their properties and effects are well known.
Despite the push-back answers you're getting from me too not-scientists who are incorrectly copy and pasting claims that alcohol cannot be resisted, the correct answer is actually yes.

Triclosan is one of the most common agents in these hand washes, and [resistance genes are well known](http://ec.europa.eu/health/scientific_committees/opinions_layman/triclosan/en/l-2/5-risk-resistance.htm).  The scientific community is angry because triclosan does not appear to confer a consumer product benefit, but resistance genes resist things we actually care about.

The alcohol in hand sanitizer evaporates in 5-10 seconds, but it takes 5+ minutes to kill most kinds of bacteria.  The breathless claims that bacteria cannot learn to resist simple alcohols is utter nonsense; our liver cells can do it just fine.

Acupuncture, which uses alcohol sanitization extensively, is [well known to create alcohol resistant mycobacteria](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC140401/).

The triclosan resistance in staph is [believed to have kicked off one of the seventeen mechanisms underlying full spectrum resistance](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4295542/).  This isn't theoretical.  We have a strain of what's commonly called "the flesh eating disease" that resists literally everything we have to stop it, including the shit so heavy duty it kills the host.  [Hand sanitizers are believed to have played a significant role in creating this](https://www.ncbi.nlm.nih.gov/pubmed/16922622).  This is [well studied in the literature](https://www.sciencedaily.com/releases/2017/07/170703085348.htm).  There is [extensive data](https://academic.oup.com/jac/article/46/1/11/695646/Triclosan-and-antibiotic-resistance-in).

Also, triclosan's entire point is to kill bacteria, and it succeeds (just not on the ones on humans.)  Supporting bacteria keep bad bacteria at bay, but [triclosan broad-pattern wipes out benthic communities](http://pubs.acs.org/doi/abs/10.1021/es401919k), leaving nothing to keep the monsters in check.

It's not a "could".  It's a "we were able to measure it 30 years ago, and it's a very serious problem."

The medical communities that refused to deal with DDT or BPA are actually sounding alarms about [how bad this is](http://aac.asm.org/content/48/8/2973.full).
It doesn’t make resistant strains, but it allows strains that are already non-soluble to flourish. Like C. diff, for instance, which has a hard outer shell & is only killed by bleach. It is one of the .1%. Handwashing with soap for at least 30 seconds will wash it away (but not kill), 
You’re confusing antibiotic resistance with bacterial biology.
Bacteria spores will survive in alcohol.  When I worked in a microbiology laboratory, we would actually store certain bacterial strains (spore formers) in alcohol to prevent contamination from other microorganisms and to provide conditions to assure they remain in the spore state.  Spore forming bacteria form spores to assure survival in non-optimal conditions.  The spores are quite resistant to temperature, pH and other harsh environments.  This assures the survival of the bacteria.  Only certain types of bacteria produce spores. 
The question highlights the difference between an antibiotic and an antiseptic. Antibiotics screw around with the bacteria (inhibiting mitosis, damaging the cell wall), and an antiseptic is like napalm, killing everything (viruses, bacteria, a few skin cells) it touches.
I was told a while ago that the reason they put 99.9% has nothing to do with the germs actually killed but our ability to test for all the germs. As said elsewhere, alcohol is thorough. We just can't test for all possible germs present. Not sure if it's true, but it made sense to me.
Yes.  This is part of the reason C. Diff infections are becoming more prevalent. Alcohol, the active ingredient in hand sanitizer does not kill C. Diff. spores https://www.ncbi.nlm.nih.gov/pubmed/20429659  Bleach does, but not alcohol.  Before you get the bright idea of adding bleach to your hand sanitizer, be warned, that makes chloroform.
While it has been pointed out that sanitizer dosent help germs evolve resistance normally (alcohol based sanitizer)   there are some anti-biotic based cleaners that could (triclosan) which i believe has been banned in soaps.  Also sanitizers are just a placebo and have almost no measurable effect on how often you will get sick. 

http://sitn.hms.harvard.edu/flash/2017/say-goodbye-antibacterial-soaps-fda-banning-household-item/
One danger of sanitizer however is that it can kill the healthy microbiome living on your skin, which was preventing pathogenic bacteria from taking root due to 'competitive exclusion'. This allows the potential for bad bacteria to get on your skin and make you sick if you sanitize your hands and then go somewhere dirty.
[removed]
Some questions: 1. When they say 99.9 percent of germs, does that include viruses and fungi?  2. Do alchol based hand sanitizers get absorbed through the skin? 3. Germs don't die instantly, right? I am guessing contact time matters?- my disinfectant spray says allow 30 seconds to a minute to kill certain microbes. 4. How do native bacteria on our skin repopulate if we kill them with hand santizers?

Common belief is that if we select against a population, that only those resistant will survive. However that is not really true, the bacteria that survive can survive through a myriad of ways, like being in the correct location to survive in the environment, but another one of which we think for evolution would be a mutation. However, if the bacteria doesn't have the mutation that will allow it to survive the hand sanitizer, the reality is that random mutation chance is too low. Instead it would usually pick it up from the environment, from another bacteria that already has the gene, through horizontal gene transfer. 
No scientist but i believe numbers like that are cover your ass numbers, remember birth control is also 99.9 percent effective but that .1 gice them a buffer incase a womam gets pregnent so she cant be sued.

Aka if it didnt say that how man many lawsuits theyd get if someone using it gets sick.
Aha- In my field! While the 0.01% may survive, that does not necessarily mean they are "actively resistant" to that drug, in this case alcohol,  by means of an active resistance mechanism. There exists, in any population, quite a different level of metabolic and various other transcriptionally different populations from the majority of the bugs. As such, there are naturally "resistant" bugs that do not encode a particular resistance gene, but are functionally resistant due to their transcriptional profile. If you want some pubs on the topic let me know- a bit tricky linking them on the phone in all
Well... It depends on the mechanism of how something kills the bacteria.

You could make poisons to kill a population of mice, and over generations they might develop a resistance to it. But no mouse survives getting snapped in a mousetrap, and so it would be impossible to evolve a resistance to being crushed in mousetraps. 

Hand sanitizer is just overkill. There's nothing to resist.
This is like a university level multiple choice question haha. No, they would not. When we talk about evolution in application to bacteria, viruses etc, resistance to a drug/treatment is a physical change at a cellular level. For example, changing the protein coat of a virus so that drugs can't target them, or changing something that has to do with the life cycle that a drug previously inhibited. However, alcohol just kills cells, kinda like a nuke would wipe out a city I guess. No mutation is going to provide protection against something that destroys on that level rather than inhibit. 
[removed]
Yes.  [Here](https://m.youtube.com/watch?v=x1SgmFa0r04) is an excellent map showing accurately modeled atmospheric levels of CO2 from satellite and ground measurements taken during a year, for example.  You can easily see humans emitting it, and then forested regions sucking it up.  Unless it’s winter in that hemisphere, in which case it just swirls around until spring.  Other gas levels show similar seasonal patterns.

(Edit: changed to specify that it is a model based on continuous samples.  They obviously can’t sample the entire atmosphere at once every day.  And CO2 isn’t bright red.  Among other points people apparently felt necessary to clarify.)

(Edit again:  wow, I was not really expecting so much karma and a double-gold for this.  The question just reminded me of this cool map I once saw.  I bet it's even a repost!)
According to *Measuring Metabolic Rates* by Dr. John RB Lighton, atmospheric levels of oxygen are incredibly stable worldwide at 20.94%. That is all locations, all altitudes and all year.

Of course barometric pressure will play a role due to Dalton’s Law of Partial Pressures, but when compensated for you’ll get such a stable reading that you can calibrate a sensor against it.

The only time oxygen is much different is when measuring essentially exhaled breath. But if you get outside a confined space and away from creatures, you’re at 20.94%. 
[removed]
While there may be a slight difference in air quality, the vast majority of Earth’s O2 supply comes from waterborne algae. So theoretically, all the trees could be cut down and we would still survive. However, trees are pretty and is like them to stay a while.
No, oxygen levels do not noticeably change. The CO2 video, while interesting, shows changes in levels measures in parts per million - like 10 PPM between summer and winter - so no where near noticeable for human.

As an example, air we inhale has about 21% oxygen and we exhale about 16% oxygen (and 5% CO2). So that change is ~50,000 PPM.

Likewise with air quality - there are differences but nothing humans could detect. And even then human factors (like proximity to vehicle exhaust) outweigh anything natural (except fires).
Most posters have focused on carbon dioxide levels, but air quality does change seasonally as well.

The two most important pollutants that contribute to smog in the US are ozone (O3) and particulate matter (PM).  Ozone concentrations are highest in the summer, as the production of ozone is faster when there is more sunlight and higher temperatures.  Trees contribute to ozone formation by emitting isoprene and terpenes; when those compounds are photo-oxidized in the atmosphere ozone can be created.

Similarly, particulate matter also has seasonal variability.  Oxidation of monoterpenes can contribute to a biogenic component of PM and is important in the southeastern US.   

However, most of the seasonal variability in air quality is a result of changes in meteorological conditions (temperature, wind speed, humidity) and not based on changes in biogenic emissions from vegetation.  Human emissions are the root cause of most regional and urban-scale air quality issues, and our emissions tend to not vary much seasonally.
[removed]
I'm from the Pacific Northwest I have traveled all over the west coast but have never gone east, is it heavily forested?

Because I tend to think the opposite when I think of the East coast I think of suburbs and citys.

I thought I lived in the most heavily forested area in the US.
Consider the CO2 trends from Mauna Loa

https://www.esrl.noaa.gov/gmd/ccgg/trends/

CO2 is on the order of 400 ppm, and fluctuates by about 10 ppm per year. If we assume a corresponding change in O2 (since photosynthesis exchanges carbon dioxide for oxygen 1:1), that's roughly a 10ppm/200000ppm (0.005%) seasonal fluctuation (rounding oxygen concentration to 20% for simplicity).

I wouldn't expect seasonal fluctuations in temperate regions to be much different.
[removed]
Are there any effects on air quality other than CO2 caused by trees? If there is only a small fraction of our CO2 absorption coming from trees as opposed to oceanic algae, are there any other factors at play when, for example, China decides to assign thousands of people to plant trees -
 or is that a public image thing rather than one rooted in good science? https://www.independent.co.uk/news/world/asia/china-tree-plant-soldiers-reassign-climate-change-global-warming-deforestation-a8208836.html
Yes! The Keeling curve which is actually a visual representation of the atmospheres CO2 can show this. If you look at the graph, the oscillation of CO2 is actually due to a bunch of different factors but one big one is the forests loosing their leaves no long perform photosynthesis and therefore do not use as much CO2. In the very early spring the CO2 reaches its maximum right before everything blooms and begins taking in CO2 again. 

[Keeling Curve ](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Mauna_Loa_CO2_monthly_mean_concentration.svg/500px-Mauna_Loa_CO2_monthly_mean_concentration.svg.png) 
Globally, as CO2 levels have increased, O2 levels have decreased (or rather the ratio of O2/N2). Charles Keeling started CO2 measurements in Mauna Loa in the 1950s. His son, Ralph, started making O2 measurements in the late 1980s.  There is a Scripp's [video of Ralph](https://youtu.be/6WFCoJgt71A?t=3m24s) describing the relationship between CO2 and O2 in the atmosphere.


I don't know if you can notice a difference from month to month because it is a gradual change and your body probably acclimates to it over time. But I noticed a difference when I went on a trip to the Carolina's, I live in Florida myself and I spent 2 weeks in the mountains of North Carolina and for the first few days the air felt cleaner and easier to breath. 
[removed]
As a restoration ecologist I would not consider the Eastern US "heavily forested". The Western US, Washington, Oregon, Calif. I would say is more forested at this point in history. The Eastern US was at a one point an extremely lush, large contiguous forest area of mixed oaks and American chestnut and many other mixed forest types. Then we (humans/settlers/da man) clear cut it several times over a couple hundred years. 
[removed]
in terms of optics and photoreceptor density, which are the principal factors in a creature's visual resolution ("details per degree of visual angle"), there probably isn't much real difference between the *angular* visual resolution of e.g. a jumping spider and a house cat - both can see, at best, about ~10 details per degree. so if you shrank a cat down to spider size (or vice versa), they'd both have similar limits to the smallest things they could see.

but since they're different sizes, the sizes of the things they can see will also scale; since the eye is getting smaller, its ~~*near point*~~ (i think i meant 'depth of field' here) is getting proportionally shorter. so if a jumping spider's eye is a thousand times smaller than a cat's eye, it can potentially resolve details that are a thousand times smaller than what a cat's can resolve. a cat can never get optically close enough to a grain of sand to make it a degree wide, so that it could see 'ten details' on its surface, while this optical distance is easily available to the spider.

the caveat to this general scalability of vision is in the "noisiness" of light, i.e. factors like diffraction (limitation in how small of a point can be focused) or chromatic aberration (the difference in focal distance for different light wavelengths) - for a big eye with a big pupil, this noisiness is insignificant, but for a spider's eye it is getting significant, since all of us are looking at more-or-less the same light bandwidth. jumping spiders, for example, deal with this by having retinas at different focal depths to try to account for chromatic aberration.

but this stuff gets complicated, especially considering that the range in optical quality and photoreceptor density across species washes out most of these limitations. i think you could safely suppose that, in the range of terrestrial creature sizes, vision basically scales with size.

*edit* the important thing is that things scale, but my concepts are confused re "near point" etc, see /u/craigdubyah for better/more detailed info---

*edit edit* sorry i didn't keep up with this, was busy all day; but let me excuse the rough edges of my answer by saying it was a lot of well-informed hand waving and of course i didn't try to go into detail about diffraction and lens power and etc etc; i just wanted to get the basic gist across that vision should more-or-less scale with the size of the eye, receptor sampling being equal (as in the cat/spider comparison). field of view, pupil sizes, etc, all very important but another time and place, ok?
[removed]
Okay, so in terms of physical optics, and not taking into account the limits of biology, a smaller aperture means less resolution. There are simply not as many photons coming in to tell you about the world, and there are limits to how far a given aperture size can zoom in before diffraction starts to make things blurry.

Optically speaking, an ant would have lower limits on its visual acuity than an elephant.

Biologically speaking, since we can't focus at arbitrarily close distances, ants would be able to see small things better, and elephants could probably only see ants in small groups, the same way we can see groups of blood cells, but not individual ones.

The pinhole effect mentioned elsewhere is only relevant to the lens of the eye, it's the aperture that determines maximum angular resolution, and the retina that determines how much of that resolution a creature can make use of.
[Different animals have remarkably different types of vision.](http://webecoist.momtastic.com/2009/01/14/animal-vision-color-detection-and-color-blindness/) Ants and most other insects have compound eyes. So bug vision and human vision are apples and oranges. Visual sharpness is a function of focus and resolution. Resolution is limited by a) the number of receptor cells used to draw an image and b) the color of light/radiation being perceived.

There's a physical limit to how small a microscopic thing that can be drawn with the 390-700nm visible light range, so it's not like tiny eyeballs could see viruses. Ants clearly see antennae and hairs on their nearby sisters, but it's not clear that ants can see our eyeballs at distance or a Paramecium in a water drop.

The sharpest visual acuity in human-like eyes is found in eagles and other raptor birds, as sharp as 20/5 or 20/4 as we can measure it. Eagles don't have large eyeballs, even for birds, but they have exceptionally dense receptor cells. It's estimate they could resolve ants from atop a 10-story building. I would see a palm-sized tile in the sidewalk; eagles would see tiny dots painted on it, and that some of them were moving.

I've read anecdotally that elephants aren't "afraid of mice", but that they probably see mice as blurry, unfamiliar movement near their feet. If cantaloupes are clear and mice are blurry, then ants are probably "microscopic" to elephants.
Leaving out actual biological examples and speaking strictly in terms of the physics of what would happen if you shrunk or grew a human eye, kind of. 

If you took a normal human eye and shrunk it down, the iris would shrink with it. This is equivalent to having a camera with a tiny aperture. If you have glasses, try taking them off, making a really tiny hole with your fingers and looking through it - you'll see clearer. This is also why squinting helps you see - you're looking through the tiny holes in your eyelashes. This makes both near *and* far clearer - what photographers call "increased depth of field". The tradeoff is brightness - make the hole too small and not enough light gets in. 

On the other hand, if you grew an eye, the iris would get larger, eventually it would be a gaping hole and the eye wouldn't be very functional at all. With a quarter sized iris I would guess you could still tell things like the direction of the sun and such, but not much more. 

The size of the lense, on the other hand, makes no difference. It's the shape of the lense that determines whether the eye sees near or far, not the size. 

Quick visual explanation of pinhole effect: http://i199.photobucket.com/albums/aa90/kimsoonee/home%20video%20audio/pinhole-effect-on-eye.jpg
[removed]
It really depends on the organism and the structures that make up their eye. Eyes have a lens to focus light onto photoreceptor cells that transfer that light into info for the brain. The main two photoreceptors are rods, which are good at seeing contrast between dark and light, and cones, which respond to specific wavelengths of light and give us color vision.

The ability to see something (called visual acuity) is measured as the smallest width an object can be and still be seen. To get a greater acuity, you either have to change the lens to focus the light better (like wearing glasses) or you have to pack more photoreceptors into your eye. For example, birds like eagles have similar sized eyes to humans but have about 5 times as many photoreceptors per square millimeter. Better vision is more common in animals that need to make out a lot of fine details, either predators that need to spot prey or nocturnal animals that have to see in the dark.

The issue with small organisms like ants is that their eyes aren't big enough to have a lot of photoreceptors, so they can mostly make out things like light/dark and general shapes and patterns. Some arthropods deal with this by having compound eyes, which gives them a greater area for photoreceptors, or simply grow massive eyes that take up a large part of their head.

For larger organisms like elephants, we can actually estimate their visual acuity by using a striped, spinning circle. Eyes will naturally follow movement, so if the stripes can be distinguished, you can see eye movement. If the stripes are too small, the circle will appear as one color and the eye won't track anything, so no movement. For Asian elephants, they found that at 2 meters from the eye, the smallest thing they can see is something a half centimeter across (about a quarter inch at 6 and a half feet, if you're an Imperial user like me).

Tl;dr, Vision depends on the animal's needs and its size. Ants are small and their eyes aren't big enough to be able to see fine detail. Elephants can see things about a half centimeter across if it's 2 meters from its face or closer.
Ok who the heck is telling you wants don't have eyes, and definitely do have eyes, look at a picture of ants, the dark spots on their heads are compound eyes. Ant eyes are like flies eyes and beetle eyes and her eyes. They have 2 compound eyes and each compound eye is made of hundreds of tiny little simple lens eyes,  so they can't see very well but they still can see.
Equation for optical resolution
  θ=1.22*λ/D

where

θ is the angular resolution in radians,
λ is the wavelength of light in meters,
and D is the diameter of the lens aperture in meters.
So yeah, your lens diameter, IE size of eye will impact the ability to resolve fine visuals. Bigger eye=> bigger lens => greater resolution of fine visuals. 
With this in mind, I suspect birds like falcons and eagles with small eyes and legendary vision have a few extra tricks to enhance resolution.
The evolution of the eye has followed a different path along the different species of vertebrates. Maybe the photoreceptors part works the same but the organs are structured in complex ways that change according to the dimension of the system, so is not a question of precision but it a problem of perception
In addition to what other posters have said, the larger a creature's retina, the more information is available to the brain.   Which is why, contrary to Jurassic Park, the T-Rex had the best visual acuity of any species living or extinct.


Edit: This applies primarily to distance vision.  
Not exactly. Although there are substantial differences in the construction of every species' eyes, it's the visual cerebral cortex that makes all the diference. It's pretty much lik ethe case of the iPhone. up until very recently they stuck with an 8 mpx camera that consistently produced very high quality photos compared to higer-end cameras, because of it's superior imagen processing capabilities. 
Wouldn't it depend on the complexity of each animals eye? For example a mantis shrimp has more developed eyes than that of a human, so they would be able to see smaller things. Also would microscopic animals be able to see us? or are we just a blur?
[removed]
As you go smaller lens and expanding light becomes too expensive. So instead of using lenses they start to use electricity sensors and fulcrums to map areas too small for the naked eye. The scanning electron microscope charges the the material your looking at and shows a map of the resistances. All of it is seen in different shades of white but because most organic matter contains varying resistances it shows up almost like you could see the object up close. The next is the AFM uses a needle, some can be as small as one atom at the point, that drags over the surface of your object and literally maps the surface. I was in my universities lab and we had only managed to be able to see the holes between the atoms of carbon atoms.
Idk the real answer here but my educated guess would be that the short answer is no, small creatures cant necessarily see microscopically and stuff 
This is actually something I've thought about a bunch but once i learned that small creatures like small fish and stuff don't actually see very well (they see shadows and some shapes but nothing sharp and well defined) i realized that the ability to see well has a lot to do with the eyes complexity which needs to be larger for that to work. Of course that varies from creature to creature but... I'm pretty sure the smaller organisms in this world tend to sense the world around it quite differently than we do. Like through feelers instead and stuff rather than focusing on eyesight. 
Even cats don't exactly have the best eyesight. They cant see their prey unless it moves which is why when they are hunting something they will "cackle" and swish their tail to get the creature to move so they can attack. Their eyes are great for night time but daytime viewing is sacrificed for that ability. I can tell you when i feed my cat treats i have to make sure she sees exactly where i put it. Shes farsighted so if its right in front of her she basically cant see it. 
Our vision is mostly limited by the wavelength of what we use to see with. So an optical microscope shows you pretty much the limits. Some tricks have been developed to see details slightly smaller than those limits, but that's about it, for visible light.

UV light and higher can help see more, but it's energy intensive and not easily available everywhere. Insects can see it, however.

Creatures smaller than that see more details through smell/taste and touching. Coincidentally, smaller animals often have antennas for that. Not sure whether any bacteria or eucaryotes can use their hairs or other organs in similar ways.

Insects have compound or multiple eyes with narrow vision instead of a few large eyes. Helps them see and identify things that are very close. For larger distances, there are often specialised eyes or several facets pointing in the according direction.

I think we can safely assume that insects can see and identify things a few orders of magnitude smaller than we - at least at close distances. Mites which we can barely see are probably quite easy to see for them, and they can probably even see details like whether the mite is carrying nectar or not.

However, such small animals have a limited nervous system, so while they can see more details, they probably can't distinguish as well as we with a microscope.

edit: micro-, not telescope...
Logically your post is sound, but factually it's minorly incorrect, it depends on not only the details per degree, but also the animal itself, because any animal atleast this is what I've been taught, can have the sa!e eye problems as humans and also worse
This is actually right up my alley as I study atmospheric science and have even done some storm chasing myself. If your son hasn’t had the chance, I’d definitely recommend watching old TV shows like Storm Chasers from  the Discovery Channel or storm chaser videos that come out on YouTube after every big tornado event! Also the weather service has great resources such as [https://www.nssl.noaa.gov/education/svrwx101/tornadoes/faq/](https://www.nssl.noaa.gov/education/svrwx101/tornadoes/faq/) with plenty of information. I've left out peer-reviewed sources since I don't imagine they're of much interest to a 7 year old.

1. When you see thunderstorms, you’re seeing rising air fueled with energy from water condensing into the cloud we see. The rising air is called and updraft, and the thunderstorm has to continually suck in air from the surrounding environment to fuel it. If the winds in the atmosphere change direction with height, this can cause the core of the storm to rotate – a rotating updraft is called a mesocyclone. Now the tricky part is we actually don’t have a complete theory for how these rotating updrafts form tornadoes – some do but most do not – and this is a topic of ongoing research! If we could figure this out, it would save many lives.  
Tornadoes rotate because the storm rotates – it’s an extension of this updraft that reaches the surface. Most of the tornadoes we typically think of are produced by supercells, which are the strongest type of thunderstorm. These storms have towering cumulonimbus clouds, have a rotating updraft (mesocyclone), and are fueled by energy from the sun and water vapor and by strong winds in the atmosphere.Fun fact - a supercell's updraft can carry tornado debris to as high as 30 thousand feet, after which it can fall out hundreds of miles away. Seeing this debris on radar is one of the main ways scientists such as those at the National Weather Service can 'see' a tornado without actually being near the storm and warn the public. The height debris reaches is correlated very well with tornado strength and the strength of the thunderstorm updraft.
2. The winds in tornadoes are so strong because the winds in thunderstorms are so strong! The speed of the updraft in a supercell can exceed 40 m (130 feet) per second, and air is drawn in rapidly to replace air that has risen. As air is drawn  into a tornado and rotates around it, it speeds up much like an ice skater drawing their arms to spin faster. This is because of conservation of angular momentum. A tornado is basically a tiny low-pressure system (like a hurricane or your sink draining) and the winds increase toward the center similarly.Fun fact - we see clouds in tornadoes (called the condensation funnel) because the pressure decreases and water vapor condenses inside the vortex, but the tornado is bigger than the cloud, and a condensation funnel is not necessary for a tornado to exist and do damage.
3. Tornadoes are not thought to have ‘eyes’ in the same way hurricanes do, but the wind speed at the center is 0. If you looked at a cross section of a tornado (see the link in 4), the wind profile in a tornado increases as you move towards the center  –  but it can’t go up forever! Near the center, it turns back towards 0 before increasing again on the other side. Any vortex has a calm center like this!
4. In some ways yes. The area near the center has the fastest winds, though the center itself has the weakest wins. Observed wind profiles from real tornadoes such as [https://www.sciencedirect.com/science/article/pii/S0167610517301174#fig6](https://www.sciencedirect.com/science/article/pii/S0167610517301174#fig6) show this pattern.Typically however, being in the center of a tornadoes path *is* the worst place to be. As a tornado moves, the areas in the center of its path will spend the longest time inside, will experience the strongest winds twice, and will endure a complete 180 degree turnaround in violent winds. This does a lot of damage!
5. Things damaged by a tornado typically are damaged by other things inside a tornado more than they are by the winds themselves. Most of the damage you see when a tornado hits trees or buildings is from debris picked up by the tornado and hurled around! This is actually a reasonably large issue in determining tornado strength. We rate tornadoes using the EF scale, from the weakest EF-0, to the strongest EF-5, but ratings are given based on damage caused. The widest tornado ever recorded, the El Reno, OK tornado in 2013, was a staggering 2.6 miles wide and is often thought to be an EF-5. Researchers nearby recorded nearly 300 mph winds (EF-5 strength) inside the tornado using radar, but since we officially assess tornado strength by damage caused and the El Reno tornado was primarily in open fields, it ended up with only an EF-3 rating.

I hope this helps, I'd be happy to elaborate or answer more questions!
Meteorologist here (MSc), and I can answer question 1 rigourously! For a tornado to form there has to be wind shear: that is, the wind speed and direction has to be changing with height. This creates the initial momentum that causes tornadoes. Then you need buoyancy, to add to the energy that causes tornados.

Careful though -- too much shear, and the system will be "sheared away" as We call it, too stretched out to do anything. Too much buoyancy, Though, and it'll collapse onto itself. So Theres this balance between shear and buoyancy (something we call the richardson number, or the BRN).

What happens when you get the right mix of shear and buoyancy? A bunch of math happens in the form of the vorticity equation. Essentially, the buoyancy tilts the shear so that instead of vertical shear you have horizontal shear (a difference in wind speed horizontally). Here's a picture of it, google vortex tube tilting: https://www.boston.com/wp-content/uploads/2013/05/Supercell20final-thumb-453x837-103416.png . As it stretches out due to buoyancy, it increases in angular velocity (think of a ballet dancer pulling himself in to increase his rate of rotation), and voila, you have a tornado!

Fun fact for your kid, the vortex tube tilting always generates two tornadoes, but generally the left one dies.

Happy to clarify anything here.

Edit: in summary: wind shear creates horizontal angular momentum. Buoyancy tilts that into vertical angular momentum and stretches it out to increase the wind speed and rotation.

Also edited: angular velocity, not angular momentum, thanks u/geeflexx for catching that
[removed]
[removed]
Hey, this might get buried, but I work for the University of  Oklahoma - and Norman is host the the NWS and Severe Storm Center. Depending on where you are located, when things from COVID are less crazy, I could help you set up a visit or even help you look into a virtual tour for your child.
[removed]
[removed]
Torandos form in the sky first as a horizontal vortex of hot and cold air streams meeting. As it becomes more powerful it turns sideways which is what actually becomes a tornado. Tornados only form when the atmospheric pressure is low enough to allow for the formation of a vortex. The winds are so strong because when the vortex first forms in the sky and begins to create the circular movement of air it acts as a continuous feed back loop where the circle gets larger and larger and it gets faster over time as long as it is still receiving input from the air streams. Similar effect to how if you were to spin a heavy object attached to a string, at first it's slow but as you gain momentum the weight of the object actually helps you go faster.

Edit to answer more:
The eye is formed once the tornado is on the ground and is actually the calmest part of the tornado. Inside the eye is an extremely low pressure zone, but almost no wind will be present. Similar again to the ball on string thing I mentioned earlier: if you spin the ball, you only have rotational movement but no horizontal movement, as you go away from the center of the spinning the horizontal movement increases creating high wind speeds.

In terms of how they crush things: it's usually just objects colliding with the ground or each other in the air. If an object was weak and lightweight it could be effected by the centrifugal/centripetal force but usually the most destructive part is just dragging large objects along the ground.
[removed]
[removed]
[deleted]
[removed]
[removed]
[removed]
[removed]
This doesn’t directly answer your questions, but a healthy fear of them might not be a bad thing to instill in him as well. I’m sure you all have already talked about it, but they can be extremely dangerous and scary - see, for example, the tornadoes in Putnam County Tennessee earlier this year in March. Lots of people die every year when the proper precautions are not taken or the proper notifications are missed/not effectively disseminated.
If it hasn't been said before, please check out Pecos Hank's YouTube channel.  He chases tornadoes, but also helps out with research, has at least one presentation that I can think of, and has a couple of interviews with a meteorologist about research in storm and vortex formation.  Plus he's an amazing guy, very laid back with a great accent and attitude.  Fair warning, he does love critters so if your little guy is afraid of bugs, snakes, lizards, and turtles, he may not like it when Hank pulls over to have a chat with them.
Just wanted to let you know that NOAA is doing a series of live webinar for kids, and the one this Friday, May 15 at 11am EDT is with a tornado researcher. Looks like there's a chance to ask questions as well. You have to register in advance, but here is the link to the series (scroll down to second webinar listed): https://seagrant.whoi.edu/suggested-educational-resources-for-use-during-school-closures/webinars-noaa-live/

What a wonderful thing you are doing, to be so encouraging of your son's interests!
[removed]
[removed]
This post has attracted a large number of personal and medical anecdotes as well as request for medical advice. The mod team would like to remind you that **personal anecdotes and requests for medical advice are against [AskScience's rules](/r/askscience/wiki/rules)**.

We expect users to answer questions with accurate, in-depth explanations, including peer-reviewed sources where possible. If you are not an expert in the domain please refrain from speculating.
[removed]
Sleeping literally changes our very physiology. Our core body temperature drops which allows certain proteins to work differently than they do during our "waking temp," as a broad example. It's not something we'd want easy control over. 

Most importantly the process of getting sleepy is highly regulated by not only our Circadian rhythm but also by other hormone systems. 

We need to burn energy to feel fatigued (when we use ATP and make Adenosine as a byproduct, which signals fatigue in humans). 

We need a lack of blue-wavelength light to initiate the process of releasing melatonin at night, which makes us sleepy and helps initiate the sleeping-end of our Circadian processes. 

We don't have voluntary control over sleep because it's chemically regulated. Adenosine, melatonin, hypercretin (Orexin), etc.. 

It's not something we can flex like a muscle. It's essentially hormonal in nature and therefore requires us to use drugs (meaning ligands that bind to targets in our body) to control it. 


My understanding is that there is an evolutionary benefit to sleep being a passive (rather than an active) drive. You need to sleep, and, if deprived, you *will* sleep (just like you *will* urinate, eventually). That said, finding the appropriate conditions for sleep can be difficult. There is [evidence of sleep patterns being conserved regardless of brain/body ratio](https://www.sciencedirect.com/science/article/pii/S0896627313009045), which suggests that passive sleep entry is also conserved. There are cyclical hormonal changes (melatonin and cortisol being the main factors), which are involved in induction of sleep, so parts of the impetus to sleep are physiological, and induction is extrinsic. Under stressful circumstances, those extrinsic factors can also prevent sleep (like a mountain lion or worrying).
[removed]
[removed]
The process of "falling" asleep is known as "Stage 1 sleep" or "stage 1 NREM sleep". Brain activity changes and the eye moves slowly (NREM stands for Non-Rapid Eye Movement). No one knows *why* this stage exists, only that people who enter it always think they are awake (they are asked to get up, breaking the NREM brain activity pattern, and then questioned)

It is followed by "Stage 2 sleep", which is a transition phase between Stage 1 and "Slow Wave" or "Deep" sleep

In short, you *do* "just decide to be asleep", it's just that being asleep involves some 20 or so minutes of you not knowing you are asleep before you lose consciousness
[removed]
There's no 'why' to how humans exist. It's more about what evolutionary advantage it would be to being able to directly control your sleep system.

What would happen if you could manually control your heart rate? Any human that could do that would quickly accidentally kill themselves. And any human that could manually & immediately control their sleep would do it wrong a lot and at inopportune times.

We can control our bladders - but it takes time and learning how to do it. Also what happens when we get terrified? Releasing our bladders spontaneously happens in inopportune times - but in that situation, it's not deadly. immediate sleep is deadly.

We can manually control our breathing (and now you are) to a certain extent. But that's because swimming would be beneficial to our ancestors in certain instances. And even with that, we can 'breathe wrong' in certain situations.

But for sleep, we DO have some control over it - similar to breathing. If you relax, get to a comfortable spot, in an area that isn't inducing anxiety or fear, close your eyes, and let your mind drift then you can "allow" yourself to sleep. This system gets you the ability to control your sleep, but indirectly and only when certain conditions are met.

That's much less dangerous and more advantageous than the direct control method.
[removed]
[removed]
We have to be very careful when describing traits/behaviors/responses in terms that make it sound 'designed' and either optimal or suboptimal.  Evolution simply favored (for whatever reason) the process that works as we know it.  There was never any evolutionary 'nudge' that changed the process to an "on" or "off" choice.  What worked 'best' (or, really, good enough) is what passed down through evolution.
[removed]
[removed]
I feel like a lot of people are straying from the core of this question. I am a senior in college studying psychology and have worked with both an evolutionary psychology researcher and a neuropsychologist who researches insomnia. The underlying reason for sleep is the circadian rhythm. While I don't have any journal articles on hand to cite, I can tell you from my lectures and textbooks and personal research (and as an individual who has suffered from insomnia) that the driving force for sleep comes from the build up of melatonin in your brain that triggers the neuron in the hypothalamus that is in charge of your circadian rhythm. If you don't know what the circadian rhythm is nows the time to learn kiddos...

The circadian rhythm is the internal clock we all have that keeps us running. Traditionally the average has been 24 hours for the length of a human rhythm but this rhythm varies with genetic mutations and what not and is different across species. Scientists first studied rat brains to understand human brains and that is where most of this research has come from (my professor did a lot of work at Harvard doing this kind of research and told me many stories about it as well as my classmates in our lectures). When it comes to us humans this rhythm is what keeps us in our daily schedule, that one you plan in your head. It keeps us hungry, thirsty, and sleepy on a schedule amougst other things. When this rhythm is in balance due to good health (which means a healthy MIND AND BODY!) sleep comes easy. Yes your mind needs to be healthy too! Stress and anxiety can kill you and it ruins this rhythm. It also changes with age, as we grow older we need less and less sleep. You can go see your doctor if you think your hormones are out of balance, which can be a cause of sleep issues, but for the most part regular healthy diet and exercise keep the rhythm on track. 

The only other factor seen affecting people today is blue light. Blue light comes from all your electronics at various levels. Blue light also is the strongest light from the sun that affects us hormonally. This light enters the retina and triggers melatonin release, and if you haven't caught on by now melatonin is the natural hormone that runs your circadian rhythm. So to help keep this factor at bay you could try switching to natural light when possible and change your sleep patterns to fit into the natural pattern of the sunrise and sunset so that you can find your true sleep equilibrium. 

After about a month or so of a consistent sleep pattern your body should naturally adjust and you could very easily "fall asleep" whenever you wanted. The key here is a balance. A balance of a healthy mind and a healthy body. Therapy exists for a reason. Your body is a well oiled machine run by the circuit board that is your brain. If you can't make the circuit board function well the whole system fails. 

If you want more information about sleep check out this website https://sleepfoundation.org/sleep-topics/what-circadian-rhythm.
[removed]
It is possible to train to choose to fall asleep in the particular moment to certain degree (see how military soldiers are trained: www.medicaldaily.com/how-sleep-solider-military-training-tactics-fall-asleep-quickly-400370%3famp=1)

Deep meditation is somehow similar. It helps to sooth the reflex of waking up and helps to rest your brain. Some experience yogis don't sleep any more but rest through deep meditation (see the book "autobiography of Yogi")

I think that both methods have a lot in common and give more control over the sleep patterns. There are of course the relfexes and other dependencies physiological and chemical. We need 8h of sleep a day but we can split it on two or three blocks (segmented sleep). NASA did amazing research on that  https://science.nasa.gov/science-news/science-at-nasa/2005/03jun_naps Some people can sleep after having cofdee, others can't. 

Didn't really give a straight answer but what I wanted to say is that sleep doesn't have to be what we are told by society or tradition or employers to be. It can be quite flexible time of our life.  
[removed]
[removed]
[removed]
The AMA will be answered intermittently by our experts. Please do not answer questions unless you have expertise in the area required. Please remember, [r/AskScience](https://www.reddit.com/r/AskScience/) has strict comment rules enforced by the moderators. Keep questions and interactions professional and remember, asking for medical advice is not allowed. If you have any questions on the rules you can [read them here](https://www.reddit.com/r/askscience/wiki/rules).
Don’t know if I’m too late for this talk. 
My question is whether there is a possibility for this strain to become seasonal like the flu? What makes the flu (with all of its types) not simply die off with a quarantine like how it happens with SARS.
Might be a dumb question, but why is it called the coronavirus?
There's a lot of conflicting information about asymptomatic carriers and the latency in presentation of symptoms. What do we know about these two subjects? Do the carriers present with viral shedding for the same duration as a symptomatic infection?  What is the average latency in confirmed cases?
What are the chances of contracting the virus in a major city outside China? The media is doing its thing and generating a lot of fear. I'd like to know whether most people here need to actually be worried about contracting the virus.
I'm curious, how did they actually first discovered this virus? For example, an infected patient goes to the doctor with symptoms of pneumonia. Won't the patient be normally treated as just another infection? Why would they test the patient for the kind of virus that is causing the infection?
How does this compare to previous viral outbreaks like SARS, Zika and Ebola? I don’t remember entire cities or regions being evacuated of foreigners, or cities walling themselves off with other outbreaks. Is this a bigger deal than previous outbreaks?
Is it true that this is "very contagious"?  I am reading that the virus size is large and droplets in the air fall to the ground or surfaces quickly, so it's not as contagious as if the virus were smaller and would stay in the air longer.  

Are people correct in saying this is very contagious? What is correct?
How can you tell the difference between being a bit ill/sick and having the coronavirus? Are there any obvious signs that separate it from traditional flu?
In the context of outbreaks like this you sometimes hear things along the lines of "Governments are working on a plan how to deal with this virus". Wouldn't there already be plans in place for pandemics that just need to be minimally adjusted to the properties of the exact virus involved?
Just wanted to point people towards this [excellent New York Times article](https://www.nytimes.com/interactive/2020/world/asia/china-coronavirus-contain.html) from today which has some very accurate graphics for understanding this outbreak.
This will probably be a very stupid question, so I apologize in advance, but during epidemics like this one, are there simulations in place that try and predict what the outbreak would be like and how far it would spread and what steps would have to be taken to contain it?
What exactly makes 2019-nCoV different from other coronavirus strains? I know it is similar to that of SARS and MERS, but I'm interested in the details that make them different.
- How does this compare to the flu, is it more virulent, more fatal?

- What age groups are most affected, what is the prognosis?

- Follow-up-- if considered more contagious, is this due to the nature of it or rather the lack of vaccines to help stop the spread.

Thank you for all the replies!

----------EDIT-----

More follow up questions:

- is it confirmed that infected people are contagious during incubation, i believe this was considered to not be the case initially yet i'm hearing conflicting evidence. Is there somewhat of a consensus on the matter?

- does viral pneumonia vary in severity based on the virus? ie: if influenza leads to pneumonia vs a coronavirus (such as this one) is the prognosis different?

- if they found a vaccine next week, what would be the timeline for them to immunize the higher risk population? 

- is the media over exaggerating the severity. I'm getting slightly appalled by the number of reports on the "deadly coronavirus". I feel like it's being sensationalized given the current numbers and evidence.


--------------------------------------

Although not relevant, i'm trying to get a better understanding of how worried i should be about my 2yr old getting this here in canada. The news does not seem like a reliable source currently and is doing nothing more than instilling fear.
How long does coronavirus, or viruses like it, survive outside of a host? Could goods being exported from China be contaminated, and if so, what types of goods?
Today's white house task force briefing had two statements about the testing that concern me.

"We have done virus isolation. But I want to be clear the current tests that we developed at CDC is not we're not sure of the natural history of how the virus is isolated. Can you isolate it one day, then, three days later, you can and we are seeing in the cases that are in the hospital. We've seen people had detectable virus, then they didn't have detectable virus. Then three days later, they had detectable virus. We're using the virus cultures right now and these individuals more to help us learn about this virus. How much asymptomatic carriage in fact is there? So I want people to understand that distinction. We're not using it as a release criteria, because we don't know the natural history of how this virus is secreted. And this is what we're continuing to learn"

\-Robert Redfield CDC Director

" I think the question you ask is really one of the fundamental basis of why this decision was made. If we had an absolutely accurate test that was very sensitive, and very specific, then we could just test people and say, Okay, we're good to go. I want to get back to the broad concept that I mentioned, when I made my brief introduction about the unknowns. We don't know the accuracy of this test. We haven't done enough people who came in with negative then all sudden, they were positive."

\-Anthony S. Fauci NIAID

&#x200B;

My question:  
If these tests have not been 100% accurate and for a week we've been testing people around the globe and releasing them, what's really being contained?
[deleted]
What's the deal with that paper finding HIV genes in the coronavirus?  Assuming that the results of that paper are true, does that make it harder to fight?  Does it make it easier to spread?  Does it make it more lethal?
What's the best way to approach fear mongering and the spread of false information? Here in the CA valley, there was rumored reports of the local Children's Hospital dealing with a case of coronavirus but the only real fact I found was the location was creating a screening process to try to catch it early on. I understand stuff like this can be scary the way its portrayed in the media but man, its just wrong to have parents worry over someone trying to get likes or attention:\\
Why does there seem to be dissenting opinion on the rate of spread and cases reported for this virus? I have seen some news stories and opinions state that China is not reporting the actual amount of cases, while others have stated there are inaccuracies in the metrics used. 

Can you offer some insight on what metrics are used and how accurate they are? Thank you!
Not a question but thank you for including 2019-ncov
How do we discover a virus like 2019-nCoV?

From what I've read about the virus, the symptoms are similar to a common cold and/or flu. So at what point does a doctor somewhere treating a patient say, "Hey, this particular patient's symptoms of coughing, runny nose, and fever are unlike regular colds and flus. It needs a new name."?
Some Indian researchers seem to have jumped the gun and are spreading misinformation with their preprint: "Uncanny similarity of unique inserts in the 2019-nCoV spike protein to HIV-1 gp120 and Gag"

https://www.biorxiv.org/content/10.1101/2020.01.30.927871v1?=1
A lot of the cases seem to be 'walking pneumonia'. 

My question is, is what determines walking pneumonia the strength of the disease attacking the lungs, or the strength of the lungs resisting the disease? Is it only 'walking pneumonia' because the persons lungs are healthy enough to keep it from getting worse? Or is it that the specific type of pneumonia this causes is not very bad?
Couldn't find this in the FAQ-Thread, so I am asking here:

how was it possible to transfer the virus from bats/snakes to humans, asuming they've cooked this meat before and how was the virus able to survive in these dead/cooked bodies and how did it then get into peoples lungs? 

&#x200B;

(I put my food directly into my stomach and don't hide it somewhere else for later.)
I've heard tell of "permanent lung damage" in regards to this coronavirus. What exactly does that entail and is that an aspect unique to this strain of coronavirus, ie, was this also an issue with SARS and we just didn't hear so much about, or perhaps don't remember as prominently?
in terms of facemasks, do only surgical masks work? there's a severe mask shortage in my city right now, people are lining up at 4am outside pharmacies to get masks when they restock, they're being resold for exorbitant prices etc. those PITTA foam masks, and also reusable fabric masks such as [these](https://i.ebayimg.com/images/g/RTwAAOSws8NdQbDQ/s-l300.png), how effective are they?

edit for context, i live in one of the most densely populated cities in china (vpn is my friend)
With the initial 41 cases, it is said around 13 or 14 had no connection to the market, if true is it possible that the market was contaminated? Also i am seeing reports of the virus infecting cured people again?
I'm just wondering what will be done for people who have the virus. Is it possible that with proper medical care, the organism can get through it on its own? No cure exists yet and a possible vaccine is probably months away. So what will happen to them? Will they remain sick until such a vaccine exists or until they die?
[removed]
One very common point of contention about the nCov is the mortality rate. People cite its lower mortality rate compared to SARS. However, we are still fairly early in the breakout, and the infected can take weeks to recover or die. Meanwhile we have many more infected daily. Is it actually meaningful to compare with the mortality rate of SARS, which we have the final numbers, and the current mortality rate? Do we know what the mortality rate of SARS was at a similar point of the breakout, and how does it compare with the current number for nCov?
In [this](https://www.reddit.com/r/dataisbeautiful/comments/euokq5/oc_coronavirus_in_context_contagiousness_and/) thread from 4 days ago, the data seems to depict that the coronavirus is rather comparably close in infectiousness and deadliness to the spanish flu. Does this seem to be a realistic comparason of the disease that infected 500 million and killed an estimated 20 to 50 million people? 

I don't see any sources in there work but *is* there in fact any research that suggest this could be accurate.
Maybe it's a small and silly question but how to be with ordering goods online from China while the outbreak lasts? Is it better to restrain from it?
The common cold is also a Corona Virus. What makes 2019-nCoV different??
How effective would antiseptics be at decontaminating surfaces that may have been coughed on or something along those lines. Would something like rubbing alcohol sprays be effective at reducing risk of contracting?
[deleted]
The [livemap here](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) shows that there are people recovering..

But I also heard that there is not yet a vaccine available.

So how those people are recovering?
Where did the 2019 novel coronavirus come from? Did it really come from the unregulated sanitation of food of the "wet markets" in Wuhan, or was the 2019 novel coronavirus really man-made? China claims it started in the wet markets, others claim it was from a laboratory, is it even possible to discover the origins of this?

Some people have had coronavirus flu in the past. What makes this new flu so different? Did this novel virus mutate from the usual strain of coronavirus?
Is media overblowing this outbreak like those that occurred previously? In the past decade we have had 6 viral outbreaks and all have died out but not before media made such a fear mongering among the general population of the world.
I’m an undergraduate in a CoV lab studying the the innate immune response (PARPs and Viral MacroDomain) and we do infections with MHV in BMDM’s and count titers in HeLa cells. I’m just wondering if there is a decent cell-culture model for the Wuhan Coronavirus yet or if there is work being done to make one? I know mouse models be yet some time away as well (given the effort required to make a SARS mouse model, 15 some passages in mice?)
How long are people typically held in quarantine, specifically people flying back from China to their respective home countries?
[removed]
What puzzles me most of all is why there's so many ignorance out there. Seems like people don't know the difference between a bacteria and a virus. People actually believe this can only affect Chinese, can be transferred via an aliexpress package, is an escaped bio weapon or that China acted too late intentionally to let the disease spread.   
It's almost like nobody took biology in secondary education. 

&#x200B;

Anyone else think this is weird?
I'm surprised there hasn't been more discussion of:

**- Smokers -**  It appears smokers seem to have much more ACE-2 gene expression in their lungs for the virus to latch onto.  Would explain a lot.    Chinese men are some of the worlds worst (best?) smokers.  The data on SARS (and CorV2019) seems to hit men more than women.  As far as women ...well, 2nd hand smoke and what not.   Children don't seem to be getting hit so hard: a. they don't smoke.  b. haven't been around as long for genes to mutate/express (2nd hand smoke, air pollution, etc).  I've seen a little discussion regards to this, but not much in the main stream news/etc.

**- Air Pollution -**  I have no data/studies on this, but I posit the terrible air pollution there would cause the same effect to happen with ACE-2 "pathway".

**- Spitting -** That was/is actually a cultural thing in China ... and is/was a problem.  I'm sure the CCP will pass a bunch of new codes regarding public spitting. (IIRC, HK, Shanghi, Tiawan have...long ago...I maybe wrong on this).   Its probably not going to get discussed publicly or on a large scale because people will call it more "racism", and that's a shame.   But watch what the local codes and laws do with regards to public spitting after this is all over with.

**- Weather-**  It still "wintery".  Self explanatory.

**- Lunar New Year / Large Dense Population -** Self explanatory

The demographics would explain a lot.   The quick burst of cases.  The almost non-existent deaths in other infected countries (1-HK, 1- Philpns.), and/or lack of aggressive spread.   Other countries with not so great (not sophisticated) health care systems are not getting decimated.   This has been going on for a while now, and I would posit, been going on longer than we are/were actually aware of it.   Everyone is looking for the virus and any possible infections everywhere ....

Even in other parts of China, outside of Wuhan/Hubei, doesn't seem all that terrible.

So there you go.  A perfect storm of demographics and circumstances.

&#x200B;

[https://www.statista.com/statistics/236594/proportion-of-smokers-in-the-population/](https://www.statista.com/statistics/236594/proportion-of-smokers-in-the-population/)

[https://www.researchgate.net/publication/339049390\_Tobacco-Use\_Disparity\_in\_Gene\_Expression\_of\_ACE2\_the\_Receptor\_of\_2019-nCov](https://www.researchgate.net/publication/339049390_Tobacco-Use_Disparity_in_Gene_Expression_of_ACE2_the_Receptor_of_2019-nCov)

[https://www.heredg.com/2019/01/spitting](https://www.heredg.com/2019/01/spittingculture-in-china/)
[deleted]
Does it have a high chance of death? Or is that only for already unwell people?
what is the real potential for spreading the coronavirus from a contaminated person, that is, in a closed place, the 2019-nCoV can spread through the surrounding space and contaminate how many people and within a radius of how many meters? 
And...
what is the period of contagion from the moment the person contracts the 2019-nCoV?
Is it more or less dangerous than influenza?
This might be more of a sociological question, but why are so many people not concerned about influenza despite it also being a deadly disease for vulnerable populations? A person in, say, the United States probably doesn't have a huge chance of catching 2019-nCoV, but people catch the flu easily in the US (and probably every other country) and many die from it, yet the general public (at least here in the US) doesn't seem as concerned about the flu as they should be. What causes the difference in public response between 2019-nCoV and influenza?

Also, seeing as 2019-nCoV is pretty dangerous, if/when a vaccine comes out, do you expect that people opposed to the flu vaccine might choose to get the 2019-nCoV vaccine? It seems like many people who refuse the flu vaccine do it partly because they think that the flu is just like a common cold. But with 2019-nCoV causing so much chaos, people seem to be taking it more seriously.

Sorry, these weren't exactly questions about the disease itself, rather the public response to the disease. Thanks in advance for any answers you can give.
The live feed [map](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) I'm following says there are 24,505 confirmed cases, 492 deaths and only 900 have recovered. So does this mean there are still 23,113 sick?
Hopefully the fire hasn't died down in this thread, I've been meaning to ask this question for a long time. **Should the coronavirus be feared?** Why or why not? Please also site sources, thank you in advanced to those who answered.
With the current political climate in China and surrounding areas, it seems there is a lot of misinformation and just plain lack of information in the international medical and sociopolitical communities. It seems as if there are reports in the hundreds of thousands, but then China gives the international medical space little to no information, and censors a lot of info about the virus. 

Is it true that the true numbers of people in hospitals and infected is being grossly underreported by the Chinese government?
Who is at risk of suffering more severe complications from the virus? How lethal is it for a young, otherwise healthy person with no compromised immune system? Ist it actually more dangerous than the common flue?
Real talk, what happens once this virus hits India? They have like a billion people and some of the worst public health standards/hygiene in the world. I say that with no malice. Would it just run through the country like wildfire?
What is the surface survival time (outside of a host) for this virus? I know virus survival times can vary from hours to months in extreme cases, but thus could be an important factor in virus transmission from, say packages coming from China via distributors such as Wish.
If you think you or a family member might have it, even if it’s showing minor symptoms, should you call someone and seek quarantine? Or should you just stay home? If you should seek quarantine, who should you call? Your doctor? A local hospital? Or the cdc? In America, if that affects the answer at all.
Realtime Map of the Virus: 

[https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)

Realtime Cases of Coronavirus in the USA:

[https://www.cdc.gov/coronavirus/2019-ncov/cases-in-us.html](https://www.cdc.gov/coronavirus/2019-ncov/cases-in-us.html)

Sources: Johns Hopkins Ctr for Systems Sci and Engin.; and the US CDC; respectively.
What is the life time of the virus outside the human or animal body?
How can we help?

I have a program named [BOINC](https://boinc.berkeley.edu) installed on a couple of different platforms i.e PC and Android. I complete a couple of work units (WU) daily for [World Community Grid](https://www.worldcommunitygrid.org) using my processors idle time to help research diseases like childhood cancers, fighting aids at home etc.

Are there any programs/WU's out there for the average person to assist in researching a cure for cNoV. I know I'd be willing to donate my computers idle time. It may not be the latest gaming system. And I don't keep up to date with the Jones's regarding phones. But every little helps

If my post gives even one person the idea that they can help and complete WU's with their computers idle time. You're Welcome
I'm understanding that the big concern is that Corona will mutate and become more deadly. 
Can anyone elaborate on that theory and the chance it could happen?
Does the Coronavirus contain a  hemagglutinin (H) and neuraminidase (N) sequence like the influenza virus?

Is the R0 (R naught) known for the Coronavirus? (The average number of individuals who will catch the virus from one person)
FYI: LAB TESTING.  Your community or academic hospital (USA) will not have testing for novel coronavirus.  A sample must be sent off to CDC if there is strong clinical suspicion.  Do not expect to walk into the ER and expect to get a lab test done to diagnose/exclude this.
Last week I was in Shanghai shooting a commercial and was on busy streets interacting with people, including a few of my crew who fell ill but believed it was just a cold. The last 3 days the city was shut down for CNY so I didn't interact with almost any locals. 

When I got back to the States (6 days ago) my job refused to let me enter the building for 14 days and strongly suggested I try to quarantine myself at home and avoid contact with others. 

Do I really need to avoid all contact for 14 days? It's been 6 days since I left and 9 days since close public contact in China. How seriously should I restrict contact for the next week?  I'm still completely symptom free.
Thanks for doing this AMA. I'm struggling to understand why this outbreak is that significant considering that many more people around the world die of regular flu each year than we have confirmed deaths of 2019-nCov right now. Appreciate if you could explain the reason for the reaction that we are experiencing.
Why is everyone freaking out so much when only a few thousand people have been infected and only a few hundred have died? Doesn't the regular flu infect and kill way more people and we aren't freaking out about that, right?
In spite of of the most recent theory regarding the spread of the new 2019-nCoV through the digestive system how likely is it that the virus can mutate in such a way that it will quickly infect a large amount of a populous quietly before anyone even notices a particular way it could spread and how likely is it that a more aggressive and lethal strain could appear in general?
Right now the question it seems most often asked by medical personnel to sick patients is : Were you in China or around anyone who visited China? At some point that won't be the best indicator as in transmits from the initially infected to others on down the line who don't fit either of those categories as it spreads across the U.S. 

When we get to that point, where the virus has moved from person to person... what will be the questions providers will ask that will help distinguish these symptoms and the regular flu?

If medical personnel ask "Have you been to China or were around anyone who was?" and your answer is "no"... I feel many people will be like "Whelp... I'm good to go, I don't have this" and could further cause the spread or they will be told by medical personnel they are in the clear because they didn't say yes to either of the questions. Thanks if you can answer this!
I'm a healthcare provider myself; in reference to the CDC's criteria for patient evaluation, why would someone who was recently in China (but who did NOT visit Hubei province) and who has fever and cough but who are NOT sick enough for inpatient hospitalization NOT meet the criteria for testing for the novel coronavirus? Given the recent concerns regarding asymptomatic carriers, wouldn't these individuals still be at risk and therefore SHOULD be tested?

CDC criteria link: [https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-criteria.html?CDC\_AA\_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fclinical-criteria.html](https://www.cdc.gov/coronavirus/2019-ncov/hcp/clinical-criteria.html?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fclinical-criteria.html)
There have been reports of significantly higher than reported infection rates. Can you comment on how to determine the actual extent of the outbreak:

 [https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30260-9/fulltext](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30260-9/fulltext)
[deleted]
Given the fact that there is an outbreak of a "highly pathogenic" strain of Influenza A virus subtype H5N1 (presumably HPAI A(H5N1) given the way the media likes to talk about H5N1) in Hunan (one of the neighboring provinces of Wuhan), is this a dangerous combination of viral infections?

Some background:

A day ago, an epidemic of H5N1 was found in chicken populations in Hunan that had been present for an unknown amount of time. Promptly, 18,000 chickens were culled, but there is no guarantee that it did not spread to other populations off of humans or that zoonosis did not occur. This strain is likely a strain of HPAI A(H5N1), and we do not know the mutations that likely have occurred to make this strain unique. This means that, hypothetically, this could be an enzootic strain for humans that could be somewhat efficient in spreading.
Need an advice: came back from trip in vietnam 5 days ago. Flu like symptoms started yesterday. Had taken precautions while traveling like masks at the airport and hand sanitizer. Have called a couple medical organizations to seek for advice. Have been told by both to take fever medicine and not to rush to assumptions as it is more likely i have regular flu. Basicly wait it out or come to hospital only if state becomes worse. Is that what i should do or should i go to hospital and insist on some tests?
The cdc website doesn't recommend the use of masks in healthy individuals in the USA. What about someone in Hong Kong? I try to wear a mask in crowded places but due to the shortage I can't afford to dispose after single use. Would I be better off not wearing a mask at all? I imagine it gets pretty filthy by the time I get home at night.
Very late to ask, but maybe this will peek through. 

I've been seeing a bunch of videos that have a reoccurring theme online, and I don't know how honest they are, and was hoping you guys may have advice?

Recently there's been videos of Chinese citizens and doctors telling the camera the "truth" about the virus, some have said it's actually 50,000 infected, some have said 70,000, and one video even said 90,000 infected. They go on to say it's much worse than their government is letting out. And now there are a ton of videos of people seemingly falling in the streets, or seizing on hospital floors full of crowds.

How much of this should I take with a grain of salt, and how much is reality?
So far I have heard it was only a risk of life for individual with already compromised immunity (like old, sick and newborn), but after the news broke that the Chinese doctor Li Wenliang, one of the eight whistleblowers, have died, I'm now a lot more concerned. I am assuming he was a relatively healthy individual, seeing as he was a doctor. Is the virus becoming a bigger threat, or was the previous information I got incorrect?
Thanks, this is super useful. I made Coronavirus Tracker for travellers in Visa List as trying to finding information from different source was becoming difficult. It's data is updated everyday. You can see affected regions, stats, casualties. Do share this with your friends and colleagues and travel safe.

[Coronavirus Tracker](https://visalist.io/emergency/coronavirus)
Regarding Dr Li Wenliang-- This concerns me. He was DOCTOR. His entire lifes work was the Health business. Per the timeline, he had first mover advantage against this virus. He knew more, probably took more precaution and had more resources overall at his disposal than the average Chinese citizen. Many sources claim those that died were old and/or had preexisting medical complications, but this Doctor's death seems to challenge that claim in a very big way.
Hi. Why would the reported (official) death rate is around 3%? SARS was like 10% (800/8000), but it was final, right? On the other hand, for now it is around 700 known deaths and 2400 RECOVERED. Why would they count 700 deaths / 30000 infected? Shouldn't it be more rational to report it 700/2400 or around 30%? Those who are infected and not yet dead/recovered should not be counted for death rate, correct me if I'm wrong.
Hi. I am not sure if this is the correct place for asking this. I apologise if it isn't. 

I am supposed to fly from New Delhi to Dubai via Muscat  for work in about 36 hours. 
I had been slightly sick with a cold and a slight fever this past week. My fever did not cross 37.55C/99.6F at any point of time. Moreover, I am in a place(New Delhi) where there are no cases of coronavirus so I'm pretty sure that I don't have coronavirus. 
I am still not fully recovered and still have a slight cold and cough. 

Is it okay for me to travel or will I be at risk of being put in quarantine at any of the airport's as I'll have symptoms of cough and cold?

I'm not really sure how different countries are securing their airports. The only thing I could find was the use of thermal scanners to check for temperatures above 38C.

TIA
I hate the idea of being this guy, but...how serious is this really? As I understand it, there's only been around 200 deaths and most of those being elderly. I could understand if we were talking about thousands of deaths, but this doesn't seem like ebola. I'm wondering if this isn't in some ways an overreaction. And that worries for the next time for a more serious effect, I'm thinking the boy who cried wolf.
[removed]
Japan evacuated 206 people in their first flight. 3 of them tested positive. It's \~1.5%. What can we say from this percentage? I know we cannot draw the conclusion that 10 million in Wuhan has 150k infected. But it shouldn't be too off, isn't it? Wouldn't 100k to 200k be a reasonable estimate? (I'm aware the published positive cases is \~11k).

&#x200B;

[https://www.reuters.com/article/us-china-health-japan/three-japanese-evacuees-from-wuhan-test-positive-for-virus-two-had-no-symptoms-idUSKBN1ZT02D](https://www.reuters.com/article/us-china-health-japan/three-japanese-evacuees-from-wuhan-test-positive-for-virus-two-had-no-symptoms-idUSKBN1ZT02D)
The short answer is:

A. They get water from their food, and avoid salty food

B. They may have modification to their kidneys to allow them to excrete more salt

C. There a lot we don't know, marine animals are hard to study



[Source and more details](https://www.scientificamerican.com/article/how-can-sea-mammals-drink/)
[removed]
I have not seen a very comprehensive answer here, so allow me to copy from "Marine Mammals Evolutionary Biology" by Annalisa Berta et al. I worked closely with her and she is one of the foremost experts on evolution and systematics of marine mammals. I will add personal clarifying results in [brackets].

>Most marine mammals are hypoosmotic; their body fluids have a lower ionic [salt] content than their surrounding seawater environment, and they are constantly losing some water to the hyperosmotic seawater in which they live.

>Marine mammals obtain the water they need from the food they eat: preformed water in their diet [as previously mentioned in this thread] and subsequent metabolically derived water [not previously mentioned]. Most fish and invertebrate prey consists of 60-80% water, and the metabolism of fat, protein, and carbohydrates provide metabolic water during the digestion of food. It has been shown experimentally that seals [also referred to as phocids] can obtain all the water they need from the food they eat. If seawater is given to seals, the stomach becomes upset and the excess salts have to be eliminated using body water. 

>Despite this, seals occupying warm climates have been observed drinking seawater (King, 1983), a practice called mariposia. It has been suggested that intermittently consume small amounts of seawater at intervals that would not be enough to cause digestive problems, but would be sufficient for facilitating nitrogen excretion [peeing]. Mariposia is especially common among adult male otariids [sea lions and fur seals] (Riedman, 1990)

>Why do pinnipeds [seals, sea lions, fur seals, and walruses] drink seawater? Gentry (1981) noted that most of the otariids observed ingesting seawater live in warmer climate and lose water by urination, panting, and sweating. He suggested such water loss , along with prolonged fasting by territorial males, may be severe enough to promote the drinking of seawater. The behavior may play a role in nitrogen excretion by supplementing water produced oxidatively from metabolized fat reserves. Mariposia has also been reported for Atlantic bottlenose dolphins, common dolphins, and harbor porpoises.
I'm getting tired of typing, but theres about one more page to go. I'll type it up if anyone wants to read more


TL;DR
1. Eat high water content food
2. Metabolically produce water from digestion and fat reserves
3. Kidneys stronger than desert animals
https://www.scientificamerican.com/article/how-can-sea-mammals-drink/

It's actually not established that sea mammals drink large amounts of salt water. So they get it from consuming food and peeing out far larger amounts of salt in their urine than humans do.

A theory is that their longer henle's loop allows better processing of salt water via their kidneys as well. 

Excellent question.
[deleted]
Marine animals produce highly concentrate urine (get rid of the excess salt). Some also have glands around eyes, nose, etc. that excrete a highly concentrated salt solution. Freshwater animals actually have the opposite problem, in which they are lacking in salt intake. They make up for this by consuming sodium rich food and producing extremely diluted urine (to retain salt). 
Renal physiology describes the sodium gradient through the kidney as what dictates how salty one can drink water. That is why the Australian hopping mouse can drink water 10000x saltier than a humans. Basically the cells in your kidney act as a semi permiable membrane and exchange salt, water, and other electrolytes. The concentration of sodium on the inner renal side of that membrane dictates how salty the water one can drink without becoming dehydrated. Marine mammals just have a different renal sodium gradient than humans.
They key is the maximum concentration of solutes in urine that the kidney can produce. For humans, that's around 17 parts per thousand. Sea water is on average 35 parts per thousand salt, roughly double the maximum concentration our kidneys can produce. So if we drink seawater, our body has to pull extra water out of tissues just to get rid of all the salt that came with it. This is why drinking seawater results in a net loss of water for humans.

If an animals kidney is capable of producing urine that with higher solute concentration than seawater, then that animal can drink seawater just fine. The kidney of a kangaroo rat, for example, can produce urine with nearly ten times higher concentration of dissolved solutes than a human kidney is capable of. So the kangaroo rat could theoretically drink seawater with no problem. It's the same with marine animals.
A small fun fact that I haven’t seen posted yet, is that when looking at the marine iguanas on the Galapagos, we see them shoot snot rockets containing excess salt acquired from swimming in the ocean. Their body is able to retain the water needed to be hydrated, and removes the excess salt. 
Animals either osmocomform (maintain an internal osmotic environment similar to their external environment), osmoregulate ( regulate the osmotic pressure of an organism's body fluids), relocate to another area or die. 

Now marine species, like seagulls, fish and fiddler crabs, among many others, have adapted to these salt stresses. Seagulls have small glands above their nasal passages that secretes excess salt through their nasal passages. Fiddler crabs osmoconform by either absorbing salt into their tissue (for high salt concentrations) or by excreting salt (in low salt concentrations). Now some species can move to different areas where the salinity fits their needs, like marine birds and alligators, who create their own little alligator holes that they live in to ride out the changing conditions. Others, like fiddler crabs and fish, often cannot move and, if changes occur to quickly for them to adapt, they die.
I am curious what advantage there would be to losing the ability to process salty water. Would it improve retention in arid environments?
In a kidney, mammals and birds (kinda) have nephrons. Theirs are used in filtration and excretion. The mammalian kidney has loops of Henle on their nephrons which allows us to concentrate our urine above our blood concentration. For saltwater mammals, they have much longer loops of Henle than we do. Also most of them have other special traits such as a salt gland for a shark who doesn’t have loops of Henle ( not a mammal but just as an aside )
-just finished this part of my bio major
Also, birds who feed and live around salt water have salt glands just behind their eyes that will actually pump out salt from their blood and they can “sneeze” out the extra salt. They’re like minor, specialized kidneys. 
Penguins have a gland that filters the salt from their blood, well actually pretty much any seagoing bird does. Called the supraorbital gland it basically secretes the salt out through the nasal passages.  So they can drink seawater & secrete a salty solution that is saltier than seawater out their nose thus removing the excess salts. 
The salt content in the blood of most mammals is equal to that of sea water. The excretory systems of most ocean mammals have evolved to process and handle the salt content of sea water (they essentially pee it away). For most land animals, the only source of salt comes from food so our excretory systems don't have to be as efficient at eliminating as much salt. 
Is it possible to take a supplement of sorts to assist in the excretion of salt ingested? If so, why don’t we use that with sea water? Has there been an efficient process for removing the salt content of sea water?
Most marine mammals obtain most of their fresh water from the food they eat! Fish are very good at flushing salt from their systems and leaving the freshwater in their bodies which marine mammals eat. Imagine if you ate watermelon all day, you probably wouldnt get very thirsty because the watermelon has the water content you need to stay hydrated.
Human kidneys (and that of most land animals) were designed overall to retain salt. i.e. to "take the sea with us". Their job primarily is to keep salt content high relative to most intake. Sea creatures don't have this need and therefore don't have the same renal physiology. Interestingly some sea fish like the bull shark can alter their renal physiology to retain salt to go into fresh water. 
The carnivorous marine mammals prefer fattier prey. The more fat the prey has, the higher the water content of the animal. Herbivorous marine mammals do sometimes have to go to fresh water sources to get fresh water. For example, manatees go up rivers and streams quite often to get fresh water since sea plants have higher salt content.
The concentrations in human cell is what causes us to not drink it. Are cells concentrations are different than other animals. since our concentration is lower, thr high concentration water makes the water in our cells be in a sense "sucked out" through a process called diffusion. Since whale cells have higher concentrations, their cells arent killed and they can use that water.
Because the dollar store readers are not made to correct hyperopia (“farsightedness”); they are made to correct presbyopia (the loss of focusing flexibility that hits all of us in our early 40s). Presbyopia occurs in all of us in a relatively equal way, so making a standardized power for presbyopia is easy.

Basically, the cheater readers are making the assumption that the wearer has perfect distance vision, and simply brings the focal point forward to a comfortable reading distance.

Now, plus power lenses that correct for presbyopia also happen to help with hyperopia. However, unless your hyperopia just coincidentally happens to be equal between your eyes, free of astigmatism, and of a small enough amount, the readers are only partially correcting it. It may be better than nothing, or even good enough for practical use in many cases, but they do not usually fully or adequately correct the hyperope’s vision.

As far as myopia (“nearsightedness”) goes, its generally too unique to the individual to standardize in a “drug store reader” kind of way. Plus if people are self-diagnosing/correcting myopia, they almost always tend to overcorrect it, making them prone to eyestrain, headaches, and if they are young enough, a worsening of their prescription. In fact a huge part of the refraction procedure (“one or two?”) is making sure the patient hasn’t overcorrected themselves.

Source: I’m an optometrist
Nearsightedness needs to be corrected precisely so that objects at infinity are in focus. Each eye may need a different correction and there may be astigmatism as well. Farsightedness just needs to be corrected for a comfortable reading distance.  A limited analogy is that it is like buying and using magnifying glass vs  a camera or projector lens.

Edit: An optometrist's explanation is here 

https://www.reddit.com/r/askscience/comments/d26nwr/why_do_nearsighted_people_need_a_prescription_and/ezt656x/
On a side note, it's a monopoly.  


"In 2019, LensCrafters founder E. Dean Butler spoke to the [*Los Angeles Times*](https://en.wikipedia.org/wiki/Los_Angeles_Times),  admitting that Luxottica's dominance of the eyewear industry had  resulted in price markups of nearly 1,000%. In the interview, Butler  noted "You can get amazingly good frames, with a [Warby Parker](https://en.wikipedia.org/wiki/Warby_Parker)  level of quality, for $4 to $8. For $15, you can get designer-quality  frames, like what you’d get from Prada.” When told that some eyeglasses  cost as much as $800 in the United States, Butler remarked, “I know.  It’s ridiculous. It’s a complete rip-off.”  


[https://en.wikipedia.org/wiki/Luxottica](https://en.wikipedia.org/wiki/Luxottica) \[47\]\[48\]
One thing that is being overlooked is that monopolization of prescription eyeglass companies as well.  This also drives up the cost.

For reading glasses while reading a book, if you cannot see clearly, you can simply move the book closer or farther away.  This is not possible with a street sign.  So, reading glasses can be standardized.  It is also a different kind of deficiency.
Something no one here considers: your eyes can vary from each other in visual acuity. I am far-sighted, but my right eye is far worse than my left eye. I have to get wonky prescriptions that are absolutely not from the dollar store.
Since we're talking about it...  my optometrist just kicked me in the head.   Glasses wearer for 20+ years, and I just learned something new.  When you're doing the eye test, don't strain.  When the doc says, #1 or #2", don't squint.  Relax your eyes, otherwise you'll be squinting when you're wearing your glasses.  Kinda defeats the point,  huh?
[removed]
If you go to Asian countries, a lot of them have ready made myopic spectacles as well.

Also one aspect I haven't seen mentioned here is that it's very hard to over correct yourself when picking out your own hyperopic Rx (and even then itll just cause some eye strain), while it's very easy to pick something too strong if it's for myopia and hence risk more progression. -Optometrist.
One detail most answers are missing is: a lot of nearsighted people also have astigmatism.

And astigmatism is not only a diopter, it's a diopter and an angle. So you can't have a "few sizes fits all" (this is kinda annoying for contact lens wearers for example.) 

Let's say you could be off 0.5d and it would be ok (it wouldn't) so you could have 16 models of glass for farsightness.  Now, on top of that, add 16 (diopters) * 8 (different axis) (= 128) possibilities for astigmatism (again, this is an underestimated number) FOR EACH myopia option.
So, most of the answers are correct in a way. i.e. you can't buy farsighted glasses reliably off a shelf for various reasons.

But, the reason your glasses cost $300 is a completely different story.  The majority of the market is dominated by one manufacturer, which also owns a majority of the retail outlets, which also (90% chance) runs your vision insurance.  So, they making the glasses, in some cases the lenses, administering your insurance, and own a lot of the retail places where you're buying glasses at.  Essentially, at any point you enter the product world of glasses you're getting screwed by a huge monopoly.  The other piece of this is a lot of these retail centers the people there actually earn commissions.  Those lens upgrades, how many different companies produce anti glare? Scratch resistance?  They're selling you options that you can't really see and unless you need transition lenses don't really need. I recently had lasik surgery but before that I would order frames/lenses with no coatings and not have issues ($17 dollars was what i paid after my last exam & pair of glasses).  There are also online manufacturers like Zenni optical where as long as you have an up to date script you can get a pair of glasses shipped to you for 30-40 bucks.

Their brands include (I"m talking about luxottica here)

Eyemed (insurance, and often where other insurance companies are getting their administration through).

Okley, sunglass hut, lens crafters, person, oliver peoples, pearle vision, target optical, ray ban, eye care plan of america, [glasses.com](https://glasses.com).

They merged with a large lens manufacturer in 2017:

[https://www.nytimes.com/2017/01/16/business/dealbook/luxottica-essilor-merger.html](https://www.nytimes.com/2017/01/16/business/dealbook/luxottica-essilor-merger.html)

More reading if you're interested:

[https://theweek.com/articles/784436/secretive-megacompanies-behind-glasses](https://theweek.com/articles/784436/secretive-megacompanies-behind-glasses)

[https://www.forbes.com/sites/anaswanson/2014/09/10/meet-the-four-eyed-eight-tentacled-monopoly-that-is-making-your-glasses-so-expensive/#89a54256b66b](https://www.forbes.com/sites/anaswanson/2014/09/10/meet-the-four-eyed-eight-tentacled-monopoly-that-is-making-your-glasses-so-expensive/#89a54256b66b)

John Oliver also mentions them:

[https://www.youtube.com/watch?v=00wQYmvfhn4](https://www.youtube.com/watch?v=00wQYmvfhn4)

&#x200B;

And that's my rant, thanks for coming.
[removed]
[removed]
[removed]
Farsighted people still need a prescription. 

&#x200B;

The glasses you buy at the dollar store are useful mostly for people with no refractive error, but they are beginning to have trouble with near vision (presbyopia). They have the same correction in both eyes, with no astigmatism correction and in set values (usually from +0.75 to +3.50).
[removed]
[removed]
[removed]
Finally something where I can contribute.

Every person has a certain range of distance where they can see things sharp. For someone with "perfect" eyes the furthest point is infinity. Their eyer are relaxed, and they focus stuff far away perfectly. The closest point they can see sharp (when they strain their eyes really hard and "squish" the lens) is around 15 cm from their eyes, but it gets further away with age. This is by the way why old people are far sighted.

So what do glasses do? In their simplest case, they just shift those points. But of course they shift both points! So they pull them closer to you, or push them further away.

Now what about farsightedness and nearsightedness? You can imagine those conditions as shifting of the two points described before. One or both points can be shifted. Let's look at common possibilities:

\- If you are a "normal" old person, your closest point just got further away. This means your range is smaller, but you still recognize faces at the distance. When you want to read something, you need to move your close point closer to your eyes, so that your reading material is in your focal range. But since you will not be able to see things far away with glasses anyway (your far point also moved away from infinity, and is now somewhere around a couple meters away from your eyes) it doesnt really matter how far you shift.

\- If you are a "normal" nearsighted person, then typically your lense is squished, so that both your points are closer to your eyes. What you want to do with glasses, is to put your far point at EXACTLY infinity. This way you can relax your eyes, and see stuff far away. If you move it "beyond infinity", then your eye has to work all the time while you are walking about in your glasses and focus stuff. This will give you a headache.

\- (The farsightedness where the lense is deformed can also happen. Then your far point is beyond infinity. It is really hard to notice though, because your eyes can just focus. Its just they are never relaxed. So people only get their eyes checked after headaches at school or something. Those people also need expensive glasses.)
[removed]
[removed]
Dollar store readers can damage your vision further by applying an inaccurate sphere to the lenses potentially straining your eyes. Beyond this, they have no cylinder correction exacerbating issues brought on by astigmatism.

Even if you're farsighted, you should still have your readers prescribed because they'll be made for your eyes and not be just a basic magnification.

Source: I'm an optician.
[removed]
The simple answer is that due to the distances they work at, it is fairly easy to correct a minor error in prescription for reading glasses/glasses for far sightedness, and virtually impossible to correct an error in prescription with distance/glasses for near sightedness.

To explain in a bit more depth, when we need glasses, it is when our eyes are unable to focus correctly. When we wear glasses, they project the image of what we see on front of us, and purposely misfocus it slightly so that when the glasses and our eyes are combined, the misfocus cancels out and we can once again see clearly.
Glasses are pretty simple devices though, and only focus correctly at one distance - so for someone that is longsighted and needs reading glasses, they will be designed to focus at normal reading distances. For someone near sighted that needs glasses to see far away, they will be designed to focus far away.

If you wear the wrong strength glasses, what this means is that they are focussing at the wrong distance. This can be compensated for however by changing the distance of whatever it is you are looking at (compared to your eyes) - so by holding your book slightly further away or slightly closer, you are counteracting the error.
With distance vision however, you cannot really change this distance easily - if you are looking at a mountain a kilometer away, you would need to move a huge distance to bring it close enough to offset the error. 
To use some numbers, if you need to change the distance between your eye and the object by 10% to correct the error in prescription, holding something at arm's length may mean moving your book by 5cm, which you do without realising. If you need to change the distance between your eye and an object by the same 10% for something that is a kilometer away, that means moving your eye by 100m.

So the end result is that reading glasses can be sold in rough grades, and rely on the fact that people instinctively adjust what they are doing to correct any erroe. Distance glasses cannot be adjusted, so need to be fine tuned to work exactly for your eyes.
It's easy to note that the situation isn't symmetrical. Every aging person will develop longsightedness which will progress in the same, predictable way. Therefore a ton of people need the same kind of glasses. This is a great target for a mass product.

If a younger person needs glasses, it's because they have a condition with their eyesight. Those conditions can be of all kinds, resulting in all kinds of complex prescriptions, usually different between the eyes. No economies of scale here.
Lenses are not always that expensive, and requiring prescription for glasses seems to be just an American thing. Now, of course you need to know what strength the lenses need to be, but in most places there is no need to have a valid prescription.

For comparison, I have two (ugly) pairs of glasses for my nearsightedness that were 5€ each with the same lens strength as my main glasses. I suspect the main reason for the price is that people want fancier frames for "everyday" glasses, while reading glasses are worn only sporadically and thus can be uglier.
just imagine the field of vision. Much easier to throw a generic, simple solution at something two feet from your face. Pretty difficult to solve the human eyeball struggling with that bottom row of that eye chart

edit: I like that this is tagged engineering. This is an engineering answer
[removed]
Prescription glasses cost so much because one company (luxotica) basically has a monopoly on eye glasses and insurance plans shield many consumers from the price gouging.  Same reason prescription drugs cost so much in the US.  

The entire process, from checking your eye to a finished prescription lens, could be automated and done for a few dollars in materials.  Frames are actually worth quite a bit more than lenses because they do still generally involve some manual labor to assemble.  They're still marked up by a huge margin for the same reason diamonds are expensive.  Because a marketing company spent a bunch of money a long time ago to convince people that glasses are face jewelry and therefore worth hundreds of dollars when in reality they are not hard to make and only contain a few dollars worth of materials.
[removed]
[removed]
[removed]
[removed]
[removed]
To put things in context, I am mildly far sighted, so my vision is very good without my glasses, so much that even without my glasses I can see details that many people just can't, but wearing glasses mean I don't bang my head on the walls to make the migraine go away. It's so mild that I got diagnosed quite late (13 years old). I'm lucky in that I don't need to wear them all the time (only when I read or watch TV), but I still wear them when I drive as I feel more comfortable.

I wouldn't even dare trying reading glasses you can find in a supermarket. If you need eye correction, you need to see an optician as these glasses will be much more comfortable than cheap reading glasses. Plus they can diagnose astigmatism which requires correction as well. And you definitely need to have anti-glare, otherwise you'll see the reflection of your own eyes in the lenses. It's less obvious with modern synthetic lenses than it was with glass lenses, but you'll constantly be under the impression that your lenses are dirty (the reflection is fuzzy since it's very close to your eyes and out of focus).
[removed]
[removed]
When wearing readers, If you’re holding your phone or a book, you can adjust the distance slightly to work with whatever generic correction is in those lenses.

When wearing distance glasses you have to be able to adjust your focus on all objects near, intermediate and far. This is done by using the Rx from the doctor and the way the lens is made.

Regarding the price, you can buy $300 readers just like distance glasses. You just can’t purchase cheater distance glasses.

Regarding how much they cost, no one is putting a gun to your head and forcing you to spend a lot of money on glasses. If you want cheap glasses, buy cheap glasses.

People don’t hold Mercedes and BMW responsible for selling expensive cars and say it’s reprehensible for charging that much. They make a superior product and people that can afford them, like to drive them.

The funny thing is, you never hear people say, “I bought a Kia for a quarter of the price of Audi and it does the same thing, gets me from point A to point B. I don’t understand these people that waste so much money on a car.” But they make that exact correlation when it comes to glasses.
[removed]
[removed]
[removed]
[removed]
The short answer is no. The long answer: Our immune systems aren't like muscles that need to be worked out to grow strong. By the time we reach young adulthood, we are (under normal circumstances) exposed to so many microbes, pathogens, spores, allergens, etc that we have very robust immune responses, even to things we haven't been exposed to. This isn't necessarily because we have the antibodies for every disease we've been exposed to being produced and floating around our bodies at all times as "strength" would suggest. Rather, our immune system maintains "the plans" for those antibodies so if we are re-exposed to a pathogen or encounter one that's similar to one we've had in the past, our bodies can quickly and efficiently drum up the antibodies necessary to kick it to the curb.

Coming into contact with a pathogen triggers an immune response whether we've been exposed to it previously or not (hence why novel covid-19, for example, didn't have a 100% mortality rate), but this won't make your immune system stronger because that's not how immune systems work. They don't strengthen, they diversify. As a result, low exposure to pathogens in 2020 due to social isolation doesn't weaken or atrophy immune systems. But we don't need constant exposure to different things for our immune systems to be able to run immune responses.

That's not to say that social isolation doesn't have an effect. There's research to show that immune responses are diminished in individuals experiencing stress, loneliness, anxiety, and other negative psychological stimuli. An aggregate of 148 different studies found that people who were more socially connected had a 50% lower mortality rate. One experiment even found that people with many social ties are less susceptible to the common cold which is another coronavirus. This is not a direct effect however, it's indirect.

And almost all bets are off with influenza. As other commenters have mentioned it's going to be very difficult to create an effective flu vaccine for the upcoming season due to the gap in case data from the prior year. Flu almost always jumps from animal populations to human populations on a regular basis so the virus is "novel" very often. Again, we have some ability to mount an immune response to it, but influenza continues to maintain a high mortality rate given how commonplace it is.
[removed]
My institution (major midwest hospital, \~20-30k employees, 800+ bed main hospital and multiple 100-200+ bed satellite hospitals) **has not had a single positive test of the flu since \~mid-November.**

To highlight, in about September we switched to all COVID tests would be combo COVID/Influenza tests to see how much co-infection was occurring. Now, because we literally have no positive influenza tests, the default will now be COVID only.

To put this in perspective, it's like all auto shops in the state of Michigan all of a sudden started saying "no one's engine oil is wearing out anymore, so we don't need to do engine oil changes until next fall, only transmission fluid changes for now".
I don't want to alarm you OP, but your home is not sterile, you have been exposed to plenty of germs unless you are a bubble boy/girl.     

What you are asking about OP, is the [Hygiene hypothesis](https://en.wikipedia.org/wiki/Hygiene_hypothesis).  This is a complex question and we still don't really have a clear answer, there is evidence that this may be important in children during the development of the immune system, but no evidence that I can see that it would affect adults with developed immune systems.
I’m no doctor, but this exact question was asked at a Q&A With a panel of doctors. Their response was that The notion that we build stronger immune systems due to exposure to germs is entirely true. But it is primarily during our formative years. As an adult going a year or more without fighting germs has no noticeable effect on our immune system.
[removed]
There was interesting article about how everyone socially distancing, wearing masks and washing hands regularly due to covid actually stopped the spread of other common viruses such as flu, which in turn stopped the virus to mutate with newer strains as it usually does every year.
The immune system is very flexible and adaptive. Some immunity can last a lifetime, depending on how aggressively the pathogen in question evolves. It is unlikely to cause any public health issues. But if you have compromised immunity all the usual precautions apply.
Individually, our immune systems will be fine.  But as a community,  we are in for a rebound effect of endemic illnesses.  Large future outbreaks are being predicted of the normally circulating viruses that have been suppressed by the social distancing over the last year. 

https://www.pnas.org/content/117/48/30547
Most of the time, your immune system is tuned to its environment.  Once you get out of your environment and get exposed to other people and the diseases *they’re* immune to, there’s a problem.  I could point to things like the 70-80% of native peoples in North America that died to exposure to European diseases when the Scandinavians arrived (leading to later colonists seeing fields of cultivatable plants waiting for them, as if by providence—and of course, the diseases *they* brought wrecked the survivors of the previous centuries’ outbreaks)...but that’s an extreme example.

A better example would be the disease seasons you typically see in colleges at the beginning of the school year, particularly in service academies.  At Texas A&M, we referred to it as “the Corps Crud,” mostly because—as I’m sure you can imagine—it seemed to affect the Corps of Cadets more than non-regs.  It’s not a seasonal disease in the traditional sense; just a mash of diseases that hits everyone because they’re coming from all over the state (or country, or world).  

That’s what I imagine the end to social distancing measures being, anyway: a whole country coming down with the Corps Crud.
i would assume no.

why? because the rate at which a species can mutate is influenced by its population. if there are 1/20th as many flu cases as last year, the flu is likely to mutate much slower.

the immune system isn't a muscle that needs to be worked out, its more like a program that needs to stay up to date with the latest updates, but that isn't a problem when no new updates are coming out.

the "silver lining" to this covid pandemic is that the countermeasures employed against it also work against the common cold and flu (not the vaccine, just the masks, handwashing, social distancing, etc), and by limiting the spread of cold/flu, we limit their ability to mutate.
[removed]
When it comes to viral infections: the memory of that virus doesn’t last long and so our immune cells forget (aka virus changes). The only reason our immune system would benefit to exposure is to remember the infection: such as bacterial infections with non-life threatening adverse effects. Then, the exposure should be slow in that the hospitals won’t be inundated. Besides this, there is little increased risk to our system due to non-exposure to microbial infection
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
We use moles instead of mass since it accurately shows how many molecules of a substance we have. The chemistry behind reactions is dependent on the number of molecules present, not their mass. 
To put more simply, it's more important to know many ingredients you have for making a hamburger, then it is to know how much the ingredients weigh. It's more important to have two buns instead of just knowing you have 100g of buns.

Edit: Forgot to mention that the OPs question is not stupid, and is completely reasonable. As some others pointed it, it would be a good opportunity for the teacher to emphasize the importance of moles vs mass.
You did not ask a stupid question. When trying to understand these conventions of science, you pretty much can't ask a stupid question. In fact, I would argue it was an **important** question, and the teacher wasted an opportunity to stress the usage of the mole to the class. 

The mole refers to a number of things, just like a dozen. You can have a dozen eggs, but also you could have a dozen molecules of caffeine. You could have a mole of caffeine, but you also could have a mole of eggs. This is important because chemistry cares more about the *number* of molecules than the weight of those molecules. 

Furthermore, consider the following balanced equation: 2(H2) + (O2) -> 2(H2O). Given 2 moles of H2 and excess oxygen, you know you can produce 2 moles of H2O. Using moles allows us to compare the actual quantity of molecules, whereas with weight it would be difficult to compare in such a neat fashion. Given 200g of H2 and excess oxygen, you have to do some annoying math to first convert to moles, then convert back to grams. 

Mass is, like you noted, more useful because it's easier to measure. You weigh chemicals with mass because it's easier, and because we're capable of converting to moles. That said, it's not uncommon to have percentages which are based on weight. Mass by mass, mass by volume, and volume by volume (m/m, m/v, and v/v respectively) are all common, with the first being solids in solids (e.g. alloys), the second being solids in liquids (e.g. solutions), and the third being liquids in liquids (mixtures and some solutions). 
Your teacher is a dick. It's a great question to ask.

Moles allows you to do calculations with the actual number of molecules.

There's a reason why people don't go around saying "I need 200 kg worth of guys to help me out here!" It wouldn't be very practical to use mass in that situation. However, saying "I need three guys here!" makes total sense.
Wow that's a really bad thing for a teacher to say.

edit: thought this was AskScienceDiscussion, I have violated a central rule of my own subreddit.
Not stupid at all. This is a concept which should be drilled a billion times in high school. Its not that obvious a thing to know moles make sense over mass. Teacher should be happy a student wants to understand the concept. 

But like others explained, in chemistry, it's more important to know the amount of molecules you have in hand rather than the mass. Hence the need to use moles. 
[removed]
In chemistry you care about the number of units that react. Not about their weight. 

It doesn't matter that 2 pounds of apples react with 4 pounds of oranges. You are interested in knowing how many apples react with one orange, or vice versa. And then maybe calculate the mass of apples and oranges that reacted/you'd need to have beforehand.

No question is a stupid question. Maybe you don't know the answer to a very obvious question, but that's part of the learning process. 

Don't feel ashamed for your question. You did the right thing, you didn't know, and you asked who you thought would give you the right answer. 


--- --- --- × --- --- ---

Edit: Answering to the reply of this comment:



The number of molecules is directly related to the substance mass, but it is molecule/atom/substance dependent.

The number of molecules in 3 grams of apples and 3 grams of oranges aren't the same. Think of it as dozens of regular chicken eggs vs ostrich eggs. Yes, the number of units in a dozen is equivalent, but the masses aren't! 

If you asked someone for one pound of regular eggs and one pound of ostrich eggs, you wouldn't receive the  same number of regular & ostrich eggs. You'd get many less ostrich eggs than regular ones. Same with chemical compounds and elements. They don't weight the same. So you want to define a standard amount of units, the mole, to express effortlessly a standardized number of units, instead of using masses!


In chemistry you are always interested in the number of  elements (element, as in, a single element in a bunch, not as in a chemical element) reacting/interacting, not their mass. Mass is a particular characteristic of the reagent/element/compound/molecule, whereas number of units is constant. 3 moles of water have the same number of particles as 3 moles of aspirin, 3 moles of cocaine and 3 moles of CO2!
As a teacher, I'm really sorry that one of yours would say something like that to you, or any student. It sounds like he truly doesn't understand the amazing opportunity he's been given by getting to spend time with you all every day--and he doesn't deserve it. Yes, there are a lot of things about the profession to be frustrated with, but a student taking the initiative to ask a thoughtful question, is far from one of them.
If you’re making a car you need to have 4 wheels, 1 windshield, 1 engine, and so on. I could say you have 1000 kg of wheels and let you figure out how many that is and how many cars it will make or I could just say you have 400 wheels. Eventually you’ll have to convert weight to number of wheels to know how many cars you can make. 

The mole is simply a measure of the count of something, no matter each unit’s mass. A mole of wheels would mean you could make 0.25 moles of cars. You don’t need to convert back and forth to mass to make that simple observation. 

So it’s not a silly question and there is a simple explanation. 
Absolutely not a stupid question. Grams are definitely important in the process, but in order to have an accurate ratio of reactants in a reaction, grams needs to be converted into moles. This is due to the fact that molecules have varying weights. For example, say we want to make sugar water with 1 molecule of sugar for every 10 molecules of water:

&#x200B;

Sugar/Glucose/Fructose is:

6 Carbons \*  12g/mole = 72g/mole

12 Hydrogens \* 1g/mole = 12g/mole

6 Oxygens \* 16 g/mole = 96g/mole

So 1 mole of sugar = 180g/mole

&#x200B;

Water is:

2 Hydrogens \* 1g/mole = 2g/mole

1 Oxygens \* 16g/mole = 16g/mole

So 1 mole of water = 18g/mole

&#x200B;

So the mass of water and sugar if we wanted to make the 1:10 sugar water mentioned above would be:

10 moles of water = 180g

and 1 mole of sugar = 180g

&#x200B;

Hope that makes sense. I'm a senior Polymer Engineering student if it matters, so this is the type of stuff I look at pretty often. Happy to answer any additional questions

Edit: whoops, corrected a careless error
[removed]
Lots of good examples of why moles are important here. You can sort of think of it as concentration. If you are experimenting with buffers and have 1M sodium chloride, and you might need to change to 1M potassium chloride.  They have different molecular weights, so mass doesn't really help you.
Chemical reaction is like basic cooking recipe. You take some molecular from material mix some with another, put some heat and bam you have some products. To use molecular quantities is the most efficient/largest yield of product - because the main purpose of business - make the most of less. So if you take everything in mass, some of your molecular which were more will be left unreacted and your product will be contaminated with reagents. Also you can write recipe buy masses and you will do when you make something the same and most of the time, but molecular recipe of masses is hard to remember complicated - atoms are light. So instead of calculating by masses we take some quantity of atoms where recipe follows only the smallest of reactive atoms/molecular. So when you know smallest amount how they act, the same will go with 100, 10000 or 6.02e23 which is basically a mole and we can easily figure out each type of atom/molecule mass, because iron atom larger and heavier than oxygen thus their moles weigh differently.

And your question is really good, you are reasoning your world and you should do that for anything. 
Imagine you have a box full of cupcakes. You know how much it weighs, but you don’t know how many cupcakes there are inside. So if you are going to a party and trying to figure out if you have the right amount of cupcakes, saying you have 1kg of cupcakes isn’t helpful, right? The unit “moles” tells you how much one cupcake weighs, so you can weigh it and figure it out how many cupcakes you have. 
Stoichiometry is how many cupcakes each person will eat. 
When looking at chemical reactions, which is a big part of the study of chemistry, the reactants and products are usually considered in terms of quantity of atoms/molecules, and not their masses.

For example, when we consider the burning of hydrogen, we write that two molecules of H2 and one molecule of O2 react to form two molecules of H2O (water). 

Similarly, when we burn one molecule of ethanol, this sort of numeric accounting will tell us the number of carbon dioxide molecules released as we work out the number of atoms/molecules required to balance the reaction.

We *could* also work in units of mass, because the two are interchangeable, but it would be unpleasant to have to whip out a calculator for every little step when working with chemical equations.

An analogy is that when you are at a carnival and exchanging tickets for prizes, the carnie will tell you that 20 tickets gets you one stuffed bear. 

Imagine what a pain it would be if they told you that 8 grams in tickets gets you 4300 grams of stuffed bears. Sure, you could convert units of mass to units of numeric quantity to translate this request into number of tickets and number of bears, but it would be quite annoying to have to look up the units of bears per gram and tickets per gram in order to perform this calculation. 

With chemistry, you’d have to look up moles per gram of each atom, which is a pain. 

I hope this makes sense. Imagine a world where basketballs were priced at dollars per gram. You’d still be able to figure out how much money to pay for one whole basketball if you could look up how many grams one basketball weighed, but it would be quite annoying to have to do this for every transaction. Still, the units convert directly, so it would still be possible.

In other situations, though, we have to convert *back* to units of mass if we care about how much mass of something we need. 

For example, in the lab we have to convert masses of reactants to number of molecules, then use the number of molecules to balance a chemical formula and figure out how many molecules of this other thing I’ll need to match the *molar* ratios, and then convert from moles back to mass to figure out how much I have to weigh out on the scale. 

Both unit systems are useful, we can convert back and forth between them, but moles are very useful when describing number of atoms (“number of tickets”) and mass is very useful when you need to know the actual mass of reaction products/reactants (“how many kg of tickets can I physically carry with me?”). 

**Summary:** units of moles allow us to convert between *mass* of molecules and *number* of molecules if we multiply by grams per mole, and it is much easier to express and consider chemical reactions (one molecule of A plus one molecule of B gives us two molecules of C and four molecules of D). Much the same way as it’s easier in many cases to specify “one dozen eggs” than “563.78 grams of eggs.”
Your teacher probably doesn’t understand it well enough to explain and is probably reacting defensively. If I were you I would very quickly supplement (replace) your teachers lessons with your textbook itself and some good ole’ educational YouTube. 

https://m.youtube.com/watch?v=AsqEkF7hcII

Learning how to look up concepts on the internet (or textbook) will be very important for the remainder of your academic career. 
I think to understand this really clearly, we have to begin with the question of what, exactly, makes atoms of one element different from another. The answer is that each element is made of a specific number of protons + some number of neutrons and then this nucleus is contained within a cloud of electrons (whose number generally equals the number of protons, at least until chemical reactions are involved). Now, these protons and neutrons have specific mass (so do the electrons but that mass is trivial in comparison.) Thus, the individual atoms of the various elements weigh different amounts. For example, the typical hydrogen has just one proton and no neutrons, so we say it has an atomic mass = 1. Similarly, the oxygen everyone wants to react with the hydrogen has 8 protons and 8 neutrons, which gives it an atomic mass of 16. (Now, we get into some much more advanced theory when we start asking questions about those neutrons and why their numbers can vary. Those are excellent questions, and it turns out that we are still working on those answers!) 

Now, with all this background, those other explanations should be easier to understand. If individual atoms are involved in chemical reactions, it's convenient to use a counting unit to describe them. Practically, however, we can't easily count out atoms like we do eggs, so we want to be able to convert that counting unit into something easier to use: grams. That's why we learn to convert moles and mass; the thinking about reactions requires moles, but the practical work of performing reactions requires mass. 

(BTW, one final interesting question you might have is about those neutrons. Why do we call any atom with, for example, 6 protons carbon even if it has a different number of neutrons? One variation has 6 neutrons, another has 8, and there are lots of cases like this. Well, it turns out that the chemical properties of elements are pretty much determined by the behavior of the electrons, and the electrons aren't much effected by the number of neutrons. Thus, the chemistry of that carbon with atomic mass 12 and that other *isotope* of carbon with atomic mass 14 are virtually the same, so we consider them to be the same element. They are, however, observably different in some ways!)
No you teacher is just not patient and she must expect everyone to understand at first attempt (which is really not pedagogic).

Mole and grams aren't mesuring the same thing and it is not comparable.

Exemple: let's suppose that if we mix hydrogen and oxygen it will always do water. Water is 2 H 1 O so with moles you can say that you need 2 times more hydrogen than oxygen. If you want 5 moles of water it will be 5 moles of oxygen and 10 moles of hydrogen. With grams you can't calculate that easily: oxygen weight 16 grams.moles^-1 and hydrogen 1g.moles^-1 you ll have 16 grams of water and 2 grams of hydrogen to have 2 times more hydrogen than oxygen.

That is why chemist prefer moles than grams. Moles are a number of element and not a weight.
Moles are the number of molecules in a given amount of mass. This differs with every atom/molecular structure. Reactions happen in ratios on molecular level, such as 2 to 1. So its important to be that precise and know exactly how many molecules you have to predict the resultant. 

Also its not a stupid question, but you probably missed the whole segment of him explaining what a mole is. Say 1 gram of oxygen has has 50 moles and 1 gram of hydrogen is has 100 moles, and I want to make water (H2O). I can't just add 2 grams of Hydrogen and 1 gram of oxygen. Think about why.


For now lets think of moles as 1 individual MOLEcule (in really theres a shit ton per mole)


Now,
2 grams of Hydrogen = 200 moles
1 gram of oxygen      = 50 moles

If I did it like this i could only make 50 moles of H2O with 100 moles of hydrogen left over. Because its a 2:1 "reaction" in this scenario. If I wanted to react everything into H2O I would need to start with 100 moles of oxygen (or 2 grams). 

Its important to talk about things in moles instead of grams. Because grams just doesn't give you enough information to understand the reaction. 

Probbly not the best explanation but we use moles because it tells us how many molecules we actually have.

This way when you plan a reaction if you worked out your stoichiometry correctly in a perfect world you would be able to generate that much of the product for a given product
Have to say your chemistry teacher has lost touch with the purpose motive of being a teacher if he said that. Teachers/educators are supposed to answer questions and stir up debate and discussion in the subjects they profess. Stay curious and keep on asking questions, it is the only way we have gotten to where we are in the many branches of sciences.
I’m chemical reactions, mass is usually irrelevant. What matters is how many molecules of a substance there is. If you only used grams, you’d have to divide by molar mass every single time you wanted to do an equation anyway
What a horrible thing for a teacher to say. This is a perfectly logical question to ask and it probably would have stimulated a good discussion point if the teacher explained the difference. 

Always ask questions about things you don’t understand, OP. That’s how you learn and grow. Everyone, at some point in their life, had to learn something from someone else. No one is born with knowing everything. 
Yeah this question isn't stupid it is one of the most important parts of introductory chemistry. Once you learn concepts like atoms and molecules the question of mass vs number becomes immediately important.

If I am setting up a reaction I have to consider both moles and mass, they are equally important. If I want to neutralize 10 g of an acid, how can I figure out how many grams of a base is needed? If the molecules of acid and base don't happen to weigh the exact same it isn't going to be 10 g of base.
Your teacher sounds like a bad teacher. It's a really important distinction that helps explain why the mole. A mole is quantity. It refers to the number of molecules (or anything, but it's mainly used for molecules) not mass. A mole is kind of like a dozen but instead of it being 12 it's 6,02X10^23.
[removed]
No. You asked a question the teacher couldn't answer and because the teacher is a small, flawed man, he chose to berate you rather than admit he didnt know and simply look it up to learn for himself.
[removed]
More often than not, a given substance is a mixture of it's isotopes. And if it's a molecule, each of it's building blocks is a mixture if that substance's isotopes (for example [https://en.wikipedia.org/wiki/Isotopes\_of\_carbon](https://en.wikipedia.org/wiki/Isotopes_of_carbon))

The difference in mass between each isotope atom is miniscule, but if you wanted to do a reaction of 1kg of one substance with 1kg of another substance without any leftovers, you wouldn't be able to predict that, because different ratios of each isotope atoms can add up to exactly 1kg. If using 1mole + 1 mole, you could say that exactly  6.022 x 10 \^(23) atmos/molecules will react with each other without any leftovers (in an ideal world.)

And we didn't even get to the part where mass is often simplified as weight, which in turn depends on pull of Earth on your lump of substance. And strength of that pull depends on how far you are from the core of Earth, and the outcome of gravitational pull of the Moon, Sun, everything else in the solar system and the entire universe. And I hope you're not doing your experiment in a fast moving vehicle, do you?

Easier to stick with moles :)

On a separate note, your teacher is bad (I'm not going to bring cosmic scales into this equation). It's okay for anyone, even a teacher to not know all the answers on the spot (we got Internet to help us these days), but making improper excuses and trying to discourage people from bettering themselves and gaining the knowledge a teacher is supposed to facilitate is inexcusable.
[removed]
[removed]
Yes, you did sadly, though your teacher could have explaned why instead of deriding you.

If I am making a hamburger dinner, is it more important to know how many packages of meat and packages of buns I have, or more important to know how much total weight of patties and buns?

Chemistry is the same thing, you are putting together patties and buns to make hamburgers.
And that grumpy answer is why i stood my ground at highschool and used joules for almost everything that could be transformed into joules.

Newtons? They're cute,but how about no? Same with watts, mAh, coulombs, tension, pressure, electron load, calories...i got you covered. It was technically correct anyway

And i gave the same reply of not answering unnecessary topics, if asked why i went for joules.
No, I don't think any telescope could come close.

For instance, Hubble has an angular resolution of about 1/10 of an arcsecond. It is approximately 384,000km from the moon. 1/10 arcsecond is 1/36000 of a degree, and a circle is 360 degrees. 

1/10 arcsecond on a circle with radius 384000 km is:

2 * 384000 * pi / 360 / 36000 = 0.18617

So the resolution of Hubble would be 186m, much too large to make out a single human. To achieve the sub-1m resolution needed to discern a person on the moon, a telescope would need a resolution over 100 times better, which does not exist.
[removed]
I'm an astrophysicist, and I believe the answer is: ~~yes~~ maybe.

First we need to calculate the angular size of a \~2 meter astronaut on the moon. We can do this easily on [Wolfram Alpha](https://www.wolframalpha.com/input/?i=%28%282+meters%29+%2F+%28distance+to+the+moon%29%29+radians+to+milliarcseconds): we find that the astronaut is about 1.04 milliarcseconds.

As other users have pointed out, no single telescope is large enough to have this kind of angular resolution. Hubble is about a 100x too small. Typically, to resolve an object, you need the resolution to be at most half of the object's size. So you'd need a resolution of < 0.5 milliarcseconds.

But you don't need a telescope 100x bigger than Hubble to have 100x Hubble's resolution. An interferometer is an array of telescopes that have the light-gathering power of their total mirror areas, but the effective angular resolution is determined by their most-separated elements.

The most powerful optical interferometer in the world is the [CHARA array](http://www.chara.gsu.edu/public/instrumentation/31-the-chara-array), located on Mt. Wilson. It's a series of six, 1-meter telescopes that are about 330 meters apart (at longest separation). This means it has the resolution of a 330 meter telescope! CHARA has an angular resolution of 0.2 milliarcseconds, which should be plenty to detect our astronauts.

UPDATE (7 hours later): Someone asked a question that led me to think of an obvious concern: while the astronaut be bright enough to be visible by a small telescope array like CHARA?

The moon reflects [\~12% of the light](https://www.universetoday.com/19981/moon-albedo/) that hits it. Let's assume an astronaut is wearing a classic white spacesuit. That reflects [\~80% of the light](https://space.stackexchange.com/questions/29901/why-were-eva-suits-never-silver) that hits it. Let's say then, that for equal angular sizes, astronauts are 6x brighter than the moon's surface. The moon has a surface brightness of [\~4](https://www.ee.ryerson.ca/~phiscock/astronomy/light-pollution/moon-brightness.pdf) [mag](https://en.wikipedia.org/wiki/Apparent_magnitude)/arcsecond^(2). If an astronaut has an angular size of \~1 mas, then let's say they have a solid angle of \~1 mas^(2). Then they are [6 x 1 mas^(2) / 1 arcsecond^(2) = 6E-6](https://www.wolframalpha.com/input/?i=6+*+%281+square+milliarcsecond+%2F+1+square+arcsecond%29+) times fainter than a square arcsecond of the moon. Which means their apparent magnitude is \~[13](https://www.vcalc.com/wiki/sspickle/Difference+in+magnitudes+from+Flux+Ratio)\+4 = 17.

What sort of exposure time is needed to see a 17th mag object? Well, on the [Kitt Peak National Observatory 0.9 m telescope](https://www.noao.edu/gateway/ccdtime/), it looks like to get a signal-to-noise ratio of 10, we need a an exposure time of 0.7 seconds. But CHARA has 6 telescopes that are a bit bigger at 1.0 meters, so lets call this (0.7/6) \* (1.0/0.9)\^2 = \~0.1 seconds. But I'm going to assume that optical interferometry is more lossy than a simple imager, like in the KPNO example. So let's just arbitrarily bump this up to 0.2 seconds.

I'm not certain what sort of exposure time is necessary for a ground-based telescope like CHARA to work well is. But their [user's manual has an example](http://chara.gsu.edu/wiki/doku.php?id=chara:pavo_user_manual&s[]=exposure) where they use an exposure time of 8 ms. Now, is this fast exposure time needed in order to be smaller than the timescale of atmospheric variations? If so, then it's hopeless for our astronaut project.

But, if it's instead the case that CHARA had a rapid exposure time in this example because their target is very bright, then we may still be in business. CHARA might be observing bright targets because they are **big** (because they are close, which is also why they are bright), rather than because they can only see bright things. In this case, CHARA could afford to take slightly longer exposure times of \~0.2 seconds for our astronaut, even if that's a bit slower than the atmospheric coherence timescale (which is usually taken to be 0.01 to 0.1 seconds or so, depending on lots of stuff).

So, because I don't know CHARA's upper limit for exposure time (if there is one), I must offer an unsatisfying conclusion of "Maybe."

EDIT: had the wrong link for CHARA initially. Fixed now.
The short answer is no, [there is no telescope on Earth that can resolve detail that small](http://curious.astro.cornell.edu/physics/45-our-solar-system/the-moon/the-moon-landings/122-are-there-telescopes-that-can-see-the-flag-and-lunar-rover-on-the-moon-beginner).  In order to resolve something the size of the lunar rover, you would need a telescope that us roughly 75 meters in diameter.

The Hubble telescope is not large enough either, so it would not be able to resolve it.

The [Lunar Reconnaissance Orbiter](https://lunar.gsfc.nasa.gov/) might be able to resolve just enough detail to see some evidence of us being there, but it still isn't powerful enough to resolve people walking around.
Lots of people saying it's not possible with a telescope that exists today, and this is true. BUT, using MULTIPLE of the largest telescopes on earth as an interferometers in sync may be able to, just like they did with the black hole images.  The event horizon telescope is able to get the angular resolution required as it had a resolution of 25 microarcseconds, but it was in radio frequencies.  You could do it in optical domain too, but it's a lot harder and they are just starting to implement this technique in optical regimes instead of radio. 

It is also not exactly a traditional image, but an interferogram, but still produces a image. The moon may be bright enough as well in the daytime to have short enough exposures to support a movie too.

But this is very very hard! More elegant solution is to put a telescope in low lunar orbit.
[deleted]
While others have explained while visual confirmation of the astronauts on the Moon from Earth isn't possible, there's a myriad of other ways to prove humans on the Moon.

Our senses are limited; there's a lot more that is either too big, too small, too 'loud', or too faint for us to perceive. So, using very established science, we've devised other ways.

Right now, there's a series of retroreflectors on the surface of the Moon. We know their exact positions; and you can shine a well-focused laser on them, and it will bounce back to you. If you're off, say, 1m away from the retroreflector, the signal doesn't come back. Not only is this absolute proof we've been to the Moon, it provides a rather accurate way to measure the 'wobble' of the Moon, and how far away it is.

When we return to the Moon with Artemis, it would be trivial to set up a simple microwave antenna to broadcast what's going on. Maybe a solar array and battery, with a small camera, and have a 'live feed' 24/7 till it gets hit by something, or fries because of the solar radiation. You'd only be able to tune-in when it's facing you. And, because it's so simple, anyone could tune in provided they have a 3m satellite dish.
On Earth?

No. The highest resolution of an adaptive optics telescope based on Earth to a target on the lunar surface is around 100 metres, which is similar to the Hubble Space Telescope. At the right time of day on the Moon, it COULD possibly see the shadow of the landed spacecraft, but not the astronauts themselves. Kitt Peak has seen the descent stages of Apollo 15 and Apollo 16.

However, we have the Lunar Reconnaissance Orbiter, which has imaged Apollo landing sites at 0.5 metre resolution. That's enough to see a human in a space suit!

There are also ways to increase visibility. Waiting for a full moon and having a retroreflector on the astronaut's head would make her visible to most research telescopes (and good amateur equipment) as a very bright point on the lunar surface.
While it has been widely covered already what we would need maybe there is an easier way to fix the issue than building a 2km telescope. 

Obviously you could just move the telescope closer but if you wanted to do it from earth (or earth orbit there is another simple way. Have your astronaut walk around dragging a length of material behind. With a long enough length of material you could watch them do that from earth.
No. Even if we neglected the atmosphere (and we very definitely cannot do that), even the biggest planned telescopes like the ELT with a mirror 39 meters across would "only" be able to resolve something about 20-30 feet across on the Moon at visible wavelengths. That's the diffraction limit of a 39m telescope anyway. Hubble doesn't have to worry about an atmosphere, but it's only 2.4m across, and even LUVOIR, one of the proposed next generation space telescope for the 2030s, is "only" 15m across (even that's only at the most ambitious level). The largest telescopes we can build will always be on the ground, because the ground is cheaper and big telescopes are more expensive. That matters because the atmosphere has turbulence that smears out images, called the "seeing", which even at the best sites in the world is around 1-arcsecond of angular size - human 20/20 vision can resolve about 60 arcseconds, for reference. That can be corrected for to a degree, but not enough to really get to the diffraction limit, and not over anything remotely resembling a wide field of view.

So no, there are no telescopes on Earth strong enough today or in our lifetimes that will be strong enough to watch astronauts walking around on the moon from Earth or Earth orbit.
No, there is not. The size of an object that a telescope can see is dependent on the size of the lens. Currently, the largest telescope is the Keck Telescope in Hawaii, with a lens diameter of 10 meters. 

In order to calculate the size of the smallest object that we can see we first have to calculate the resolution:

Resolution \[radians\] = (wavelength)/(telescope diameter)

In our case we can assume wavelength is 600 nm because the range of visible light is 400-700 nm. Resolution = (600E-9)/(10)=6E-8 radians.

To put this into a contest we can understand, we can use trig to draw a triangle with two points on the moon and the point on earth. The length of the distance between the two points on the moon (x) is the length of an object that the telescope will be able to see. 

x=tan(Resolution)(distance to moon \[km\]) 

x=tan(6E-8)(384,400)=0.023064 km. 

Convert this to meters and we are given 23.064 meters (75 feet) as the smallest object we can see on the moon from the strongest telescope on earth. In order to see a person, we would need a telescope with a diameter somewhere closer to 200 meters. 

SOURCE: [http://curious.astro.cornell.edu/about-us/45-our-solar-system/the-moon/the-moon-landings/122-are-there-telescopes-that-can-see-the-flag-and-lunar-rover-on-the-moon-beginner](http://curious.astro.cornell.edu/about-us/45-our-solar-system/the-moon/the-moon-landings/122-are-there-telescopes-that-can-see-the-flag-and-lunar-rover-on-the-moon-beginner)
If you count non operational telescopes, then yes:

NASA still has several donated spy satellites from the NRO that are comparable to Hubble that weren't ever launched. If we sent one into orbit around the Moon similar to the LRO you would have enough resolution.

Assuming the same stats as Hubble and optimal conditions of 31 mile orbital height you get a resolution of 0.0242m per pixel. The average person would be about 70 pixels tall at that scale, or about the same size with both arms outstretched since we are looking from above. Not great but you could definitely identify the landing site and astronauts at that scale.

If you're only allowing ground based telescopes, then no, ignoring any technicalities like using interferometry.
A large part of the misunderstanding is the actual distance between the Earth and its natural satellite. Graphic representations remove empty space to represent size over spacial considerations. It looks big to the naked eye because it IS huge for a planet of our size. That said, it is far more distant than most people have been led to believe.
The answer is "almost".  

Lets do the math:(assuming we can build a diffraction limited telescope)  

- Resolution of the scope equals objective lens diameter in wavelengths(=600nm).  
- Distance to moon is 360 000km  
- Resolution needed to see a man ≤1m

So we need a 'scope with a lens diameter d ≥ 360E6*600E-9m=216m.  

Biggest single mirror telescope today is 10m.   

You could of course have multiple telescopes far enough apart, and construct an interferometric image from all of them, and see a person this way.  The VLT interferometer is such a telescope, and could have 3m resolution on the moon surface. (0.002 arcsecond Angular resolution) This will require a very bright target, and the sunny side of the moon might work
If anyone knows the answer to this I'd be very happy: could we communicate to the people up there with simple internet tools? Could we play games together or have reasonable FaceTime-esque talks? Or are they just kind of on their own?
Just to note. The ISRO's Chandrayaan-2 Orbiter camera has sightly better resolution of 0.3m than LRO's 0.5m. In theory, it might see it quite well if it's still operational in 5 years and in field of view with live transmission bandwidth.
In theory we could do it. 

The same way we looked at the black hole, the person would just have hold still for a **long** time and lie down (for the best result) 

We can triangulate something we cannot see with a lot of imperfect information. 

Its easier to use a sattelite though. 

Or just reflect of mirrors we left there.
No, that needs a telescope with an effective aperture that is more than the diameter of the earth.  The earth sized  synthetic telescope array that observed the black hole was theoretically able to see an orange on the moon.   

https://www.nationalgeographic.com/science/2019/04/first-picture-black-hole-revealed-m87-event-horizon-telescope-astrophysics/

“What we’re trying to image is really, really small on the sky,” says Caltech’s Katie Bouman, a member of the EHT imaging team. “It’s about the same size as if you were trying to take a picture of an orange on the moon.”
No. The aperture size needed to resolve objects of size X at distance Y at wavelength L can be derived from the Rayleigh criterion and is given by:  

Aperture = 1.222*L/atan(X/Y).  

If we assume a human, seen from above, is about half a meter that works out to an aperture size of over 650 metres.    

That's 16 times larger than the largest telescope currently under construction. 

And 650 metres is the absolute minimum, given perfect optics and no atmospheric distortion - a real telescope would have to be even larger.
From what I understand, technically the James Webb Telescope will have the resolution to be able to watch astronauts walking around, but won’t be able to do so.
The reason given is that for the telescope to function it has to be shielded from the heat of the sun, earth, and even the moon. 

https://en.m.wikipedia.org/wiki/James_Webb_Space_Telescope

https://www.thenakedscientists.com/articles/questions/will-james-webb-telescope-be-able-see-lunar-landing-sites
This is less of an earth science question and more of an economics / consumer psychology question (which as an earth scientist, I'm not going to be particularly able to answer). From a very basic level though, there are a lot of assumptions baked into this as a line of reasoning, e.g. that consumers (or most people in general) understand risk assessments, etc. The faulty (ha, pun not intended) line of reasoning could equally be applied to the continued existence / expansion of population centers in earthquake prone regions, e.g. 'If earthquakes are real, why haven't we seen a plummeting of real estate prices in Los Angeles as the time since the last major earthquake increases?'.  That being said, some quick browsing does suggest that there are the early signs of what you're describing (though none of these could be construed as a crash in real-estate prices at this point, more that there does appear to be a measurable influence of sea level rise on home prices), e.g. [Bernstein et al 2018](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3073842) showing that houses more in danger of sea level rise sold for 7% less than equivalent less risky homes or [Keenan et al 2018](https://iopscience.iop.org/article/10.1088/1748-9326/aabb32) showing a price premium being placed on homes at higher elevation in south Florida.
A lot of good answers in here i would just like to make one final point. Almost all predictions of sea level rise says that we wont see anything large untill the next century so buying a water front property now means that even your grand children might be spared any real damdage from rising waters(exluding more frequent floods etc)
[removed]
In nuclear engineering, we occupy ourselves with a lot of risk assessment and probability.

One thing I realized that people are not just bad at assessing risk, they are terrible. 

You see the similar results from both people who are ignorant (underestimate the consequences) and people are are educated on the topic ( underestimate the probability).

Climate change is not unlike people who smoke and those who are worried about terrorist attacks. Some do not think the consequences will be that dramatic/that rapid, there are those who think the probability that *they* will be affected is not that high.

People in real estate, honesty don't give a shit. You will frequently hear them saying that the new developments takes into account of the sea level rise. No one really asks for the details.

I am not entirely if it's still true in in US, but private insurance does not typically cover flood and is in fact subsidized (?) by the government. Here too the insurance companies don't give a shit as long it's not them who bear the risk.
While maybe not drastic at this point, we are definitely seeing reductions in real-estate prices due to climate change. Here are three studies that were highlighted by [a Washington Post article from last year](https://www.washingtonpost.com/national/health-science/sea-level-rise-is-eroding-home-value-and-owners-might-not-even-know-it/2018/08/20/ff63fa8c-a0d5-11e8-93e3-24d1703d2a7a_story.html):

[Disaster on the Horizon: The Price Effect of Sea Level Rise](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3073842)

[Climate gentrification: from theory to empiricism in Miami-Dade County, Florida](https://iopscience.iop.org/article/10.1088/1748-9326/aabb32)

[Rising Seas Erode $15.8 Billion in Home Value from Maine to Mississippi](https://firststreet.org/press/rising-seas-erode-15-8-billion-in-home-value-from-maine-to-mississippi/) (Updated)
Look at Flood Insurance pricing, not real estate pricing.  

Those selling real estate by the water are always going to list it for more and try to get top dollar.  

Look at those who are calculating risk when it applies to one living at the shoreline.  That's the real calculation for the seriousness of rising sea levels in a capitalistic society.  

https://www.bloomberg.com/news/articles/2019-03-18/climate-advocates-cheer-trump-policy-shift-on-flood-insurance

https://www.fema.gov/media-library-data/1382115115666-0fba8b9a68fef69d546513c6da105bbe/BW12_AgentWhat_to_Know_Say_Sect205_Sept2013.pdf

Rates are increasing by 25% each year until the cost catches up with the actual calculated risk...meaning that rates are SUPER low, and not actually reflective of the true "risk" to the insurance company. 

Some people who were paying 700 a year a decade ago are now being asked to put up almost 10k a year, and the pace of premium increase has only increased as the insurance industry is re-calculating the cost of insuring these homes when new risk factors (ie global warming and sea level rise) are taken into account.
Read an article that quotes the Mayor of Coral Gables FL (Miami area) saying there are thousands of homes with boat docks behind bridges (between the homes on lagoons and open water). The day a sailboat can’t pass under a bridge, no one will be able to get a mortgage for one of those houses.
edit: Hundreds
edit: linked article (I hope)

https://www.bloomberg.com/news/features/2017-04-19/the-nightmare-scenario-for-florida-s-coastal-homeowners
Something I haven't seen mentioned yet is the perversion of market Dynamics caused by insurance companies. People are not "rational" actors when their risk is insulated by insurance companies. 

The problem is that these insurance companies base their premiums on historical risk, which hasn't really fully realized the cost of global warming. When it does, expect a day of reckoning; we will see people finally want change when their premiums double for no reason.
Everybody is trying to be the last person to come out ahead in the game.

But somebody  (*actually many different "somebodys" in different places at different times*)  is going to get left being the last person who can't find a good buyer.

.

Analogous to this  https://www.scamwatch.gov.au/types-of-scams/jobs-employment/pyramid-schemes#how-does-this-scam-work-  

(*It's not quite the same,  but it's  similar in that various people can make a profit for a while,  
until the whole thing collapses and the last people in are left holding the bag*)

or this https://en.wikipedia.org/wiki/Musical_chairs
There is one other factor that no one else seems to be mentioning. 

Along with flood insurance subsidies from the govt. State and even city level governments are subsiding coastal homes in another way. 

Beach replenishment. As climate change moves forward and average temperatures rise we see an increase in the overall frequency and power of storms. Most beaches are not naturally sandy, most are rocky. Costal cities and towns all over the US bring in sand to make the beaches more tourist friendly. However these wide artificial beaches also make for pretty good erosion protection, allowing beach homes over the last 100 year or so to be built much closer than they were historically. 

 Now with storms being more powerful and frequent they wash away the sand (not natural to the area) faster exposing the houses to erosion risk and the towns to decreased tourism revenue. The towns cities and states then bring in sand to replace the lost beach. The cycle will continue until the cost is to great to replenish the beach, at that point prices will start to tank.


People are very bad at planning for the future and acting proactively. Usually things need to start falling apart before anyone actually reacts. We are seeing this with climate change now (we've known about it since the start of the industrial revolution) and we will see it with beach properties in the future. Everything will be fine until one day it isn't.
I asked an expert about this (a person who studies climate change and economic impacts for a living), and he said that there are still too many non-economic factors at play. For example, with regard to E coast flood zones (where property losses are influenced by interannual variability of sea level, as well as storm surge, vertical land motion, and global mean sea level rise), the insurance is heavily subsidized by the Fed's. He felt that the property and risk markets are so far out of equilibrium that it would be unwise to place bets on sea level rise impacts, yet. By simply extrapolating the mean sea level rise and predictable ocean tides, you could quite reliably predict "sunny day" flood events in Annapolis, Md, and you could probably make a reasonably accurate prediction of the added economic costs related to this kind of flooding, but to make money off of this you would be betting against an opponent with effectively infinite resources to mitigate short-term losses.

Investors with sufficient resources are likely placing long-term bets, but they probably won't pay off for another 20 to 50 yrs. The current rate of mean sea level rise is about 1.5 inches/decade. It is not much, but it has been leading to observable impacts especially in places where land subsidence is also occurring. Some effects, such as the intrusion of salt water into the local water table, are not visible and might not effect property values short term, but are certainly going to lead to major changes in land use.
They keep shoveling sand back toward the water, to regain what the shoreline looked like. Especially after a storm. I know the state of Florida is known for doing this. They won’t give up an inch of that real estate.
This is already a reality in some parts of Denmark, not because of rising sea levels, but because of the increased frequency of long-duration storms with northwesterly winds.

These storms frequently flood coastal areas, and house prices - especially in Roskilde Fjord - have seen a decrease.
Real estate prices are about current demand not long term risk.  They are also offset by governments taking active measures to prevent flooding, such as Boston considering to build a 14+ billion dollar barrier to protect the city from sea level rise.  

If you want an economic first response to climate change look at flood insurance and the flood zone maps.  FEMA offers historic flood maps of towns where they can.  Storm barriers can cause sudden changes, but you can see the alterations first hand.

https://msc.fema.gov/portal/advanceSearch

Every mm of sea level rise is only for the average.  The increase in the swell levels are more than a mm.
I want to point out that the sea won't rise in like a standardized way in every area. Eustatic sea level rise is just the physical "more water equals higher seas" way of looking at it. The continents behave like buoyant objects. This buoyancy - "Isostacy" - is the effect you will see when ice melts. It's when a part of earth's crust rises due to less of a load on it from, in this case, ice. So you'll see some areas that are still isostatically rebounding from the last ice age rising, while others are inundated with water from eustacy. This is just to explain part of the misconception of how sea level rises (tectonics and local effects also play a part).

Now when it comes to why real-estate prices aren't drastically decreasing... I'm not sure that they aren't. We have systems of levees that keep some areas dry that shouldn't be. But there are places where a single storm event completely leveled a town and took real estate prices from *something* to non-existent (because there was no land to sell). Louisiana especially comes to mind here, as do properties on the Outer Banks, and even River properties on the lower Susquehanna.
My understanding is that indeed, real estate prices on at-risk shores are decreasing, and insurance rates for those structures are also going up. High-tide flooding is also becoming more frequent and businesses are already facing tough choices about staying.

If people aren't convinced that the climate is changing, just watch the insurance companies and business that are impacted. One could also google "insurance companies climate change" for numerous articles
It boils down to this: humans are really bad at anticipating novel threats.  


We've been through enough hurricanes to know what a hurricane warning means, at least in Florida. But a friend of mine in Massachussets a year or two ago shrugged off theirs with "dude, people are literally outside doing yardwork".   


Fortunately for them, that storm went out to sea (I think it was Ivan) leaving them with just a little rain, but it could have been a disaster not unlike Sandy had been in New York/Jersey.  


In the Bible we have the parable of the man who built on stone compared to the man who built on sand-- the sand man was washed away when the floods came down.  


Take a tourist who sees a cute (but dangerous) animal for the first time and tries to do a cuddle-selfie despite warnings from their native guide.  


There is a saying "Once bitten, twice shy" which is great when the thing biting you is only an annoyance; unfortunately, climate change won't be just an annoyance, it will result in fundamental change with a life-changing price tag. And unfortunately, price-tag is not a factor for these sort of novel threats like Mt. St. Helens blowing up (people refused to leave, for example) or rattlesnake bites (hold my beer) or climate change (my property is mine).  


We even see examples where I live in Colorado. We've had a rash of bear encounters this last year, and the victim's response is sometimes "I built my house here, why are they on my property?". Well, for one, bears mark property by scratching and peeing, not by drawing lines on a map with dollar signs. And two, you left your trash out and the bear found out-- what did you think was going to happen? And yet they insist that this is theirs and seem to be legitimately confused as to why the bear doesn't recognize that. (Some do recognize it, but a surprising number do not-- much to my amazement). Anyway, same thing with climate change. In their mind, the human went through all the right steps to earn money and purchase the property, and the weather will somehow (magically) understand and respect that.  


As a last example I am reminded of King Canut, yes, he is real. Apparently some of his supporters/advisers saw kings as all powerful, so he had them take him to the coast where he tried to stop the tide from changing. There is no record of whether or not his knuckleheaded followers got the message.
Real estate prices are largely garnered by supply and demand, not expert opinion about the future value of those homes, and supply and demand can be as fickle as the populace's own belief in climate change and its effects.

Insurance, on the other hand, does appear to be increasing rates on coastal areas and other areas likely to be affected by climate change:

https://www.insure.com/home-insurance/climate-change-home-insurance-rates

https://grist.org/article/insurance-experts-rank-climate-change-as-top-risk-for-2019/

https://www.lowestrates.ca/blog/homes/climate-change-home-insurance-going-to-cost-you

https://globalnews.ca/news/5236990/climate-change-insurance-cost/
Why do people in CA live in the mountains where it’s often more expensive (e.g. Malibu Canyon, Mulholland  Drive, Calabasas, etc) when there’s always a huge risk of fire? I think it’s because humans tend to enjoy living by nature, and also rich people like to flaunt living by the beach, mountains, etc and don’t really think about rising sea levels and all that (especially if they can just move)
Realtor in SFL. insurance prices have jumped in flood prone or high risk flood areas, so there IS evidence the market and customers are responding to the change in risk.  In the uninsurable markets , much of waterfront property is purchased with cash, by deep pockets that can afford to take the risk and enjoy the coastline wherever it happens to be that year.
Because people who own these properties aren't gonna drop their prices, see levels wont rise for a long time either. I dont know why people associate this with the biggest problem of climate change, the biggest issue that will arise is a collapse of our food chain I  that we wont be able to produce enough food to feed our global population, probably be some issues with water to stemming from that
Bit late to the party, but I watched a short docco about a small costal Welsh town in the UK that's due for decommissioning around the 2040s, as maintaining the sea wall is too much. Obviously this has caused house prices to fall to almost naught. 

https://www.theguardian.com/environment/2019/may/18/this-is-a-wake-up-call-the-villagers-who-could-be-britains-first-climate-refugees
[deleted]
My comment will probably get buried, but still want to try. I work in commercial real estate in Texas and we’re seeing the very beginnings of this happening. I work with investors that have anywhere from 10-100 properties in their portfolio with us in Texas, and they also have portfolios with other companies nation and worldwide. 

Several of these investors do 5/10/15/20/30 year planning. Climate change isn’t really a part of the 5-10 year plans, but it’s most definitely part of the 30 year plans. There have been studies showing which states will be affected by climate change the most, and the least, and the clients are paying attention. For now it’s business as usual but I have clients who are already planning to buy and hold properties in the ‘safer’ states in the next 10-20 years. They’ll likely sell the properties in the most dangerous states before they’re really impacted.
We do to some extent already. Look at the city of Norfolk. It's on the east coast by the Chesapeake Bay in Virginia. 

Over the last 15 years a lot of it has been designated as different levels of flood hazard. These flood hazard designations have been doing a slow creep through the neighborhoods. 
The flood hazard zones come with the requirement to buy expensive flood insurance (extra expensive as the houses are in flood zones... )

The drop in value of the houses is masked by an inability to actually sell the properties. Instead of taking a huge loss selling, they get converted into rentals. Alternately the houses just sit in the market for a really long time as people try desperately to get what they paid for the house back.
Because flood insurance is too costly for insurance companies to cover so the government actually covers floods, as disaster relief was becoming too expensive to deal with without insurance. 

So the government has spent lets say $800,000 on a house that's actually worth $100,000, multiple times over, and it's actually a huge problem. John Oliver did a great show about Flooding on Last Week Tonight.  That's part of the answer, anyway.
That’s a great question for a psychologist.  I’d imagine it’s because humans have a brilliantly designed bit of built in software that causes the user to deny reality when it’s spelled out, and even strengthen their position against a negative outcome.  

Think about how many times you’ve argued a point and how solidly the person you’re arguing with has just denied your arguments.  Frustrating beings, humans.
This is more a risk/reward scenario for people buying beachfront property. Humans are notorious for undervaluing risk in expensive purchases. For example, there's a flood plain in central California where rich people buy $1,000,000+ homes, despite the area getting flooded and destroying their homes *every couple years*. 

They can see the house now, but a flood is only a *possibility*, so they eschew rationality.
as soon as banks start saying were not giving loans to homes in these areas then you will see massive devaluation. If you are a seller in these areas you will need to find an all cash buyer that does not believe in climate change. Until then people love living by the water for obvious reasons so they are willing to take the risk.
So you may have heard of the fires in Alberta a few years back, which caused an entire city to evacuate. 

Now, they cannot get insurance for their condo's, or if they are it's very high priced. The prices are starting to come down and people will be losing a lot. It's the insurance companies that are starting to push the change. If you can't insure it, you can't afford to own it. 

 I've was keeping an eye out for a cottage property but after some more research decided against it. I've seen both cases in Canada, where the lake is drying up and the docks have to be extended each year to reach the water, and situations where the cottages are now under water. 

Here's the news article on the condos.

[https://www.cbc.ca/news/canada/edmonton/fort-mcmurray-condominium-insurance-1.5318750](https://www.cbc.ca/news/canada/edmonton/fort-mcmurray-condominium-insurance-1.5318750)
Live in SFLA - let's be very frank  - the reason why sea level rise isn't a serious consideration for most of us is that the current level of rise is \*far\* \*far\* \*far\* below what humans can deal with.   The same reason the Netherlands still exists (and thrives) in large part because we've had the tools, technology and most importantly the economic willpower to deal with a 1 inch rise per decade.

You'll see lots of FUD articles but with all due respect the constant flooding in MN & ND (see Red River of North floods) causes far bigger problems.
I can add a little here, I work for government in land use and environmental issues. Insurance companies are making moves to increase premiums in coastal areas but a huge factor is government policy. In theory (in my country) if the risk is great enough government would 'red zone' and begin slow managed retreat from the coast. 

But as soon as we implement a red zone the property price would free fall, which the home owners don't want. And unsurprisingly coastal home owners often happen to be wealthy and well connected. So while we are advising almost complete red zoning of the coastline nothing is done at a political level.

People are still building on the coast because the zoning is for residential, so they assume why not, if it floods it's the governments fault for not telling me and they will pay.
Because most people don’t take it seriously or don’t want to take it seriously. Myself on the other hand who has built my little empire in S. Florida is planning ahead. First reason is because I want to leave my kids something. I won’t see the land flooded but they might. They say the Florida Keys will mostly be gone in just 50-60 years. When prices crash all my work will have been for nothing.

Second reason is to beat the land rush. Once the majority of people get it, then things will happen quickly I’m afraid. Also, you only mentioned shore prices going down but inland  prices will go up. I bought a. Outlet acres cheap in N. Georgia and plan to add to it. 10 years till retirement and I want all my Florida property gone by then. Pack up and leave, course, can still visit in Winter!
Several reasons:

1) A sea-level rise of 3-5cm per decade isn't significant to most property values. That can be mitigated by pretty-standard property maintenance. Generally speaking, anyone who buys coastal property as a long-term investment is prepared to maintain it to this level.

2) Property-market prices are heavily influenced by units which turn over quickly. These tend to be higher-priced properties, since people are less inclined to sell properties whose prices are stagnant or falling. 

3) The primary risk from sea-level rise isn't the level of the sea, but storm surge. This risk is pretty unpredictable, but coastal areas are generally skilled at mitigating it — for instance, by spreading the risk among many people through property insurance.

4) Most individuals' longest-term horizon for property-owning is a generation or two. Forty years would be a very long horizon. Climate effects just aren't strong enough to reliably affect prices in this term. 

5) The main influence on property prices is demand, since as is famously said about real estate: They ain't making any more of it. So as long as a region's population and wealth continue to rise, its property prices will rise. And they will probably rise faster than average at the coast, as long as people tend to value coastal property over inland property.

Calling climate change "a serious threat" is kind of like calling an extinction-level asteroid impact or alien invasion "a serious threat." The resulting impacts are devastating, but if you approach the problem rationally then you also have to estimate your likelihood of encountering those devastating impacts — and that usually comes down to your assumptions about the time frame involved. Over the course of a billion years, an extinction-level asteroid's *approach* toward Earth is pretty likely. But in your lifetime, the actual impacts you'll see are likely to be minimal.

And the same is probably true for climate change. I doubt you'll see large numbers of people selling coastal property for cents on the dollar in your lifetime. I often tell people that if anyone feels climate-spooked enough to panic-sell at that level now, they should give me a call. But nobody ever does. I guess there is some climate panic out there, but the rationality of the property market remains basically undisturbed for now.

Edit: typo
You are seeing it in areas likely to flood. Flood plain designations are being updated and more people are being required to purchase flood insurance as a result. 

That's the "problem is small enough that economics can solve it" stage. You used to not have to worry about it as much, now the problem is getting steadily worse, and by 2050 or whenever is cited it could be bad enough to cause people to move en masse away from affected areas.
[deleted]
[removed]
Current estimates project sea levels to rise so slowly that the real damage will be in a hundred years from now and US houses are not made to last that long anyways so its not something you will see priced into the value of current real estate.
No capital entity will ever allow profit leakage until the entire asset class is universally abandoned.

Why is there food wastage? Why do grocery stores dispose of unsold produce? People are always hungry, in fact it breaks the idea of supply/demand: The demand for food is infinite, every person needs food every day. So why is there food waste? Because giving it away or reducing its price reveals that it doesn't have to be sold at the price. You avoid profit leakage to avoid full-on profit annihilation; once people know all the produce is half price in the last hour of business, half price jist becomes *the* price and anyone shopping before the reduction is just paying a fee for convenient shopping hours.

So why sell a coastal home for half its value? That would just be admitting this thing is gonna *completely* worthless soon and the market won't buy a soon-to-be-worthless asset no matter how big the discount.
It is in many places, just subtly at the moment, expect it to be more interesting over time. . Wealthy folks are buying east of the pacific coast highway and selling their beach houses. Long term real estate investment holding areas of interest also moving inland compared to a decade or more ago. 

There are many real estate zones that now exist, at least casually, between investors that are hoping to account for climate change: including sea level changes, season shifts, catastrophic weather (fire, tornadoes, hurricanes, etc). 

Interestingly northern Canada is a super hot market for beachfront property if you are a speculator.  I know numerous people buying arctic because of climate change. 

I don’t recall who, but some group did analysis for next hundred years and mapped results for investors to be able to shift holdings responsibly before the changes happen and make their land worthless and other land a super premium rate. 

I should state it’s all hypothesis and speculation based on current data. It might be an over reaction. I know many people who have changed purchases based on the above and other factors.
Related are wild fires in California which are being exacerbated by climate change. People are still buying houses out here and property values are as absurdly high as ever but one new development is that it’s now near impossible to get fire insurance. In high risk areas, anyhow. Insurers know it’s a financial disaster just waiting to happen and don’t want to risk it.
[removed]
[removed]
[removed]
[removed]
[removed]
It is kind of like before the financial crisis why did people keep buying mortgage backed securities even though they were total garbage. Some people knew they were garbage so would buy them and then pass them off to people who would either also pass them around or to people who didn't know they were complete garbage. A lot of people see bubbles are try to get in on it before it pops and some people don't and are stuck with the consequences
Because it is not a certainty and nobody can predict wether it will happen in 2020 or in 2120 or never.
People are short-minded, forget everything after a short period, because of life, problems, responsibilities and the simple fact that most don't really care.
[removed]
Flood insurance is Federally backed and does not accurately price in the risk of living in flood prone areas.   Also many of these properties are second homes bought by the rich so the government is less likely to change this status quo.  That is right your tax dollars are hard at work rebuilding second homes for millionaires.  The federal government is starting to buy up frequently flooded areas though.  This is an American answer, not sure about the rest of the world.
Not an economist but cities are already investing in retaining walls like the 5 billion project in SF. Maybe real estate won’t be affected as much because people believe that the sea wall will be enough. It will be a slow process but it is inevitable.

https://www.google.com/amp/s/www.mercurynews.com/2018/10/18/seawall/amp/
When there is an increase in the number of deaths due to car accidents, do you see a decrease in the price of cars?  

People just don't care, in large enough numbers, to make a difference.  

There are still people that buy houses in flood plains, or even on the side of a volcano.  There are still people who smoke regardless of the dangers pointed out to them over the decades.

Basically, while people are still willing to pay the price for something they want, the price won't go down.  And if it did, more people would start buying at the cheaper price and...guess what?  Yep, the price would go up.
As a subset of this question I'd be curious about how the corporate market for real estate looks in low lying areas e.g. Silicon Valley, New York. I think largely the reason real-estate hasn't plummeted is most consumers aren't thinking that long term. Corporations however tend to be more likely to factor in such considerations in their long term financial planning.
As is usually the case in property markets, speculation can keep prices high long, long after the underlying values of the property have fallen. This effect is where property bubbles come from. When the breaking point arrives and buyers can no longer justify paying the listed prices, we call it  bubble bursting.
The simple answer is that it's not imminent.

It's the exact same reason Houston, Miami, Hawaii, still have valuable property. Basically people gamble that "it won't happen to me" consciously or subconsciously. Plus, balancing with trust in insurance companies and balanced with the benefit of living there. It actually does make sense that real-estate prices aren't impacted much.
It's more erosion areas. So towns and villages on/near cliffsides have crashed dramatically.

https://www.theguardian.com/money/gallery/2018/jun/01/clifftop-homes-for-sale-in-pictures

The bottom house from this link says that because it's so near a cliff, despite the large property size it's still high risk and you cant even get a mortgage on it
Because the value of living in a coastal area that will be inundated by flood waters is high enough during all the time that it's NOT inundated by flood waters that demand remains high.  See the NC Outer Banks -- nobody is disputing that those homes will get wrecked by hurricanes eventually but people still build and real estate still sells.

The Federal Government supports this by offering low priced flood insurance which the Government comes out ahead on because it collects taxes on all the economic activity that is generated by eventually flooded properties meaning it shows a net gain (since the Government doesn't have to make money on insurance premiums to profit)
Because it only affects homes that look at risk.  Most home owners don't instantly go, "oh that oak tree is going to grow into my septic tank in 15 years"  or "maybe a forest fire will burn my house surrounded by woods" but it still happens and people pay crazy amounts to have a mansion in the woods or too keep that tree that gives the yard character.  They look at the superficial aspect and if it seems in good repair it must be a good house.  I live very near Lake Michigan so I constantly see super expensive homes go up in flood areas that are close to the river and others on the cliffs overlooking the lake.  But we also see the older existing homes being tore down or being sold for less than before because of water damage and erosion.  We have had multiple homes in the area collapse down the hill into the lake.  I suppose they don't hold very much value when the danger is perceived as imminent.  
A few homes are supported by stilts too keep them from just falling down.  Once all the old houses fall or get tore down new ones (slightly farther back) pop up.
NPR had a great interview last week related to this topic. Can't recall the interviewees name, but he released a book on the history of people living on US coast lines. I'll summarize what I can.

So, insurance companies learned in the 1920's that it is not economically feasible to make a profit providing flood insurance to coastal homes (floods happen often and cause massive destruction instead of minor damage). Since there was no way to insure oneself against coastal flooding not a lot of development occurred directly on the coast (hotels and other businesses maybe, but not houses).

Then, in the 1960s, LBJ commissioned a study to see if it was viable for the Government to provide some type of Flood Insurance because developers desperately wanted to start building and selling homes directly off the coast. The study found that it would likely be possible to do this if a couple of key rules were followed. First, the premiums had to be really high because its extremely expensive to rebuild/repair after flood damage. Second, there could be NO government subsidies (ie gov't covers part of the expensive premiums) because doing so would not be economically viable (again, price of covering flood damage/destruction is really high).

Welp, the study led to the National Flood Insurance Program, but of course those subsidies the Government shouldn't have allowed? Well they allowed them. So now you have flood insurance on the cheap, which led to a boom of coastal development and purchasing.

So people kept re-building because they had the insurance to do so and because they could keep affording the premiums, which allowed demand for coastal homes to stay high even though its stupid to do so because floods will likely happen again and destroy the home again. And if demand is high, then home prices will continue to increase.

The Government now only allows for a home to be rebuilt 3 times, after that they won't cover your expensive premiums (this may be part of the original 1968 legislation, not sure on that). With Climate Change and more flooding, people are hitting that 3rd rebuild quicker than in the past (this is currently happening in places like Houston). So, we are starting to see some instances of people moving away from the coast, because they can't afford to re-build on their own.

So, why have we not see a drastic drop on coastal home pricing? Its a mix of greed from the private sector and mis-management of a government program. AKA the cause of the majority of issues in the US.  




EDIT: the book is "Geography of Risk" by journalist Gilbert M. Gaul
I think this is also a good psychology question. Demand for coastal homes still hasn't dropped, hence why prices haven't dropped. So the question is, why are people still willing to pay premium dollars for a home that years of research has proven to be in an incredibly risky area.

I think you would need to start looking at the psychology of climate denial, apathy, and risk/reward behaviors to get your answer.
People that don’t believe in climate change will have no problem buying in coastal areas because they don’t see it as a threat. It just means that they ( or much more likely their kids) will be screwed when the houses are worth less. But we have already seen some low laying land in Miami areas affected by climate change but it’s like a 7% drop(according to Harvard) there won’t be a run on property.

Also you should go read an annual report of top US hotel companies in US that have lots of coastal properties. They seem to think it’s risk to their assets.
In history you have examples of prices increasing dramatically around absurd things - the best example is Tulip mania from XVII age ([https://en.wikipedia.org/wiki/Tulip\_mania](https://en.wikipedia.org/wiki/Tulip_mania)). In short, economy reflects peoples desires, not peoples needs, or, God forbid, scientific necessaries.
I saw what I would have said about insurance subsidies but wanted to add: the poor, as expected, will be screwed. Many low income people in the U.S. (and worse in other countries) live in areas of rising flood risk, including by oceans but also by lowland waterways, bayous, etc. They would move, or build higher, or build seawalls, or take out pricier insurance, but they can't. What they do own may end up being worthless.
Just today there is a [story in the Australian media](http://www.abc.net.au/news/2019-10-23/the-suburbs-facing-rising-insurance-costs-from-climate-risk/11624108) about how climate change is likely to impact home insurance premiums here. When people's home insurance triples or they even get refused insurance, watch property values plummet
You can think of the present value of a rental property as more or less the combined present value of any future rental income less upkeep plus or minus the fall/increase in the real value of the property at sale. If you're looking at a total loss of the property at, say, 100 years from now, that "large" loss way in the future is worth a relatively tiny amount compared to the present value of all the incremental income that starts more or less immediately.
Coastal areas have always been desirable places to live. 

Despite the all-consuming knowledge of climate change people still want a nice view and a sea breeze.

Not to mention most of the world's major cities are coastal. So even if you did fear the effects of climate change, people may not be able to move away due to work, family, money, etc
I’m not sure surrounding the science of it all, but I can tell you as someone forced to live in a coastal town that while purchasing we actively avoided places on the oceanfront and around lakes/streams. The flood insurance is exorbitant and often enough to leave those houses on the market often for months and sometimes years. We pay some flood insurance with a hurricane rider
If I remember correctly, federally subsidized flood insurance is partly responsible. Also every time the coast gets hit hard, the old houses are destroyed and then get rebuilt into nicer newer houses. Someone wrote a book about this happening in New Jersey. I forget.
Because the seller wants the most money for the house and i dont think the seller is going to worry about the buyer worrying the house will be damaged in a flood or something. So they have an excuse to sell it for full price.
I'd assume it's because there's very high demand for coastal properties but limited supply, and for this segment of home buyers the premium cost of living on the coast does not outweigh the risk of sea levels rising.
It is changing, just slowly 

https://www.wsj.com/graphics/coastal-home-sales-affected-by-climate-change-worries/

“In a growing number of coastal communities, homes near the sea are appreciating more slowly than those inland. That's bad news for people on the beach, good news for those farther away.”
There are alot of wealth that they buy for the moment and not for savings for the future, simple. Most clients have other property to quell for any issues including tropical storms and or king high tide floding to hurricanes.
Prices of houses prone to flooding after hurricanes decreased in Texas and specially Houston.  Some really high end and expensive neighbourhood lost a lot of their house values . There are million dollar houses with backyards opening to a lake.
A bar near where I grew up was about to sell for big $$$, replacing it with condos. The deal was postponed because of flooding in the area that has never happened before. 2 years later it happened again, flooding large areas in the Norther part of the town. A deal that was being pitched at them for decades was terminated and now it's worth nothing since it's been deemed a Flood Zone. 

Hundreds of waterfront properties have lost value because of this.  


Montreal, Quebec, Canada (Pierrefonds)  


 [https://montrealgazette.com/news/local-news/west-island-gazette/pierrefonds-watering-hole-bar-chez-paul-remains-open](https://montrealgazette.com/news/local-news/west-island-gazette/pierrefonds-watering-hole-bar-chez-paul-remains-open)   


 [https://www.cbc.ca/news/canada/montreal/flooding-in-pierrefonds-seen-from-above-1.4103231](https://www.cbc.ca/news/canada/montreal/flooding-in-pierrefonds-seen-from-above-1.4103231)
An analogous situation is properties that have a fixed term (40 year lease) that have ownership revert to the original owner after the term. 

These leases sell for very close to the price of full ownership (which would be forever). These properties hold their value until just a few years before the lease runs out. The last owner is left with nothing. The second to last owner figures they can unload it in time to get their value out. Often they do.

It is a human nature thing and even in the case where it is indisputable that the property value will disappear in a few years, people deny the future until it is upon them.

Oceanside property likely will be underwater at some point in the future but owners figure they can can sell the property before then so they pay the full amount now.
Much the same way cigarettes are technically higher when you account for inflation. Just because something is known to be bad whether sooner or later, its a value they still want. 

Likewise the belief that housing on the shore can overcome the dangers of climate change is still an idiotic belief that science can simply whimsy away the problems enough that an ocean side property owner won't ever have to worry....unless it's Florida.

Some people go to Florida to buy a house in the shore cuz chances are they'll die before climate change destroys their house.
Idk. But in cali, lotsa rich folks keep buying houses in the hills and risk life and property with frequent earthquakes and inevitable evacuations due to fires.

Seeing how that continues, i’m not surprised that rich folks still wanna live by the beach
A lot of the answer lies in the status quo of disaster assistance.

We built FEMA, flood insurance, and disaster assistance funding on the assumptions that 100 year events happen every 100 years.

The events are increasing but we haven't made the changes yet.

Like it or not, it doesn't make sense to rebuild a house in a flood plane over.

At some point the government will realize that certain places of the country are just not practical to inhabit anymore and they will no longer be insured or assisted at the rate they are now.  At that time real estate prices will change but until it happens they don't.
Because most people in modern countries dont care at the time scale presented. They would rather enjoy the beach property than care if it's not a good 50 year investment. I have worked in construction and you would be surprised how little people care about the long term value. If they want some trendy but awful kitchen they will drop 20k on it knowing full well that in 5 years it will look like a joke and need to be replaced. Coastal living is either business or luxury and neither of those care about long term.
I think it's all down to perception. The sea isn't creeping up the beach, at any perceptible rate. Most of the time, it stays down there, where it's supposed to be. Except, one day, it's over your head, and your home is gone.

But, even then, there's probably a community memory of this happening before, maybe 50 years ago. Everyone rebuilt, and had 5 fine decades of swimming, surfing and picnics on the beach.

It'll be the same again, they think - except, this time, it will be 40 years, 35 years, 30 years to the next catastrophe. At what point do you give up on the pleasures of beachfront living?
Gene Hackman AKA Lex Luthor was playing the real estate gamble by detonating nuclear war heads along the fault line which would have plummeted California into the Ocean. Superman stopped him then and Superman will save us now.
Simply put, people don’t price that far into the future. The beach front homes in the USA still have at least ten years of being awesome, so the prices will stay high. When it starts going downhill, prices will drop. 

Your argument is similar to someone saying you shouldn’t buy a PS4 because a PS5 is coming. And that you should skip the PS5 too. Eventually you’re just missing out entirely, or you gotta get in the game. (PS being the different generations of Playstation video game consoles.)
[removed]
Drastic decreases don't happen due to slow and progressive changes.

We're already seeing a shift in demand, a change in patterns, as some of the lowest-lying and most vulnerable places become non-viable.

For instance, I live in Long Island, we were devastated by Hurricane Sandy. Instead of re-building as they were, some areas are now forbidden for building and owners were forced to move. [Other homeowners are being forced to lift their houses and put them up on pylons.](https://www.newsday.com/classifieds/real-estate/long-island-elevated-home-1.23440165) Coastal houses that aren't lifted are selling at drastically lower prices, if a buyer can even be found. Lifted homes are selling at a relative premium.
Oh yes, they are dropping. Here in El Salvador, people with money have beach houses for holidays or weekends. People are not buying anymore because the high tide is higher than normal and a couple times a year you will get flooded. New homes are being built farther from the shore, but built houses are at constant risk and have to spend extra money in walls that don't really help on the long term. Prices are dropping as people don't want to buy houses that are at risk.
[removed]
The most simple answer is the misinformation campaign has been effective. Energy companies and big business have been spending a fortune to convince people climate change is ‘fear mongering’ as the other commenter put it. And it has been incredibly effective, unfortunately. First off, no one is saying that death is coming tomorrow, this month, this year, or even this decade. What is being talked about is the inter generational responsibility that we have. We should start the work now to reverse the effects of climate change so that future generations aren’t stuck with an irreversible problem. 

Now as far as your question, the reason is because the danger is not imminent. Again, it’s years away. And the reason we’re in this mess and having trouble getting out is because of how short sighted humans are. No one thinks about humans 50 years from now because presumably most of us will be dead. Same goes for housing values. No one is thinking what will my house be worth in 50 years, I’ll be dead.

As we move close to the true danger levels, you’ll start to see a shift of people moving inland. You’ll also see people moving out of hotter climate areas. People will leave Arizona, New Mexico, Nevada, places of that nature because the average temps will go up by a few degrees. They will end up Middle East hot and will eventually become unlivable. People will flock to the north and to the Midwest. States along the border of Canada will see a huge influx of new residents. Souther Texas will be abandoned as will most states on the southern border. If there is an increase in average temps of 5-10 degrees, many places there cannot be comfortably lived in.

Now a 10 degree shift is not in the near future. No one alive today will see that increase. But we will see the steady increase of maybe 2-3 degrees during our lifetime.

This is all assuming that we don’t shift our priorities and start focusing on undoing what’s been done. But even if we do, there will still be an increase in temps in the short term because we’re already on the path. Even if we correct our direction now, there is still going to be a correction period. The change will not be immediate.
From a non scientific POV.. because  ppl are ppl... they want to live in pricey places for a variety of non logical reasons and only think of "now". Also ppl are greedy and will take advantage of those mindsets as long as they can.. buyer beware.
It's far enough off that most people buying real estate don't think of it, and even if it does flood the government will pay you to rebuild.

Same reason why people are still buying property anywhere there are frequent natural disasters, there is either money to be made or that's where they want to be.
[removed]
You could say the same thing about areas that are prone to natural disasters, like earthquakes. 

People are not that great at assessing long term risk and short term considerations are often more important drivers of price. Coastal areas are generally pretty, have good economies, and offer lots to do. People value that more than some threat that seems decades off.
Not really a science question lol but it's because real estate is (for the purpose of this question) based on perceived value. As long as enough people believe that those areas aren't in serious danger of massive flooding and/or the benefits of living in those areas outweighs the risk to enough people, the value won't decrease. In simplest terms, as long as enough people still want to live there, the value won't decrease.
[deleted]
[removed]
Keep in mind that the life of a mortgage is 30 years, and most of the interest on an amortizing schedule is paid up front. If the home has 30 years of life in it by even the most worrisome projections, then why not buy and why not provide a loan if you’re a bank? 20 years in, if the home sold for 2/3 of the original purchase price, you could still pay off the mortgage.  That is without inflation adjustment, so we’re really talking another 40% lower than that in real dollars. 

So the bank isn’t going to lose out; seems like instead like better times are coming to charge a premium. If you’re 65 and retiring, seems like a decent way to go, too.

These properties have 1-2 generations of use value in them still! I’ve 35 years left in my life if I’m lucky before I die. Maybe I’d be looking for discounted oceanfront property if I didn’t have kids.
I recently listened to a report discussing the rising costs of rebuilding coastal communities along barrier island around the world due to rising sea levels. Problem is that every time these islands suffer a hurricane or rising waters,  FEMA (tax payers) foot the bill of repairs.
Because people believe their eyes more than their brain. 

So to say they might believe in climate change but have no concept of how fast and devastating it could be. In general, prices for coastal areas are only slightly decreasing because the demand for them also only slightly decreases from an ideal economic standpoint. 

You can also think of it from the standpoint of a buyer. Someone buying a house now will naturally inspect it and the surrounding area. They probably have no idea of the history of the place and where the water was years ago. All they see now is (for example) that the beach reaches 200m until it hits the water. Since climate change comes slowly they might not even notice the water creep up to them until it is within much closer proximity (say 50m) over the course of years, depending in which area they are. Only in the last few meters they actually realize what will/could happen and that is when the prices will drop.

&nbsp;

For the whole "eye over brain"-discussion you can look up the [bacteria in a bottle gedankenexperiment](https://www.albartlett.org/articles/art_forgotten_fundamentals_part_4.html) which works with exponential functions (the natural language of growth) and you can ask yourself how much of it you find "plausible and natural".

It is imo a fitting analogy to energy consumption and population increase which is the main cause for the climate change.
The majority of human beings do not react until the boot is pressing on their neck (or their toes start to get wet while sitting in the living room).

Nearly for sure now we have blown it and just as sure an expanding number of coastal properties will need to be abandoned.  The march of rising sea levels will continue as methodically and as surely as rivers flow to the sea.  Moneyed cities such as NYC will fight the good fight but will lose out to the forces of nature.For relatively compact coastal cities such as NYC, there are solutions; laying down 25' of concrete (filling in the first floor of every building and basement, raising the subway and infrastructure and laying down new roads and walkways on top of that.... buys them 50 years or more.
They are in many places. Large swaths of coastal Florida land are plummeting in price as building restrictions get tighter and tighter (coastal France too). Some cities are building pump out systems with levee's and elevated gravel based road ways to extend the useable span of the areas as there are massive hotels and condos that aren't easy to relocate.
I had a coastal house that got flooded and I sold it for what I bought it for and consider myself lucky. I live 100 feet above sea level now.  If you can see the ocean, the ocean can see you. It's the storms that will drive you away, any sea level rise is barely perceptible so far.
Because they're going to milk money out of it until people have to evacuate. Once cities start to go under, only then will you see prices decrease.


People aren't the best at thinking ahead and some of them are actually actively ignoring the problem so they don't have to deal with it. To be fair, it's going to work because it's their children that will deal with it, not them.
You did see a pretty big drop in real estate prices in lower Manhattan/FiDi after Sandy since a lot of offices had to relocate and left their offices vacant after they reopened. A lot of those offices are slowly converting to apartments so the rents are climbing back up.
The average person isnt in tune with reality is why, they're more concerned with drama and what not. Same thing with asking why do people with little to no money spend above their means? They just arent as aware as the more self aware and intelligent people. Hence why there's only a small percentage of wealthy people
Because humans are full of cognitive biases such as:

Confirmation bias; listening to information that confirms our preconceptions.

Conservatism bias; where people favor prior evidence over new evidence.

Ostrich effect; decision to ignore dangerous or negative information.

Selective perception; allowing our expectations to influence how we perceive reality.

Zero risk bias; humans love certainty even if its counterproductive.


Basically humans hate change and will deny it so they buy houses in Miami and if they are smart they will also buy insurance for that real estate.
The rate of which and actual measure of the seas rising will not be catastrophic to most homes. We’re talking about centimeters through hundreds of years. It has a great affect on many other aspects of the world, but likely not your beach house.
We are already seeing "climate gentrification" - i.e. property developers buying higher elevated land in poorer neighborhoods to develop. At least in Miami, we are.

This question is also a lack of basic understanding of human nature, though.

If a hurricane is coming in the next week, raid Home Depot, savage Publix aisles, and go full Mad Max on the gas station.

If there's a slow, but inexorable march toward sea level rise over the next 100 years, people will worry about it seriously when the water rots their mailbox out.

Coastal communities are already having to deal with more flooding, so the process is ongoing and people are having to start taking this seriously.

Now that it's too late to stop it.
[removed]
[removed]
[removed]
[removed]
This seems like a question for AskReinsurance. I would assume that insurance rates are significantly up, but because home values tend to be very high on beachfront/coastal property any adjustments made in asking price are not easy to decouple from market fluctuations. Once houses get over $2M or so it can be hard to judge what makes them sell or not sell for a given price.


To pick an extreme example, there are barrier island homes that get destroyed every ~10 years or so, but still sell for tens of millions of dollars because the owners can afford to insure them and know that ultimately the federal government is picking up the majority of the bill.
Similarly, we could see the same phenomenon taking place in high risk seismic areas, such as southern California, the Pacific Northwest and BC, Canada. However, in this cases, economics, and offer/demand play a much bigger role in the scenario.
Aside from the great responses provided already: a change in overall prices has to be deliberate on the part of the sellers. Many sellers of real estate in those coastal regions expect to end up profiting from the investment they initially put in, so they keep listing their homes at the level where they remain comfortable selling it, not necessarily at the level of what the home is actually "worth" if you took flood risks into account.

Additionally, housing prices often depend on the price that comparable homes in an area can be sold for. That tends to insulate a market from price drops until some catastrophe strikes.

What that catastrophe *is* can vary. It could be a major flood that destroys the homes in an area, causing the sellers to sell the remains cheaply just to recoup some losses. It could be foreclosures on nearby homes, resulting in seizure and then auctioning of those properties setting new base prices. Perhaps nothing really goes wrong but the frustrated seller, with a listing that stays up for years, finally drops the price just to offload it, often to investors looking to flip the property and turn a profit.

So generally, climate change can't really directly affect market value so long as both sellers and buyers are not focusing on those risks when setting prices. Prospective buyers could try negotiating for lower prices with those risks in mind, but if there's a chance that someone else could offer a higher price, the seller is just unlikely to accept it.
There are many places where prices are dropping. Some higher-class places have taken measures to mitigate rising sea levels to keep their investments safe...for now. Eventually, things will get worse and you will see those coastal areas being abandoned and the value will shift to properties near the coast in safe/higher areas.
It is quite likely that Florida will be protected by dikes, because there is too much money in Florida real estate.  One downside of dikes is that you no longer have a beach which will hurt property values.  Netherlands has many dikes.
Not an earth scientist. But like u/CrustalTrudger said, this is more of an economics question than it is an earth science question. I do have a bachelor's degree in economics. The first thing that comes to mind, at least in the case of the United States, is the National Flood Insurance Program. [https://www.fema.gov/national-flood-insurance-program](https://www.fema.gov/national-flood-insurance-program). The government is absorbing part of the cost of flooding from land-owners and renters. So that is why you don't see prices declining in coastal areas. That, and some people believe that fossil fuels really will be phased out before rising sea levels become irreversible.
We're not good at believing in, or visualising, or preparing for events which we (individually or collectively) have never experienced before. "A big storm is coming" we can prepare for. "The sea is going to rise dramatically over coming decades and it's not going back down in the foreseeable future" is beyond the scope of a human lifetime so is difficult to handle. A more community/collective-driven society might fare better than an individualistic one, perhaps.

Also, I believe we're far more inclined to look for positive benefits than negative risks. I'll happily spend a lot of money on a marginally better computer, but won't spend a little on a backup drive. I'll spend a lot on a dining out every month, but can't/won't spend money on insurance. etc.. The prospect of living with a sea-view and being near to a beach vastly outweighs the (as above, hard-to-imagine)  dangers/risks of living in low-lying coastal land.
This thread have been locked due to the high number of anecdotes and medical advice requests. Some good answers have already been provided. 
Hi, medic here! It depends. For the sake of the conversation, I'm going to assume you mean something like a run-of-the-mill, basic cut. You nicked yourself shaving, or your knife slipped a little when you were cutting veggies. Nothing that would require sutures. 

Typically, its best to wash the cut with antibacterial soap (like Dial) and warm water, that probably constitutes "wiping it away", but only to get any dirt/nasties out of the cut. You do this because when you DO hold pressure to stop the bleeding, you don't want to keep the trapped nasties in there. So technically, it's better to take a little of column a, a little of column b. After your cut has been wiped clean, put steady pressure on it until it clots, then put a layer of protectant (neosporin or the like) and a bandage/bandaid/gauze over it to help it heal. 

Now, if you're talking about a trauma situation, it's a little different. Say, for example, you're first responding to a motorcycle accident and this dude's calf is just wrecked to hell, blood everywhere, gravel everywhere inside the laceration- your best bet isn't to keep wiping the gash continuously, you need to apply direct pressure first and foremost to stop the bleeding. Wiping it will do nothing and not allow the blood to clot properly. Pop a tourniquet on too. (This includes keeping the nasties and gravel and all, unfortunately, the hospital will properly clean and suture when they arrive. Motorcycle dude has bigger issues than a little dirt and gravel right now.) You don't want to be exsanguinated just because you kept wiping blood. Wiping continuously doesn't really promote clotting factors, nor allows platelets to form to stop the blood. It's why you are always taught "direct pressure" any time there is blood leaking from a wound that shouldn't be there. 

TL;DR: when in doubt, keep pressure on it. 

I hope this makes sense and answers your question, I'm on maternity leave, pregnant AF, insomnia and heartburn have kept me up for over a day. (: 

Edit: holy shit RIP inbox, I wake up because my son decided to falcon punch my bladder after I could finally go to sleep (weird sensation from the inside out) and I have 100+ comments, replies, and messages. Obligatory thank you for the gold! Now, a few clarity points:

For my first basic scenario, I used a common example of what might happen at home/what people have at home to use. Water and really any kind of soap is fine, Dial soap is just what I've been taught to use if I have it, and many of my colleagues do too. (I HATE it though, but I'm also allergic to a lot of other cleansing agents and have to stick with it.) Personally, I don't think water temperature is a difference/makes a difference in the basic cut situation. I cut myself in the shower shaving the other day (big 'ol 32w pregnant belly over here) and I just let it run in the shower, cleaned with soap, but it didn't clot until after I got out and put pressure. EVERY PERSON'S CLOTTING FACTOR IS DIFFERENT, your mileage may vary from others. It takes me 20 minutes to stop a simple little cut like that, whereas my husband just washes and slaps a bandaid on it and forgets about it in three minutes. 

On using alcohol/sticking your finger in your mouth- as long as it's rubbing alcohol or peroxide, I don't see why you couldn't dab a bit on. We carry it in the ambulance in situations like this. (Since we don't exactly have a sink with soap and water.) If the drinking alcohol (high proof) you want to use is fresh/unopened, or ensure that nobody has drank from the bottle directly. I've never used drinking alcohol as a sanitizer, so I couldn't tell you. As for sticking your finger or cut in your mouth- I'm completely guilty of doing this too, but just keep in mind that human mouths are NASTY AF! We treat human bites wayyyy more aggressively than dog or animal bites (save for snakes, because venom). Human mouths are just... filthy. I learned how gross our mouths are and made a conscious effort to stop sticking my cut up hands/arms in my mouth. Google some human bites, and look at when they get infected. They aren't pleasant. After seeing some in the field and looking at the infections they harbor, I wouldn't ever consciously do it again- but I mean, you do you. Like I said, I'm guilty of it too, and I think it's a bit of a reflex if I'm honest. (So many people do it!) I'm too tired to do any link adding, so if someone wants to add it, I'll put it here. (:

ON TOURNIQUETS:

A little background for those that don't know me: before I got off the truck due to my little parasite I have affectionately decided to raise, I work in a huge metropolitan area with a combo critical care/911 system. I'm mainly on the CCT (critical care transport) side, meaning we transferred people from hospital to hospital, higher level of care, etc. Got to see some gnarly stuff. We also backflowed our 911 areas when they got busy, so I've seen some crap there too. 

Tourniquets are coming back into the mainstream. Using a tourniquet is simple, easy, and (in my opinion) should be part of a public safety course/first aid/CPR for civilians. You have HOURS (if tourniquet applied correctly) before you risk ANY harm to the limb. (And remember! Life > limb, regardless. So that argument is out.) 

You do not need to loosen it every fifteen minutes. That's the exact opposite of what you're trying to do. You do need to check for pulses in the extremity very frequently- if you feel one, readjust till there isn't one. Double tourniquets are a thing, I've had to do it on people before. You are restricting blood flow. A pulse means that the person has blood flow and they will continue to bleed out. Once you apply it, hospital staff need to be the ones removing it, not you. In my trauma scenario, I pictured a free flowing bloody calf that got ripped to shreds by a gravel road and is squirting blood that you can't stop with direct pressure alone. (Guess I should've said femoral, I wasn't thinking straight. Either way.) Put the tourniquet on the femoral artery, as high as you can, and cinch it tight. If that doesn't help, double tourniquet it. 

Tourniquets are a very underrated emergency medical device, in my opinion. We carry several on the ambulance. They're good use for GSWs, deep cuts, lacerations, arterial bleeding from pretty much anywhere. It's easier to remove a tourniquet than remove a limb. We might use them more than you think. Open fractures that are bleeding and you can't put pressure on because bone is protruding, etc. They're very useful and can be used for several situations. 

Another local EMS company near me actually does civilian first aid training, and they are doing a good job in removing public stigma around tourniquets in our huge metropolitan area. "Stop the Bleed!" is really great too. 

As for in-field debridement: if I have the chance to squirt some saline and remove the dirt from a wound (and the bleeding IS CONTROLLED) I do en route to the hospital. If they're so messed up that I got called because of a GSW or severe accident, I'm going to be a little too busy maintaining airway, keeping BP up, etc to do much. We'll be at the facility that can take their time to clean it properly versus my crude knock-up job when I really need to focus on doing other things. If I have an extra hand or some firefighters, I might ask them to help me if I have everything else under control. (S/O to my fire guys, you rock!) 

I hope I answered all the questions and cleared some stuff up in the edit. I will totally try to get back to all that commented! Baby boy is craving some tacos, now I gotta go make some tacos at 7am... my husband will be very confused when he wakes up, lol. 
[deleted]
Neither, but more option B than A. Pressure is one of the keys with first aid for open wounds, and assists the healing process greatly. Certainly don't wipe, this will greatly extend the healing time and has potential to leave scarring.

[EDIT] Disclosure: I work with mines rescue and the first aid we provide is specific to the environment we work in and may not be applicable to all scenarios. Nontheless, basic first aid doesn't really change.
Depends on other factors like whether the cut is likely clean or not, how bad the wound is.

If you cut yourself superficially  while preparing raw meat or digging in dirt, for instance, it is  best to cleanse the wound before bandaging to ensure you’re not leaving bad bacteria or debris in there. 
If you’re gonna remove blood, dab or soak it up with something instead of wiping. Wiping can irritate the wound, reopen it, and generally make the owner of the cut have an even worse day.

Pressure is key on cuts of any size. After cleaning it as well as you can, use something clean and preferably something without a lot of lint to apply pressure, as it can get stuck in the wound. Continue applying pressure until  A: It has sealed up on its own and the bleeding has stopped, or B: The wound can be properly stitched shut by someone trained to do so is it is large enough.

By applying pressure, you not only soak up the excess blood with the cloth/gauze/whatever, you are also leaving blood on the cut to harden and help close the cut and let it scab over.

Don’t pick the scab.
The real answer is pressure and irrigation depending on the cut, and what resources you’ve got at your disposal. Yes, blood coagulation is what stoops bleeding, but it’s better if you clean the wound first 

If it’s a slash on the neck, or something like that, apply immediate pressure to the wound, improvise with a t-shirt or something. At that point all that matters is slowing the bleed before they shock out and die.

Smaller cuts like on the finger, just pressure and wash it, and cover with a bandaid. Most bacteria would be washed out, and depending on what cut you, like if you were cutting onions or whatever, you’ll be fine without a topical antibiotic. 

Large arterial bleeds like if you’re bleeding from the femoral (thigh) or brachial (upper arm) arteries, you’re better off using a tourniquet, which can be easily improvised with a strip of cloth and a sturdy stick or something. I’ve seen tourniquets improvised with a t-shirt and the spare barrel from an M249 Squad Automatic Weapon.

Other bleeds like inguinal ( groin ) or axillary (armpit) require packing and pressure dressing, which is something best left to trained professionals. 


Tl;dr 

Hold pressure, call for help 
Don’t know if it’s been added, but if this is a small cut near the finger tip or just surface level, applying pressure in the area in a rolling motion around the cut can rupture really really small capillaries, which in turn further triggers the clotting cascade, which helps the cut clot faster. 

So for small cuts, I’ve always cleaned it, then applied pressure to the site and pressure to the areas around it 


[deleted]
The outward pressure of the bleeding process works to prevent bacteria/other bad things from getting into your body.

Initially you should rinse or wash the cut to reduce the amount of fighting your body will have to do during the healing process (remove most of the contagions)

Then you should cover the cut with a bandaid and let yourself bleed.

The blood contains what your skin needs to undergo optimal healing, as well as providing a primary layer of cover (scab) to block out any further contagions.

Rub but don't scratch your scab when it's itchy. Don't pick it off
Wash the cut if you can. If you have it, put Neosporin on it and cover with a bandage. The most important part here is to not let it get infected and dried out. If it dries out, it will get irritated, take longer to heal and probably leave a scar. 
 In the Mark Whalberg movie Lone Survivor, a team of SEALs comes under fire and are trying to escape bad guys. At one point they all tumble down a rock face and end up with bad gashes and proceed to pack their wounds with dirt.

Is this last resort sort of thing? Is this safe to do? 
[removed]
My father is a fire investigator. I asked him the same question. He showed me photos of the last one he determined the cause. All the knobs on the stove were off besides one. It melted obviously being on. The people had left the stove on. They start at the area that has the most fire damage then look for something that isn't how it should be. 
In this case, it was easy - the fire was seen when it started, reported, firefighters attended and extinguished the fire in that flat - but not before the fire spread to the outside of the building. The questions to be answered here are engineering ones - why a cladding material that would have been designed and tested as safe proved to be so unsafe in practice.

But even in less obvious cases, the source of the ignition is often obvious. When ignition happens, there is lots of oxygen there, so things burn completely. When the fire gets going, there's less oxygen available, so things burn partially. Fire generally burns up - so the source of a fire is often the only thing on the floor that is badly burned.

Edit: Lots of good replies to my comment - including some fire investigators that state that the source of the fire is usually less combusted than the surroundings, as they burn cooler before the fire gets going.
As a firefighter myself, we tend to have a general idea of where the fire started due to witness accounts and phone report. Once on scene we pay attention to where most of the smoke and fire production is coming from, and after extinguishing the fire we look for stuff like "V" patterns on the wall, alligatoring (aka charring) on wood, melted plastic etc which is usually pretty good indicator of what direction the fire spread from and work almost like arrows pointing in the direction of the original source, after that we can narrow it down, we let the investigator know and then they probe deeper with looking/testing for signs of accelerants, looking at wiring, etc
I'm sure someone who understands how burn patterns work will be able to explain it to you in the more general case ... but it has to do with the fact that things don't burn out completely, and the scorch marks are different where the fire started and was small.

In this particular case, however, a big helping hand came from the fact that the guy who lived in the flat where it started told people that his fridge caught fire, and it went from there.  Don't have a source right now, but some of the initial reports of the fire quoted some guy as saying his neighbour told him it was his fridge.
I'm a certified indoor firefighter.  At the fire academy, we learned how cheaply houses are built nowadays and what to look/listen for in a fire.  A lot of buildings now have what are called truss roofs.  One beam is stretched from wall to wall and smaller beams are fanned out from the center of the beam to support the roof.  To attach the supports most of the time, an aluminum plate is attached and get this, staples or rivets are used, not nails.  

When a fire is hot enough, it will make the plate expand, causing the staples/rivets to "pop out", making this metallic pinging noise.  If you hear that noise, you need to get out NOW because the roof is coming down.  Construction doesn't include the attic vent sometimes.  This vent is designed to allow the flow of superheated gases to escape the top of the house, preventing a flashover.  Everything is made of plastic nowadays, some which burns at 1200 F, while your turnout gear is good to 800 F.  So many dangers to be aware of in fires nowadays.  I love it when people ask me if I have a fear of being immolated.  I always tell them that I'm too busy thinking of other things, to be worried about that.
There are lots of comments in this thread which are citing "known" things from the field of fire investigation.

In recent years, it's being discovered how shoddy much of this field is. Scientists have started looking into some of the supposed indicators of various kinds of arson practices and described them as ["witchcraft"](http://cen.acs.org/articles/90/i37/Forensic-Science-Innocence-Project.html), like lots of other forensic science which the National Academy of Science was [highly critical of in 2009](http://www8.nationalacademies.org/onpinews/newsitem.aspx?RecordID=12589), particularly in the fast and loose way in which uncertainty of various analysis methods is portrayed in court. In addition, the National Academy found that some forensic practices have no basis in science at all, including hair texture analysis and handwriting analysis.
[removed]
Most people don't know this, I think, but there's an entire field of study about fire science. You'd be surprised what you can tell just by looking at a burning building about what started the fire, where is it burning already, and what's the safest way inside, if need be. 

Not to mention even after the fire is extinguished, experienced firefighters will be able to tell easily where the fire probably started. There might be darker burn marks on certain surfaces, or in this case, they probably saw a fridge half-melted and wires with their cheap coating melted clean off. 
[removed]
There rumours before the investigation started that it started from a fridge freezer. The man went and told his neighbour his fridge caught on fire, he even had time to go in and get his belongings, no one really fathomed what it would become.
What I find very interesting is how did the fire escape the place (apartment/flat) where it started and get to the outside of the building? And as well, how did it manage to burn into other higher areas (through windows, I'd guess?) 
Fire investigation is an art and detective case all in one. 
In a nutshell during fire operations in the back of every firefighters mind is preserving evidence and limiting water damage to the scene to ensure fire investigators can do their job. After the fire has been extinguished we start what is called salvage and overhaul. Overhaul is searching for hidden fires and hot spots. I won't go too in depth on overhaul because it's not pertinent. Keep in mind like I said before salvage is taking place the entirety of the operation. Now in this instance where a structure is so fully involved it makes it more difficult to find the source of the fire but the same principles and steps are still carried out. Fire investigators will start from the outside of the structure and move in and towards the source. Some things they may be looking out for is darkened roofing, fire trails, etc. for example lighting fixtures and furniture will point you to the source. If a piece of furniture is burnt up the most destroyed blackened area acts like an arrow to the origin of the fire. You follow what you can see until you typically find the most destroyed burnt area of a structure and you can usually call that the point of origin. Firefighters and investigators are always on the lookout for arson. Some ways to identify arson include, fire trails, empty gas canisters, fuel lines, accelerants, civilians near the scene who consistently appear at fires and tend to appear sketchy. People who set structures or anything really on fire tend to like to admire their work which can often be their downfall. Some other signs could be all important documents, expensive jewelry, electronics etc all missing from the structure which could be a sign of insurance fraud, especially if weeks prior the owner for some reason invested in fire coverage and took out a large insurance policy on their home etc. There are 4 different types of fire cause, natural, accidentally, arson, and undetermined.  In this case when it was determined it was an electrical source I can only assume some signs they noticed were, a v shape near the outlet of the fridge which indicates a short in the appliance, they may notices signs of arcing and beading around wiring. I could go on and on for days but if you want to know more I can always send you messages. 
Also note I've only been in the fire service for just around 3 years now. 
2 years Fire explorer in California and 1 year DOD firefighter in Anchorage Alaska. So if anyone would care to share more informations I would be happy to learn
Here's a glossary of a couple of words you may not be familiar with (not to insult intelligence)
Accelerants: a substance used to aid the spread of fire
Arcing: luminous discharge of current that is formed when a strong current jumps a gap in a circuit or between two electrodes.
[removed]
When wires get overloaded to the point that they start a fire they will often show signs of arcing. That is the electricity in the wire leaping from the wire to another conductive object will leave traces of that occurring because copper has a low melting point. Cut wires have a clean edge and arced ones have bulbus ends from where the wire has partially melted.
There are lots of indicators at a fire.

For instance, heat rises and radiates, so damage from a fire will be in a cone shape, back to the area of origin.

Also, different metals melt at different temperatures.  so if here you have aluminum and brass melted, and there you have just brass melted, it was hotter here.

Also, glass and plastic containers will be melted more on the side facing the fire, causing them to "point" to the area of origin.

So yes, there is quite a lot you can learn from investigating the scene of a fire.
[removed]
Former firefighter here.  I spoke with an investigator about one of our fires and they determined that an extension cord caused the fire. The only thing left of it was the metal blades inside the outlet.  No other cords in the house were burnt with the same intensity.
[removed]
Two of the most common techniques are calcination depth measurements using a constant force to determine the area of highest temperature exposure using gypsum wallboard and arc mapping if they suspect an electrical fire.

"The dehydration reaction, also known as calcination, is an endothermic decomposition reaction which occurs between 100 C and 120 C. When gypsum is heated in a fire, the dehydration follows the reaction in Equation 1.1 as solid gypsum starts to degrade, loses its strength and is eventually transformed back to the powdery material of calcium sulphate hemihydrate." http://www.civil.canterbury.ac.nz/fire/pdfreports/Chu04.pdf

Arc mapping can be found on wikipedia, but it's simply a technique to identify the first location two energized wires melted and arced.
[deleted]
The guy who's fridge it was allegedly packed all of his belongings then knocked on a neighbours door telling her that his fridge was on fire before leaving... the answer for this case is kinda disappointing but if any science guys could tell us what the answer would be for another circumstance thatd be pretty cool
I have two degrees within the criminal justice field. One in police studies and one in criminal justice itself. (Just for source purposes) Usually they find out the exact origin of the fire from something called "alligator tracks" now I know that sounds incredibly red neck but bear with me for a second. The reason these marking are called alligator tracks is because there is always a series of lines right above where the point of origin of the fire began that resembles an alligators back. I haven't read many of the comments so sorry if I'm reiterating on what someone else said. Just thought I could lend a hand. 
Think about a running river. You can see the water run down the river all the way to the bend. You can also see the water coming from up river. You can't see where the river starts yet. If you follow the river upstream you will eventually get to its start. Once there you see that a bunch of snow is melting and trickling down into the beginning of the stream. Now it's possible to say the melting snow is what causes the river. Big events like this fire are viewed as if you were standing at the river at the start. By following how the fire spread backwards you can get all the way to the very corner of a room. From there you might see the burnt up appliance as the only thing in that corner. Then you can look inside the appliance and potentially be able to trace the damage the same way all the way to the exact part of the appliance that started it all. 

TLDR; fire leaves a burn pattern similar to a river. Go backwards until you can't anymore. Tacos are great. 
You'd be able to identify flash marks, where an ignition occurred. You would be able to trace the path of the fire to its origin. 

You'd be able to tell by the circumstances of the fridge freezer, i.e a flash mark indicating a sudden fire and the electrical cord showing signs of fault, as opposed to combustion caused by exposure to pre existing flames. 

Eye witness accounts would be able to rule out where the fire didn't start, i.e. "I live in apartment C on floor 27 and I heard the alarm without any visual signs of flames in my apartment."
Actual Forensic investigator here (well... I am studying) 

Usually a fire will leave a big pointy arrow on the walls and ceiling (not joking) to where it started

It looks like a V as at the point of the V the fire is hottest and burns longest leaving more damage from the fire and smoke. 

Edit: they will test for accelerant's with a dog as it is the fumes that burn not the liquid itself 

 
I think it also depends on the severity of the fire. For example, the Ghost-ship fire in Oakland, investigators were unable to 100% determine the exact cause of the fire because it was so intense and destroyed a lot of it's own forensics.

Last I heard the closest they could pinpoint that one was some kind of electrical fault in the kitchen area. Most likely, but can't confirm, the fridge. 


ahem... probably time to remind everyone that this "cause" stated without any proof or substantiation has not been officially determined by ANYONE yet.

all we have are wild speculation from biased sources

https://www.theguardian.com/uk-news/2017/jun/17/grenfell-tower-results-fire-investigation-published-years
[removed]
[removed]
Oh shoot!  As a geoscientist and a huge Subnautica fan, I'm sorry to come in late on this.

**No**, the lava depicted in the lava zone is completely unrealistic (but *so* cool.)  Let me comment on the pieces of the answer that people have already given:

As /u/Little_Mouse points out, real underwater volcanism on Earth doesn't have much glow to it: the water cools the lava so fast that it's almost all dark except for a few glints of red.  Their video was taken at shallow depth by a scuba diver: here's a video from 1 km deep, similar to the lava zone in Subnautica:

https://www.youtube.com/watch?v=hmMlspNoZMs

No glowing pools, no red lava falls.  Water is a fantastic reservoir for heat, and the fact that warm water rises lets it carry away heat by convection really *really* well.

/u/PresidentRex has a great analysis of pressures and the phase diagram of water, but there's one thing they didn't realize: **hot supercritical water is always less dense than cold**, as shown in the graphs [here](http://www1.lsbu.ac.uk/water/water_density.html).  Thus, there will be no "stable layer of supercritical water": it would be buoyant, rise, and be replaced by cool water, carrying away heat by convection.

What if the layer of water near the lava surface had a ton of salts dissolved in it, so it was denser?  As /u/Bassmanbiff points out, the thermal radiation law applies to *everything*, not just rock: the supercritical water layer *itself* would glow.  But that's clearly not what we see in Subnautica, and in any case the water above this layer would still convect, rapidly cooling it just as if it were lava itself.

Finally, as /u/UniqueUserTheSecond points out, there's a thermometer in the game, and it reads 70 degrees C in the active lava zone.  That's probably a reasonable temperature, actually -- note that in the video I linked to, the submersible isn't damaged by the volcano's heat, and /u/Little_Mouse 's video was taken by a scuba diver swimming just a few feet from the lava!  But this is nowhere near the temperature at which stuff starts to glow -- no matter what stuff.

As a side note, several people are commenting on air pressure and O2.  One thing's for sure: the way Subnautica handles air and breathing at depth is completely wrong, and trying to dive the way you can in Subnautica would kill you dead.  Nobody in the real world has done a dive on pressurized gas to a depth greater than 700 meters, the people who've done it to a depth below 100 meters only do so with hours of preparation, a special gas mixture, and slow cautious pressure changes, and even then many people who've tried to dive below 300 meters have died.  The vehicles and seabases behave as if they are at sea-level pressure (if they weren't, they wouldn't implode if you take them too deep), but you can't just hop from 800 meters of pressure into your sea-level pressure vehicle without dying immediately.  And let's not even talk about how moonpools work....

Of course, a realistic approach to lava and air pressure wouldn't make for nearly as fun a game!
[removed]
I can't speak directly to lava coexisting next to saltwater at depth, but there's some other misinformation thrown about this thread that I wanted to clear up:

**Lava temperature**

Lava glows because of [thermal radiation](https://en.wikipedia.org/wiki/Black-body_radiation). This is linked with the concept of blackbody radiation, where matter emits electromagnetic radiation based on its temperature. All matter emits this radiation above absolute zero, but the color becomes visible to humans around 800 K (980 °F/526 °C) as a dull red. As temperature increases further, objects appear yellow and then white hot (possibly with a tinge of blue).

The in-game lava is a deep red, so it's likely on the lower end of visible thermal radiation (800 - 1000 K). So while it's possible the lava doesn't have the same composition as typical earth-like lavas, it can't have a temperature much lower (e.g. lead melts at 600 K, but you can melt lead without it emitting a red glow).


**Atmospheric composition and air pressure**

The planet in Subnautica could have an atmosphere of anywhere between about 0.4 to maybe 5 atm of pressure. The partial pressures (pp) of various gases in the atmosphere are the important part for humans. [Partial pressure](https://en.wikipedia.org/wiki/Partial_pressure) is neat because if you take out the percentage of each gas in a gas mixture, the partial pressure would be equal to that percentage. Earth's atmosphere is (currently) 21% oxygen and basically 1 atm at sea level; that means that the oxygen has a partial pressure of 0.21 atm. On Mount Everest with a pressure around 0.33 atm, that's 0.07 atm of oxygen partial pressure. We need about 0.15 atm of partial pressure to breathe over the long term, but we can survive in less for brief periods (minutes/hours). 

On the other extreme, our bodies would suffer if the [composition in a high-pressure environment was not just right](http://www.physiology.org/doi/abs/10.1152/jappl.1970.29.1.23). Non-noble gases start having detrimental effects at high partial pressures - including oxygen. Oxygen-related problems can start at 0.3 atm pp (aside from a risk of fire, this is one reason why we don't use 100% oxygen atmospheres at earth-like pressures in spacecraft), but up to 2 atm can be used [for short periods](https://en.wikipedia.org/wiki/Breathing_gas#Oxygen). Carbon dioxide starts negatively affecting us around 0.06 atm of pp. Nitrogen narcosis is also well known in diving circles. The only somewhat safe options are neon and helium, and they can even start affecting our cell structures at extreme pressures. This really only applies to a human on the surface breathing air, though. 


**Pressure at depth**

(Tiny edit: I should note that these pressure calculations are based on normal earth gravity. Higher gravity means more pressure for equivalent depth; lighter gravity means less pressure for equivalent depth.)

Atmospheric pressure ends up being of little concern once you get deeper. The water pressure exerted at 1300 m of depth is about 130 atm. Adding 0.5 or 4 atm on top of that is miniscule. Water at normal temperatures is still just a normal liquid at this pressure (as we can experience here on earth diving into deep ocean trenches). Nobody is really going to dive that deep on a regulator though; you'd need a pressurized tank to breathe (otherwise the water pressure would collapse your lungs) and the gases will do unpleasant things to your blood and body once you start breathing gases at those pressures. There are reasons the current free dive record is 214 m and the scuba record is 333 m. 

As an explanation for the [phase diagram for water](http://www1.lsbu.ac.uk/water/water_phase_diagram.html): Temperature is the horizontal axis (in Kelvin); pressure is the vertical axis (usually in Pa and/or bar). The basic ice/water transition is the vertical line around 273 K (0 °C, naturally). In the big graph on that page, E is basically normal earth conditions (293 K or 20 °C and about 1 atm or 1 bar of pressure). Pressure in water (like any fluid) increases with depth. The rule of thumb is 10 m of water = 1 atm of pressure (technically it's 10.33 m = 1 bar, but everything else I wrote is in atm and 1 bar is just about 1 atm). This means you increase pressure by 1 atm each time you go another 10 m down.


**Phase state at pressure and temperature** 

Water is a normal liquid at 130 atm at standard temperature. Water is a supercritical fluid at 130 atm at 800+ K. (I wouldn't recommend swimming in it; it'll do unpleasant things to your body other than just burning you). This means it will be stable and won't turn into steam because it's already a weird mixture of steam and water. Unfortunately, my chemistry isn't good enough to tell you how salt is going to affect this in detail (other than to say that solids tend to dissolve better at higher temperatures and pressures so it could be denser).

So, at the very least, it's at least plausible for the lava to sit there covered in a layer of supercritical, denser saltwater.

to people answering, i want to point out if we go super technical, this water must have a property we do not know about, there's light from the local star at 100+m depth, which is just impossible on earth and the player goes to depths of 300m with only a high tech wetsuit. This means i would imagine that the water on this planet is less dense than water on earth which might allow for other possible interactions than the usual
The other possibility is that if the rocks are certain mixtures of lead and/or mercury and other low melting point metals, the lava could be much cooler than it is on earth.  Some have melting points around 30 C.
No, not really. Well, for a hot second maybe. At midocean ridges lava cools almost instantaneously on contact with water, so quickly that rocks with basalt compositions have glassy shells. For example [here](https://www.youtube.com/watch?v=hmMlspNoZMs) is an underwater volcano in action. You'll see that even very hot, not particularly viscous melts don't have time to flow much before freezing.
In general underwater lava tends to act like [this](https://www.youtube.com/watch?v=xsJn8izcKtg&t=1m15s) where you have a thin layer at the surface cooling while the remainder continues to flow, breaking out and expanding in a chaotic fashion.

Sometimes VERY hot things can persist in places where they would not normally be able to due to the Leidenfrost effect (where a thin layer of superheated steam insulates the rest of the hot object.) Looking at pictures, I can't see any way that you would use this effect to produce anything like the lavabeds shown. Maybe at best a short waterfall of glowing lava that then disappears into a lava tube, surrounded by lots of boiling water and steam.
No. Water is an extremely efficient heat conductor, so the lava would cool down to ocean temperatures very quickly and solidify. This is constantly happening at the mid-ocean ridges, where lava is squeezing out and solidifying very quickly to form pillow basalt. [Here](https://youtu.be/xsJn8izcKtg?t=65)'s a video of this process from Hawaii. It's not a mid-ocean ridge, but it's the same process.
One thing to note is that in the deepest (and hottest) part of the active lava zone, the water temperature is only *70°C*. Although you instantly die when you touch the lava, *one of the vehicles can walk on it* (while taking a lot of damage)

So it has to be something that's a solid at 70° but also glows when its that hot
[removed]
[removed]
[removed]
The answer, as far as I can tell, is that we don’t know for certain. What we know about the development of pillows and bedding, however, suggests a few options. 

I found this site which details the history of the pillow. 

http://hankeringforhistory.com/pillows-throughout-the-ages-guest-post/

The oldest known pillows date back around 9,000 years and are made of carved stone. The shape suggests suggests an attempt at providing comfort...a smooth surface is much better for laying your head on, after all, than a rough one. It’s also been suggested by historians that pillows like this protected people from bugs. It’s possible, therefore, that cavemen used nearby rocks to keep their heads elevated. 


This is not, however, the oldest known bedding. That dates back 77,000 years and consists of compressed grass, leaves, and plant stems found at archaeological site called the Sidubu Cave. Interestingly, the specific plants they found in the bedding served as natural insecticides, suggesting that protection from bugs was at the front of mind due cavemen as it was for ancient Egyptians. It’s possible a mound of this material was used on top of the bedding as a makeshift pillow. Here is the study. 

http://www.sciencemag.org/news/2011/12/earliest-human-beds-found-south-africa

Lastly, the Japanese Geisha have used wooden blocks with cushioning on top to serve as a pillow. These blocks allowed them to protect their elaborate hair styles while sleeping. A log or other piece of wood, therefore, is a potential third option for cavemen pillows. 

In the case of rocks and pillows, there’s no reason grass and leaves couldn’t have been used as a softer barrier to increase comfort. We wouldn’t know that, however, since such technology wouldn’t necessarily register with archaeologists if it wasn’t shaped or processed on some way such as to distinguish it from other debris. 

Edit: the rocks are no longer imprisoned. TIL proofreading matters. 
[removed]
[removed]
Kind of relevant; a really fascinating thing in Nelson Mandela’s biography is when he describes sleeping on a mattress and experiencing pillows for the first time when he was a boy. The way he talks about it is so insane to me because I have always had such luxuries.

Edit* It’s from his book: A Long walk to freedom

Can’t be bothered with finding the part where he describes sleeping with pillows but here is a passage that alludes to it

https://ibb.co/egDt5H

[removed]
Soldier who slept in the dirt a lot. You don't *need* a pillow now. . .. .but I sure did want one a lot. 

Also, I believe I remember reading somewhere that ancient egyptians used a wood or stone platform as a pillow: https://vanwinkles.com/where-did-throw-pillows-come-from
[removed]
[removed]
[removed]
[removed]
[removed]
Great questions and answers in the comments. Very interesting. So the general answer is that, yes, they use sleeping platform back in the days to rest their heads and necks. I'm wondering, are there any implication if these sleeping platforms are not used over a long period of time? Neck and spine problem perhaps? Or will the body adapt?
[removed]
[removed]
[removed]
Re most comments in this thread that have been removed: /r/askscience is not the right place for speculation / personal take on the matter. 

A good answer will provide a reference showing stats that can address this question. "We don't know at the moment / those stats don't exist" is also a perfectly reasonable answer. 
TLDR: Yes and no. There is no direct connection between having and education and then getting more education as a result. However, education leads to more money and more money leads to more opportunities for education and more success in obtaining education, both for an individual and for that individual's children. It would be perfectly correct to say that the Wealth Gap has created an Education Gap because the American education system is so dependent upon local economies.

-----

Sociology professor here.

It can be stated as a scientific fact that [wealth inequality exists in America.](https://books.google.com/books?hl=en&lr=&id=FpkMaxCWUsMC&oi=fnd&pg=PP11&dq=america+wealth+disparity&ots=mfq68ureXx&sig=3pdwpch0YiMTgCa2g2QS8eoRVJU#v=onepage&q=america%20wealth%20disparity&f=false). It can also be stated as a scientific fact that [wealthy individuals are more likely to graduate high school, graduate college, and obtain advanced degrees](https://books.google.com/books?hl=en&lr=&id=-EiLBQAAQBAJ&oi=fnd&pg=PP1&dq=america+wealth+disparity&ots=UUSV9oWRtc&sig=uN3VqNhDi4H_TaB14KEdeIzlRXI#v=onepage&q&f=false). Therefore, the wider the inequality in wealth the wider the inequality in education.

However, there is a worse problem that's hidden in that statement. Education attainment is not a perfect indicator of how educated a person is. That's sort of counter intuitive, so here is an example:

Group A) A poor section of town has low property values which means less taxes that fund schools. This then means that the school cannot hire the best teachers because it cannot pay very well. This means that the education being received by students is delivered by average or below average teachers, that textbooks and campus facilities are more likely to be outdated, and that after-school activities and extra curriculars are more likely to utilize sub-par equipment.

Group B) A wealthy neighborhood has high property values which means plenty of funding for schools. They can afford high salaries, so many teachers apply to work here, meaning the school can select the best of the best. They also have money to keep their facilities and extra curricular activities up to date.

Now if both Group A and Group B have a perfect 100% graduation rate, can we say that the students that graduate have the same education? Yes and no. They passed 12th grade, but in one situation the academics were more rigorous and the education included more than just the bare minimum. So on one hand we can point to statistics that say America's graduation rates are increasing, so we must be smarter. On the other hand, our schools are getting less funding and in some neighborhoods the curriculum are more lax, therefore the graduation rates increasing may be showing that our schools are getting easier to pass.

Some studies have shown that [private schools emphasize critical thought and leadership, while public schools are more likely to emphasize discipline, cooperation, and obedience](http://archive.wilsonquarterly.com/sites/default/files/articles/WQ_VOL11_W_1987_Research.pdf). Is this a disparity? In a way. Leadership and critical thought are skills needed in management and entrepreneurial jobs, while cooperation and obedience are more useful in menial labor. This isn't specifically tied to school funding, but it is worth noting that education quality will affect job prospects and success.

Lawmakers and policy makers are well aware of these trends. To correct for this, America has the "No Child Left Behind" law which forces all public schools to meet at least a minimum standard of education. The problem is that some schools barely meet it, while other (well funded) schools exceed it by leaps and bounds.

Some states also have a "Robinhood" law which takes excess money from wealthy neighborhoods and redistributes it to poor neighborhoods to keep schools from becoming too polarized. These are effective, but not perfect, and I'm not an expert on this area so I'll leave it at that.

However, all of this just says that the wealth gap creates an education gap. So the next question is, does higher education create more education?

For an individual, no. Most people seek a particular degree, and then stop seeking education. If education snowballed like the OP implies that then we'd have individuals who obtain dozens of Ph.D's, and that's pretty rare.

It is common, though, that [educated parents are more likely to raise educated children](http://psycnet.apa.org/buy/2005-06518-016) (pay wall). There's a lot of different factors here, mostly having to do with parent's higher education meaning more income and more free time. At the same time, this social fact is why colleges have programs set aside to specifically help first generation college students. People who go to college when their parent's hadn't gone to college are [half as likely to graduate](https://education.cu-portland.edu/blog/classroom-resources/first-generation-college-students-graduation-rates/) as students whose parents do have a degree.

In summary, the answer is a soft yes. There is an education gap in america and it is growing, but it has more to do with the wealth gap and school funding than with education specifically.
[removed]
I think you need to define "we". This would be very different for each countries laws. Things like access to education, ability to use that education to actually benefit lifestyle, etc, will dictate the answer to this question.
Sort of.

To be clear, overall education rates have gone way up - more people are getting bachelor's degrees, master's degrees, and PhDs now than were historically. This is not such a good thing for bachelor's degrees - only about a third of jobs require such, but about 2/3rds of young people go to college, and about half of them are getting degrees. This is resulting in the value of college degrees dropping and inflation in what degrees people are asking for for jobs - there are lots of jobs that ask for bachelor's degrees now which really don't need to, but can because there's an excess of them.

However, on the other hand, in recent years, college attendance has leveled off a bit as a result of this.

We've seen a modest decline in overall college attendance in the last few years, but [we've been seeing the largest drop in low-income people going to college,](https://www.insidehighered.com/news/2015/11/25/study-finds-drop-percentage-low-income-students-enrolling-college) and low-income people were less likely to go to college to begin with.

But you asked about education. This, too, is so; children of more educated families are more likely to go to college.

Contrary to what many people might expect, this is not an American trend; in Norway, we've seen the same thing:

> [And what happens is that — even though it’s essentially free — only 14 percent of children from the least-educated families in Norway go to college, compared to 58 percent of children from the most-educated families, according to an analysis by a Norwegian education researcher, Elisabeth Hovdhaugen.](http://hechingerreport.org/in-norway-where-college-is-free-children-of-uneducated-parents-still-dont-go/)

Interestingly, this bottom-end rate is indistinguishable from the bottom-end rate in the United States (13%), suggesting that it is primarily cultural factors, not economic ones, which control college attendance.
[deleted]
[removed]
[removed]
[Here](https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/648165/HEIPR_PUBLICATION_2015-16.pdf) is a recently published compilation of official statistics from the UK that shows that the percentage of people in higher education in the UK has gone up from 42% in 2006 to 49% in 2016. Obviously, this means that there is an overall higher attainment in education in the general population, but if I understand correctly, this wasn't the point of your question. What I think you're asking is whether there is a bigger *gap* between those in that are high attainers in education and those that are not. And the answer seems to be yes! 

For instance, have a look at the gap in gender attainment. This has actually increased since 2006 - females are now 11.9 percentage points more likely to be in higher education than men. And this is just using one particular variable - gender. Once you factor in social class, parental involvement, income brackets, cultural differences, etc etc, it begins to show a much clearer view of the gap in attainment. [Race and class background](https://www.lambeth.gov.uk/rsu/sites/lambeth.gov.uk.rsu/files/The_Educational_Attainment_of_White_Working_Class_Pupils_-_Empirical_Evidence_2014.pdf) are also major variables that account for differences in attainment. 
[removed]
The Economist ran a nice series of articles with references to data that showed what they call a ["hereditary meritocracy" exists in this country.](https://www.economist.com/news/briefing/21640316-children-rich-and-powerful-are-increasingly-well-suited-earning-wealth-and-power) A combination of a knowledge based economy that values education, high tuition costs, and rising "enrichment" spending by the rich mean that the gap between rich and poor keeps spreading not he education front with things like SAT scores. 
No. Educational levels are going up by any measure just about everywhere. You can look at this site for the National Center for Educational Initiatives https://nces.ed.gov/programs/coe/indicator_cac.asp
There you will see that educational levels are going up in all OECD countries. So, you could ask if these stats are just about the more educated, but that doesn't seem to be the case. 

Look here for world literacy rates: https://ourworldindata.org/literacy

Developed countries are achieving higher levels of education and developing countries are becoming more literate. There are likely some pockets of exceptions such as parts of the Middle East where girls have recently been forbidden from attending school. 
Even though we are graduating more people from high school than the 1940's (as one poster here pointed out) Americans demonstrate an appalling ignorance when it comes to Science and Civics: http://sciencevibe.com/2017/02/14/the-appalling-ignorance-that-america-embraces/                                                                      
And this problem is made worse if you attend a school in a poor district.                                                                                       
In wealthier communities schools are better. They are able to afford better  teachers and facilities (science labs, media centers, technology) so the opportunities for learning are so much better:  https://www.theatlantic.com/business/archive/2016/08/property-taxes-and-unequal-schools/497333/ 
The answer is that government must get involved, at least as a facilitator, as this is a problem too big for local areas to solve many times: https://www.npr.org/2016/05/01/476224759/is-there-a-better-way-to-pay-for-americas-schools
.
**Yes**. In at least one form of your question. The wealth gap is manifesting an education gap. [Summarized in a graph](https://www.theatlas.com/charts/rJGC4qmzZ) from a [story about shifts in spending](https://qz.com/999078/the-new-conspicuous-consumption-is-a-lacrosse-playing-child-and-an-npr-tote-bag/)

Education is now a primary form in how [the wealthy signal their status](http://www.bbc.com/capital/story/20170614-the-new-subtle-ways-the-rich-signal-their-wealth). This makes education is a (new!) mark of higher income families, [who spend more on education, as a percentage of income](https://www.npr.org/sections/money/2012/08/01/157664524/how-the-poor-the-middle-class-and-the-rich-spend-their-money). This is visible in [many ways](https://fivethirtyeight.com/features/rich-kids-stay-rich-poor-kids-stay-poor/). And is well studied as [an increasing trend](http://journals.sagepub.com/stoken/rbtfl/SMSfpaQqTyleY/full).
[removed]
[removed]
[removed]
If you are taking about the US then no.

[There’s more people graduating higher education than ever before](https://www.google.com/amp/thehill.com/homenews/state-watch/326995-census-more-americans-have-college-degrees-than-ever-before%3famp) 

It’s extremely easy to get a higher education in the US. There are so many grants and scholarships that even the poorest of kids can still get a good degree if they are willing to work hard.
I did a little looking into it and there appears to be some evidence of a gap starting to form. The first chart of [this site](https://www.business2community.com/us-news/the-20-us-cities-with-the-worst-educational-progress-charts-01250158) shows educational attainment levels in the US over time. The final 10 years shows something potentially troubling, an uptick in the percentage of people dropping out before they reach high school, resulting in growth in the most and least educated at the expense of those in between. While this doesn't give enough information to speculate on why this may be occurring, or even if it is an anomaly in the steady increase in education at all levels, it does lend credence that an education gap may be starting to form. 
[removed]
[removed]
[removed]
[removed]
Non-human primates lack the neurological regions responsible for producing speech as well as the musculature in the throat. There are several theories of how language and learned vocalizations evolved in humans, songbirds, parrots, bats, and cetaceans (whales, dolphins), but a general consensus is that it arose independently several times. Some of my favorite neuroscientists who write about this are Erich Jarvis and Johan Bolhuis. Both are songbird researchers. Jarvis has a three part series on [YouTube](https://youtu.be/E0MtW9URFhg) about this if you want to learn more. I haven't watched it but have seen him lecture a few times and he does a great job explaining it. 

Also, I wouldn't refer to parrots as lesser animals in terms of intelligence. Corvids and parrots have exhibited a wide range of intelligent behaviors that was once considered only available to humans and some other apes such as tool use and recursive learning. A recent [study](http://m.pnas.org/content/113/26/7255.full) has shown that the density of neurons in birds' brains, especially parrots and songbirds, are comparable to humans and primates.
Parrots evolved to emulate the sounds of their surroundings to survive, to confuse the competition or predators. Monkeys' environment so far has demanded that they use their voice only for advance warning or intracommunal communication.
Most of these answers are mainly about Primates, but not much about Parrots.
Thing is, Parrots dont 'emulate' Human speech. They are just very good at repeating sounds. When a Parrots says 'Polly wants a cracker' its not really talking, it's simply Repeating <Sound A> as it has learned it gets an interaction when it does so. I cant think of any evidence that Parrots can put words in sequences or use them in any way that indicates an actual language. They can just reproduce the sounds they hear neaby.
It's not being physically incapable of producing sounds. The FoxP2 gene is a mutation that had allowed us to learn to communicate with words easily

There have been rare instances of people born without it. They are completely healthy otherwise but have great difficulty communicating using words
I have some problems with this question. Namely, it is important to consider the evolutionary context of a species rather than attempting to quantify its intelligence or abilities. Without being facetious, primates aren't capable of human speech because they're not human. They are however capable of complex communication and have [intricate social structures](https://books.google.co.uk/books?id=XsrhU2vV5PIC&redir_esc=y) much the same as our own. The evolutionary factors that acted on humans to drive our ancestors to develop complex symbolic languages did not act on other primates in the same way. But that doesn't necessarily make them 'less intelligent' than humans.

Instead I'd argue that the notion of more or less intelligent is meaningless. To compare species we would first have to determine some measure of how intelligent an animal is. But that is impossible. We often use IQ tests to measure intelligence in humans but these test only make sense within the context of the human mind. There are many tasks at which other animals are vastly superior to humans whether for example, that's navigation or [visual memory](https://www.theguardian.com/science/2006/nov/07/animalbehaviour.psychology). These animals are very good these tasks because it is important to their evolutionary niche. Just the same as social intelligence and tool use is important to humans. You can't separate the intelligence of an animal from the rest of its nature and it's intelligence can only be understood within that context.
Parrots can mimic human speech which primates cant do because of physiological reasons. But we've been able to actually teach language to a bonobo which is almost cooler! Kanzi the bonobo was taught words using a keyboard of symbols, and could answer questions or ask for things. 
Brain size relative to body size is a good indicator of how intelligent a species can be. In your question's case, parrots tend to have very large brains when compared to body size. 

Monkey brains are approximately 0.4% of their body mass. Humans at 2.2%. Dolphins at 1%. 

Additionally, parrots and macaws have double the amount of neurons per equal volumes as primates. I used to teach this in comparative anatomy. 
What, scientifically, makes these animals "lesser"? I think your question is misguided in origin. Birds evolved differently in order to mimic many sounds. Primates today have a common ancestor with humans we don't trace our lineage directly to them so it makes sense that we have abilities they lack. 

I think you should careful of thinking of any species as "lesser" because it's not scientific and won't help you to reach accurate conclusions. 
It's because of their throats. Human throat is designed in a way that it can be willfully moved in a way to produce delicate sounds. Primate's throats are not.

Primates do have a certain capacity to learn "language" though. Chimpanzees have been told sign language (of course they're not capapble to use it on a human level) and they even pass their language knowledge onto off-spring.

Just a side note, one shouldn't automatically assume that closer related to humans = the most intelligent animals. Capuchin monkeys possess some cognitive abilities superior to apes, and the same is possibly true of some cetaceans, corvidae and parrots.

Parrots produce sounds in a complete different way as humans, so as far as I know the way they produce sound is more different to us than that of apes such as chimpanzees.
I also find it interesting that after so many years of close proximity, canines are beginning to learn new skills like looking in the direction of a human pointing and I've even seen dogs learn new sounds in an attempt to say, "I Love You".  Apparently we're helping them to rewire their brains!
Perhaps the real question is why do you consider birds as "lesser"? Especially as the corvid family is known to contain several of the smartest animal species on the planet. Look up Betty the crow – her tool-using and tool-*creating* abilities have surpassed that of chimpanzees (and frankly many humans).
It would also seem that humans are incapable of any other non-human animal's speech either. It's very interesting that we often take on anthropocentric views on everything, forgetting that we're not that more advanced.. Compared to the thousands of languages that we've invented and learned for ourselves, we can't understand or speak with any other animals on this planet... 
I would love for someone to delve further into the science of nonverbal communication. I know primates can't utter human verbal sentences, but we've certainly seen multiple primate species capable of understanding and producing basic sign language.

We also know of other species learning to associate human words with objects, colors, numbers, actions, etc., obviously dogs, but also horses, dolphins, etc. 

It all involves the difficult to define concepts of what is a "language" v what is "Language" itself? 
They can though. Look up 'talking chimpanzee' on YouTube for an example. Dogs are also capable of reproducing human speech, although I'm unsure as to what extent they can. Look up 'talking animal' on Wikipedia for some good examples and look up 'talking dog' on YouTube for dozens of other examples.
[removed]
[removed]
Primates do not have the anatomical capability of fine-tuned vocals. They make up for it in other ways, however. The course of evolution later traded off some of their good traits for our larynx. Kind of the same concept as apes trading off strength for manual dexterity (that we have) later on. Evolution rocks! 
now to the parrots: they are able to learn sound patterns that may sound similar to human speech. they do NOT "understand" what it means. they simply adopt call patterns from other members of their flock.

the result is an acoustically similar sound pattern -- the production is different from humans.

articulation requires motor schemes involving the vocal tract. signs require cognitive processes (semiosis). some things are just there, others can be learned.

p.s.: we must distinguish the phonetic aspect (ability to make such noises) and the cognitive aspect (usage of such noises as signs). primates have all had a very reduced linguistic complexity; many tests were not totally adequate (cf. maturana & varela 1987: the tree of knowledge), focussing on "grammar" -- it would have been good to test cognitive and pragmatic abilities rather than purely linguistic ones. e.g., once kanzi said "kanzi sad" (in sign language) which is most remarkable: that was not the usual "get food"; that was a statement about an inner state due to the circumstances. that is of course difficult to evaluate (what does kanzi understand when she uses/hears "sad"?). ---- since it is very difficult to work on that, i believe this research field is not yet fully exhausted. 

p.p.s.: i prefer maturana & varela's definition for "languageing": "the coordination of the coordination of behavior", i.e., second-order self-regulation. that means, languageing (existing in language) is another level of biological organisation. this level is by and large not reached by other beings than humans. they have only few signs inside their coordination of behavior. what they can never say is "your dialect is funny" (self-reference) or "at home we will pretend to not have hunted anything" (lying requires alternative worlds in cognition). it seems bonobo individuals are close to that -- only close, not there.
It's a matter of vocal cords. But remember that parrots and corvids (crows and ravens) may mimic our speech but that does not mean they necessarily understand it. Alex the parrot might have even been an exception to his species. (African Grey) 
Excellent Minutephysics video explaining exactly this. [Why is the sky dark at night?](https://www.youtube.com/watch?v=gxJ4M7tyLRE)   

Summary:  

* Universe had a beginning so there aren't necessarily stars in every direction
* Some of the far away stars light hasn't reached us yet
* The really far away stars light is red-shifted towards infrared (not visible to the naked eye) because of the expansion of the universe. 

Edit: To add in some points from the comments.  

* Yes some of the light from distant stars is blocked by dust and other objects in the way. The dust tends to absorb visible wavelengths and re-emit in the IR range which we can’t see but that wasn’t in the video so I didn’t include it in my summary.   

* Inverse-square law for light intensity. Intensity reduces massively over interstellar distances but that doesn’t really help answer the question because every star does this. Multiplied by an infinite number of stars in every direction, suddenly that tiny bit of light from each star adds up and the night sky should be far brighter than it is. For why it isn’t, I refer you back to the video and my original 3 points.
[removed]
The existing top comment correctly realises the OP is asking an age old question, that of Olber's paradox. The top comment though goes on to make some mistakes, the first is the solution of the paradox and the second is crossing the CMBR with the paradox which are not related.

The paradox is: If the universe is infinite then in every direction there must be a star. In such a scenario the whole sky would be a uniform brightness, the same brightness as the surface of a star in fact.

The paradox was first resolved long before we knew about the expansion of space, with a finite speed of light and a finite life time for stars there is only so much of the universe that each star can be illuminating at once. Imagine a shell that has a thickness equal to a stars lifetime propagating through the universe at c.

We later learned that not only would an infinite universe not be bright that our universe is not infinite, there is a observation horizon due to it's expansion and a start point 13.7bn years ago. This defeats the entire premise of the paradox where every single line of sight direction intersects with a star.

While you can explain the lack of light from distant stars as being due to redshift, it is answering a question already answered and is being a bit dishonest anyway since, you are going to be caught out in several other aspects of the more classical solution on your way to a more complicated unnecessary solution. For example, if you were to work out the average redshift of each unit solid angle in the sky you would find the sky would be much brighter than it is, and much MUCH brighter than the 2.7K you rattled off.

This 2.7K is where the mistake really lies is in equating the redshift from distant stars to the CMBR. The CMBR was not emitted by stars (which are the subject of the OP and Olber's paradox) but by a global distribution of hot gas circa 380,000 years after the big bang. 

The biggest difference here is that the CMBR was in every direction, unlike stellar light which is only where a star is, it was also initially much cooler (<3000K) and importantly this was emitted long before - and therefore much more heavily redshifted - than the light from even the earliest, most distant stars.
Here's a video with a great explanation of both the parodox and the answer.

https://youtu.be/yQz0VgMNGPQ 

The paradox is basically.

1) If the universe is infinite then no matter where you look eventually you will directly see a star.

2) If every point in the night sky directly leads to a star then the entire sky will be as bright as all those stars. 

I've seen a lot of responses about light dropping off in intensity based on distance however you have an infinite number of stars so it doesn't really matter how little light each star provides.

The correct answer to this parodox is that the universe is not infinitely old. So light from stars far away from us hasn't had time to reach us.

The expansion of the universe will prevent us from ever having a sky as bright as the sun because most stars will always be too far away for the light to reach us.
Incredibly sad to see that no one has provided the correct answer. While it is true that the red shift from the expansion of the Universe does play a role, it's a minor one, at least at this point in time, and the inverse square law is entirely irrelevant as the amount of stars at some distance grows with the square of that distance, which cancels out the inverse square law of the intensity of the emitted light.

EDIT: For the CMB the expansion of the Universe is the key factor for not lighting up our sky and in that regard /u/Surprisedpotato's answer is correct , but not when it comes to star light as in OP's question.

The real answer to your specific question is time. The Universe has a beginning, a birth. The Big Bang happened a finite time ago so light from distant stars have only had a limited amount of time to travel, which means that light from the furthest reaches of space haven't had time to reach us. And most of the light emitted from the far reaches of space will never reach us, because of the expansion of the Universe, which is why that does play a role but that role will be a bigger, more important one, later on in time.
The question you are asking is famously known as [Olbers' Paradox](https://en.wikipedia.org/wiki/Olbers%27_paradox)

The main to arguments given are due to:

- The Doppler effect, the further a galaxy is away from us, the fast it travels away from us, thus the light shifts into the infrared spectrum in our perspective which is not visible to the human eye.

- Light from stars that are very far away has not reached us yet.
we don't see see as much light from far away stars because the photons are less 'densely spaced' with more distance. Think of a star like a ball with lots of holes in the surface, and little bullets shoot out of the holes. If you stand right next to the ball, you'll get hit by a lot of the bullets, but if you're further away, you're less likely to be hit. The further away you travel from the source, the more spread out the projectiles wills be, as they travel in a straight line outwards form the center of the star. The bullets are the photons, or rays of light from the stars. Definitely don't think that was a good explanation but I hope you get it
I just always thought about it like a shotgun, if you’re right in front of it you’ll catch all the bbs, but if you’re far away you may just catch one or none. But judging by me not understanding most answers here I’m probably way off. It just doesn’t make my head hurt as much..
The strength of the light from distant stars (ignoring dust and whatnot blocking it) is based in an inverse square law. Let B be brightness and D be distance. B = 1/D^2. So if we moved 5 times farther away from the sun than we are now, the sun would be 1/25 as bright.
This is one of the primary ways that we know that the universe had a beginning, and is not infinitely old.  There are (probably) stars throughout the universe in every direction, but the light from some of them has not yet had time to reach us.
Space is not perfectly empty. In between Earth and any given star, there is all kinda of dust, particles, rocks, gravitational distortions, and radiation and that absorb, deflect, scatter, and redirect the light before it gets to Earth.
Light behaves just like any other electromagnetical wave, getting weaker with 1/(r^2 ), r being the distance to the light source because it spreads out to a increasing volume. So even if there is a lot of stars, and they are very bright, they are still far away and there is a lot of "empty" space in between. The light doesn't get lost, but most of it simply does not reach earth. 
I think the biggest effect comes for the fact that the universe expands faster than The speed of light. 

We can see less than 15 bly back at which point there is an event horizon where light from things beyond that is too slow to ever reach us. 

That light would in effect be moving backwards in time relative to us and can never be seen

Every second billions of miles of space. Galaxies and stars are is falling over that universal event horizon relative to us dissipating the light energy that would otherwise make the sky bright white. 

If it was not so the sky would be brighter than white all the time and visibility would be impossible. 
[removed]
Light can't travel freely through space, there's plenty of interference from dust bodies and non light-emitting bodies such as planets. Dust has a tendency to absorb all visible light, but not necessarily infra-red. There's also the issue that the stars are all so distant that the visible ones appear so small.
Light can travel freely yes but there are also objects that can alter the path of light. For instance a black hole can bend light in towards the centre. Also as the distance between galaxies increases the red-shift of light also increases until a point at which the light is so dim that it barely registers. There are a lot of reasons why light from other corners of the universe don't illuminate the earth but these are just a few 
The light from the most distant stars gets lost at what is called the [Hubblesphere](https://en.wikipedia.org/wiki/Hubble_volume). There is a point where things get so far away that the expansion of the universe between two distant points is greater than the speed of light. So no matter how much time passes the light will never reach us. If the universe wasn't expanding at an accelerated rate the night sky would be much brighter. 
Well, light doesn't infinitely expand to fill any volume. I believe much like the cone from a flashlight, the further away, the less densely packed the photons are, and thus the less intense the light is. The further from the point of origin, the less intense and illuminating it is.

I'm sure there's a lot more to it, but light isn't infinite, so as it expands into larger and large volumes it becomes less and less intense (ie: less densely lit).
It is lit all the time. But we have the sun on one side blasting us with much more light. If we think of the dark side of the earth as the constant and the sunny side as over saturated this makes more sense.

If we turned off the light from the sun the earth would be more evenly lit, just at a much lower level than daylight.
I'm way too late to be seen here, but the answer is Flux (ie the amount light going thru a 2d plain). Light comes out of a star radially, so the flux is reduced the farther it travels. Less flux, less intensity, less light. The sun is millions of times closer than the nearest star, so it's light is much more intense
It is perfectly lit. Depends on your definition of perfectly lit though. 

What is perfect? 5500 kelvin all round? 10000k? 30k?

The earth receives as many photons from stars (and reflected moonlight) as there is light emitted from that side. The stars are so vanishingly distant though, and the earth is such a small light bucket for photons from any given star, that it is incredibly dimly lit. That dim light can’t compete against the enormous amount coming from the sun. 

It is perfectly lit on all sides on the basis of how many photons it receives on each side, it is a bit imbalanced though given how close the sun is. 
As many others have made relevant theories,
I’d like to add that a lot of our artificial light reflects off the atmosphere and back down to earth. Where the external light is diminished by the internal light.

Go to Big Bend, Texas and you’ll see a lot more light from what you’ve seen. At night of course.
It is. This goes back to the question "If the universe is infinite, then any direction you look there should be a star, so why isn't the sky white?"  
It is it's just in the microwave region not the visible spectrum your eyes see and it came from the big-bang before stars were formed.
It isn't lost.  It spreads evenly in a spherical manner, so the light released can be thought of a spherical surface.  So all the photons can be thought to be uniformally distributed over 4XPIXr^2 surface.  So the number of photons of light that reach the earth is proportional to 1/r^2, where r is the distance from a star.  We get a ton of photons from the Sun, because it is relatively close, the next closest star is about 270,000 times farther away than the sun, so we get 1/270,000^2, or roughly .000000000014 times as many photons as we would get if it were as close as the sun.  That is the closest star, most stars are much, much, much farther away and because of the inverse square law, we would get even fewer photons from those stars.

However, if you are ever somewhere on a moonless night without light pollution, you will see that you can see dimly by starlight, because although there are relatively few photons from distant stars, you will still see some light.
Congratulations, you've just rediscovered [Olbers' Paradox.](https://en.wikipedia.org/wiki/Olbers%27_paradox)

The answer is: There just *aren't* stars in the black parts of the sky. Or at least, any stars that came into existence there did so too recently for their light to reach the Earth yet. The light isn't 'getting lost', it just was never there in the first place.

Partly this is because the Universe has a finite age. It's only 13.7 billion years old, so light has only had a chance to travel 13.7 billion light years since the Universe began. (Well, turns out it's actually about 46 billion light years due to the expansion of space, but yeah.) Anything farther away than that just hasn't become visible to us yet.

However, even as the Universe ages infinitely, we wouldn't expect it to fill with arbitrarily large amounts of light in every direction. For one thing, there is only so much fuel for stars, and after some tens of billions of years star formation would just stop, and the larger stars would quickly 'burn out'. Red dwarf stars would last much longer, but eventually after a few trillion years they too would 'burn out' and the Universe would go dark. This is what we expect to happen in our own future. For another thing, the expansion of space 'stretches out' the light going through it and makes it dimmer. The more light there is, the faster it gets destroyed by the expansion of space, setting a limit on how much light there could be flying around at any one time (given a certain density of stars).
There is a name for your question, it is called Olbers Paradox.
https://en.wikipedia.org/wiki/Olbers%27_paradox

This is a German Video, but it addresses part of your question perfectly.
https://www.youtube.com/watch?v=OylN0gaPqNE

The guy is a professor of astrophysics.
He says that the universe is not infinidetly large because then the sky would be perfectly lid all the time.
He also says there that the starlight becomes dimmer with the square of the distance.
So to answer your question

1. the universe is not infinitely large so naturally there are "empty spots" with no light
2. extremly far away light is so dim that we cannot see it (With our eyes)
3. Some Galaxies are so far away that their light has not reached us yet.
4. some light is also red-shifted so too much for us to see.

You probably know about the cosmic background radiation that can be measured everywhere in the universe.
If you count this as light, the the earth is indeed perfectly lit all the time ;-)

Light intensity decreases with 1/(r^2 ), where r is the distance.  

If your eyes could function like the hubble space telescope, you'd see all the stuff by just standing there with open eyes and waiting for enough light from the source to come to form a decent image. 
That's how hubble's deep field was created, it just stared into a direction for a long time. 

Other relevant factors: ISM, other absorbing matter, redshift. So even if our eyes were like hubble, we still wouldn't see all things

If there was nothing but stars & planets in the universe, then we would see everything with hubble-like eyes. 
The earth *is* perfectly lit all the time. However, the intensity of light from very distant sources (stars) is **so dim** compared to near sources (our Sun), that it's relatively negligible. 

Also, consider the [Inverse-square law](https://en.wikipedia.org/wiki/Inverse-square_law), wherein light intensity is inversely proportional to the square of the distance from the source. Considering that even the nearest stars are over 4 light years away, that's a **lot** of intensity dropoff. 
Space isn't a perfect vaccuum, there are dusk particles that block light. This is why you can't see a good portion of the galactic plain of the milky way, and the core/buldge of are galaxy; the light is obstructed by dust.
You can do an experiment yourself with a small, low power flashlight to experimentally see why.  Not a laser pointer.  A white light flashlight.

At night, look at a book using the light source within a few inches of it.  You will be able to read the text easily.

Now, set down the light, pointing across a room or long hallway, and take the book at least ten or so feet away.

You will find that there is far less light to read by, if you can read the text at all.

But, why is this?  Because the illumination has expanded in an arc.  The greater the distance from the point source of light, the less light hits the pages, and the more light will bypass the illuminated object.

Other stars are vastly more distant from Earth than our sun.  The Sun illuminates the Earth like the flashlight a few inches from a book.  The light from other stars is from so far away that we can barely see it, like a flashlight miles from our book.  Just a faint twinkle in the sky.  The other stars would have to be much closer to Earth, far greater in number, or vastly more energetic to project enough light to illuminate Earth so that our eyes can see it.

But it can happen, in one scenario.  SN 1006 was a nearby supernova, which is believed to be the cause of some ancient stories about a star in the sky that was barely bright enough to cast shadows.  It was still much less bright than our moon, but it was VERY bright for a star.

That illumination was brief, however, only a couple days, and the exploding star was nearby.

EDIT: Spelling and capitalization.



[removed]
Space is full of stuff like Gas, dust, rocks, planets, dark objects like black holes etc which absorb and block the transmission of light.

Combining that with the inverse square law (further something is away from the observer, the less total light they receive) means that for the majority of far away objects the light is either too dim to be detected or has been blocked en-route.

Would be my overly simplistic explanation. 
Well you started on a huge fallacy right off the ripper. Light does NOT travel freely through space. Space is full of particles and matter which absorb light. Some areas are so dense with junk no visible light gets through at all.
I'm sure this will get lost at the bottom, but here goes...

&nbsp;

I do not understand why so many bright, educated people seem to believe that the inverse square law is **not** sufficient all by itself to account for the lack of light. Even in an infinitely old universe, without expansion, this property of light is enough, because "math".

&nbsp;

The amount of light (aka [illuminance](https://en.wikipedia.org/wiki/Illuminance)) that reaches an object (e.g., Earth) from another object (e.g., [Alpha Centaury](https://en.wikipedia.org/wiki/Alpha_Centauri) 3-star cluster) is reduced by the square of the distance, aka the [inverse-square law](https://en.wikipedia.org/wiki/Inverse-square_law). Note: [Proxima Centauri](https://imagine.gsfc.nasa.gov/features/cosmic/nearest_star_info.html) is currently the closest star in that system and will be for as long as humans are likely to be around.

&nbsp;

If we assume a simple, but ultra-dense universe (far denser than it is) of an average of 1 star every light-year away from us, then we find out what the lower-bound maximum light would be. If light intensity dropped off not with the square of the distance, but the distance itself, we would have a simple [harmonic series](https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)) which is [divergent](https://en.wikipedia.org/wiki/Divergent_series) meaning that the sum keeps getting bigger and therefore approaches infinity with infinite numbers of stars further and further away. This would cook us in no time. **However** because, light has an inverse square property, it is not a divergent harmonic series, but a [convergent](https://en.wikipedia.org/wiki/Convergent_series) [Basel problem](https://en.wikipedia.org/wiki/Basel_problem). What the heck does that mean? It means that it does not matter how many more stars you add further away (not the same distance from us), the sum of the light from all the stars will be bounded to a maximum.

&nbsp;

The above assumes that there is never more than one star at distance x from us, which is probably not true and therefore not enough to encompass a universe where if you go far enough away from us we would expect more and more stars at the same distance from us. In theory, in this static, eternal, infinite universe we've imagined, as the distance approaches infinity, we might expect the number of stars within the sphere at that distance to also approach infinity. Fortunately for us, not all is lost. Some things approach infinity faster than others, and this quirk can keep an otherwise seemingly divergent infinite series in check.

&nbsp;

Let's start with the simple, but divergent case: If we make the distance from us "x" and the intensity of light "1/x^2", then if for every new unit of distance we add on average one star, we have a problem. This reduces to y = x/x^2 = 1/x which is the divergent harmonic series above. At this rate, our universe would be wall-to-wall stars in no time and, yes, we would be blinded by the light. Lucky for us we know by observation that the rate of new stars added to our surrounding sphere as we go further out must be considerably less than this. How much less? Well, just a tiny bit less is enough. For example, let's assume a tiny reduction in the number of new stars y = x^0.9 / x^2 (in stead of the above y = x^1.0 / x^2 ) the problem goes away. This new series reduces to y=1/x^1.1 which is convergent. As you can recall, this means that the amount of light reaching us will never be infinite **even though the number of stars at distance x as x approaches infinity also approaches infinity**). If the observable universe is to be trusted, then we can safely assume that the rate of "new" stars as we go further out is **much** less than this, and as such, we don't need dust, expansion, red-shifting, or anything else to account for a mostly dark universe even in this static, infinitely large and infinitely old universe we've imagined since the amount of light reaching any one point will be fixed at some maximum. Now throw in the dust, expansion, red-shifting and non-infinitely old age of the universe, and you have all the more reason why the sky is not a blinding light turning us all into plasma before breaking us up into some exotic sub-fermionic soup caught in a blast of infinitely intense electromagnetic radiation.
Hey all,

Please remember that in /r/askscience we require _accurate, in-depth explanations_ to our questions and their concerns. Please refrain from posting analogies and "ELI5" explanations which don't directly answer the question.
A CPU can only work on stuff in its cache and the RAM of the device (be it PC / Mac / console / mobile / etc). However, such memory is volatile, and loses all its data if it is not powered. To solve this problem, secondary storage exists: hard disk drives, DVD drives, USB disks, flash memory, etc. They hold persistent data that is then transferred to the RAM as and when needed, to be worked on by the CPU. 

Now, when a computer boots up, a lot of its core processes and functions are pre loaded into RAM and kept there permanently, for regular usage. (The first of this stuff that loads is known as the *kernel*.) They are also heavily dependent on each other; eg, the input manager talks to the process scheduler and the graphics and memory controllers when you press a button. Because these are so interconnected, shutting one down to update it is not usually possible without breaking the rest of the OS' functionality*.

So how do we update them? By replacing the files *on disk*, not touching anything already in memory, and then rebooting, so that the computer uses the new, updated files from the start. 

*In fact, Linux's OS architecture and process handling tackles this modularity so well that it can largely update without a restart.
[removed]
The only correct answer is that it is simply easier to treat the code as immutable, and restart the program whenever you want to change the code. It is more than possible to design systems, even operating systems or other low level programs which don't need to be rebooted in order to update(this concept is called 'hot swapping'), but it is harder to design those systems and sometimes also harder to reason about their correctness. Imagine it this way: Rebooting to update software is like putting a car into a garage and upgrading the engine. Doing a live update is like upgrading your engine while you are going down the highway at 65mph.
[removed]
Windows places locks on files in use.  The reasoning is you don't want to open a file, make changes but not save, and then have something else make changes to the file and save them.  Because when you do save the file, you'll overwrite the changes made by the other process.  So when your computer is on, a lot of system files are locked.  If windows needs to make changes to one in a patch, it'll set a flag and upon reboot, make the change since the file will no longer be in use at that point.
I used to work on Windows, so I can speak a bit as to why the xbox needs this.

Windows requires rebooting because of a few key OS processes that cannot be simply replaced and restarted. For instance, lsass.exe, which is responsible for logging you in and taking care of lots of security "stuff", cannot be shut down and replaced at runtime. This could, possibly, be fixed. However, untangling the dependencies and sorting things out safely would be a nightmare.

There were so many things on Windows that would be a lot easier if back-compat wasn't so important. However, we always had to be sure the last 20+ years of applications would run after any changes. This makes things a bit tricky at times, to say the least :)

The reboot pain is understood, and that's why new features have been added over time to help make things easier. "Use my sign in info to automatically finish setting up my device after an update or restart." is one such baby step.

edit 1: Sorry if it wasn't obvious, but I'm talking about Windows because xbox runs Windows.

edit 2: Also, if the hypervisor is being patched, a reboot is almost always needed. Reliably hot patching the hypervisor is possible, but it's much simpler to reboot when applying hypervisor updates.
[removed]
Two reasons. Simplicity and Security.

The kernel is the main component of any operating system. Modifying the kernel while it's loaded into RAM is both *difficult* and a *security risk*. Modifying the kernel files on disk is much *easier* and *safer*. 

It's hard enough to write robust and reliable kernel code, trying to make it self modifying would be a nightmare. 

The kernel has the highest privileges of any running process, it has the permission to do anything. Technically it could modify itself,  but if it allowed for such functionality it would open a significant security hole. A virus that could get inside the kernel would be a nightmare. The reboot process includes an integrity check, which isn't possible to perform on a running process.

That being said, a modern OS now allows for hardware drivers to be modified live, via Loadable Kernel Modules. Core system updates still require a reboot though.
[removed]
Many updates include newer versions of system files and services the entire operating system uses.  While these files are running, they can not be updated.

If they are shut down, the operating system can become unstable or flat-out not work.

Some updates may include kernel or renderer updates which is the core of the operating system that controls every other system file or service.  In-place updates of these types of files or programs can not be done while it or anything else is in use.
Because things at the kernel level are really complicated and it's so much easier to just reboot rather than manage how all these low level components update without the system crashing.

Or, in other words, imagine asking a car mechanic to service your car while you are still driving. It's much easier to turn off the engine at rest and let him/her do his/her job.

This is about as layman as I can explain it.
They [don't necessarily](https://www.ubuntu.com/server/livepatch) at all, it's entirely a property of the particular systems you seem to have experience with. As far as I know it's possible on both desktops and servers, but on desktops it may have some additional caveats as servers typically have simpler setups, e.g. no graphical interface to complicate things. I couldn't immediately find out how difficult it is to pull this off for desktop systems.

It's easier to just expect people to restart a device though, and since there is little market demand for devices not to reboot, many companies are reluctant to put aside resources for this.

Even as a home user who knows that this is possible, I don't require continuous uptime, so I too just reboot after receiving important updates.
In principle it would not be necessary. While, as per the other responses, the natural way to do an update is to stop the old software and start the new one (a.k.a a reboot if the software is an Operating System) many Operating Systems (including Windows) support Hot Patching of almost all OS components. But in order to support Hot Patching a fix needs to be written with that process in mind, which involves converting any persistent data structures that change in format or content, informing any dependent components, etc. Doing so can easily double the effort to write a patch, and more importantly also lenghten its development and test period.
Most OS patches fix security vulnerabilities, so developers try to have them ready as quickly as possible (any minute a fix is delayed increases the odds of it being exploited by the bad guys), which means skipping Hot Patching altogether and getting the minimum viable fix on the shortest time possible.
Combine that with the fact that patches often get installed in bundles, so a developer may not feel it is a good investment of his or her time to work on hot patching when most likely some other fix will require a restart anyway. 
In many cases the fix gets rewritten after it ships with the incorporation of hot patching, but by then most people already installed the original one when it vane out, and so they already had to reboot.
So to summarize the reason is time to market (and in some cases laziness): requiring a reboot instead of coding a fix so it makes all required changes on the fly saves a lot of development time and when you are in a rush to ship something any minute counts. 
Source : I was a PM involved with reliability improvements for a major OS at the time this feature was being written. Biggest disappointment in my career how this feature ended up being ignored, but in all fairness I suspected what was going to happen when I started with it. 
Short answer: It’s cheaper, more secure and less costly. It has nothing to do with any technical limitations. 


The technical answer is; they don’t. The longer answer is called Return on Investment. The time it would take to make a OS, such as the Xbox Windows OS be patchable without requiring a reboot is too costly in time and in security vulnerabilities.

Look at it this way, if all the squares on your screen on the Xbox were just websites, you’d hit refresh and bam you may have a new version of the site. No rebooting. We can abstract this concept out all the way to the base kernel layer.

The “kernel” provides a set of common ways of talking to all the physical (hardware) pieces inside that Xbox. This way ever little app or game doesn’t have to know the specifics of the physical pieces inside the box. It’s told hey there’s something that can do math, something that can draw on your tv, something to make sound, etc. and it doesn’t care how it’s done because the Kernel knows for you. For those that want me to say HAL. There I said it.

Now, let’s say App Netflix is talking to the Internet and all of a sudden the Xbox is told there’s an update to the Kernel for the Internet piece. We can still keep the old version running and provide the new service for when you restart Netflix. But, what if the update included a security fix where a hacker can get into your machine if you keep using the old version? Now the Xbox has to force you to shutdown Netflix and reload it. 

Now let’s take that a bit deeper. The Xbox itself uses the Internet for updating news feeds, player scores, friend lists, etc. before you know it there are 20 “parts” are are required to be restarted or else the security hole will still exist. All this is still technically possible to so without “restarting” the box. But at what point are you not restarting the box?


[removed]
The absolute need to reboot depends on at what level you are applying the update.  You often need to access and replace locked files that are associated with these process and services.  Many can be stopped and started without a reboot but if these are at a sufficiently low level (e.g. associated with the OS Kernel itself) then you do need to restart the OS itself.
Because in order to update specific components you need to have a system above it to handle the update. While this is possible it is often hard.

For instance suppose that a computer game has a patch and it involves updating the executable and a file containing a model (the graphical information like the vertices and colours, etc).

For the graphics file you could program your loader in such a way that you can invalidate models and be forced to unload them, then switch to the new one however this means it disappears on screen. You also might end up clipping because the new model has different sizes, and hit detection stopped working for a moment. You need to handle all of these cases.

Alternatively you can make it update the file on disk but keep the old one but what happens if it's multiplayer and two people see different things? What happens if you're forced to unload the image (like if you alt tab and lose the graphics memory).

The executable is even harder - you need to build your executable in such a way that you can lose a part of the game for a moment then plug in a new one seemlessly. There's a lot of reasons that this is hard (optimization being the most difficult one as well as rewriting the entire executable in such a way to make this possible).

Instead the game waits until it's not running to update - it's much easier, less bug prone, and good enough for 99% of applications. It leverages the operating system to do this.


Operating systems are the next step up in complexity. You can write everything in such a way to let everything be able to disappear during an uninstall but it's very difficult to do so - Linux has tried but certain components still require a restart. You could write an operating system above the operating system - often called a hypervisor and done in data centres - but then how do you update that?

The difficulty in writing software to update on demand is the primary reason it's not done. Trying to retrofit an OS to do it would be a buggy nightmare. Generally speaking restarting is annoying but hardly damaging for regular consumers, the massive number of bugs would probably annoy you more.

-A Microsoft OS dev
There is no abiding need to restart anything, but to perform a hot update you need to make sure that the new program image is compatible with the data created by the old one. It’s costly to do so, that’s why most companies don’t bother.

It’s roughly analogous to updating a video game: why bother making sure that the new code can handle all the internal data structures made by the old code (and most updates do introduce changes to those) and that they remain consistent with its expectations of how they fit together when you can just save the game, quit and reload it?
[removed]
Really at its core, the answer to this question is that they *don't* strictly **need** to given the technical constraints of the hardware... but it's much easier to write the *software* in a way that they do need to be restarted to perform an update.

Here's a few of the difficulties with updating during operation to give you an idea:

* Updating during operation requires you have to have very well engineered "boundaries" between different parts of your system to do an update while running. If the parts of the system that aren't being updated and want to keep running know anything "internal" about the part that is being update, that internal part might change in an update such that the other system's knowledge isn't correct anymore. By analogy, consider an update that changes the engine in a car from gas to electric. If the driver was specifying their speed via an amount of gas / second then the update isn't possible, vs if they were specifying it via a target speed it would be fine... but specifying a target speed is more complex.

* Updating during operation requires you to be able to save and reload the "state" of a component very with fine grained accuracy. When you update a given component on the fly, you can't just have that component start back up fresh, it has to start back up *with exactly the same configuration that the old version had* just prior to the update. With our car example, if you update the engine during operation, you have to make sure that the new electric engine is spinning at the same RPM as the old gas engine was otherwise it's going to blow out the gearbox.

* Finally, updating during operation means that you have to have finer grained control over dependencies between systems so that make sure none of the components that depend on the one you're updating try to use it during the instant in which you're switching it over from the old one to the updated one and restoring the old state to the new one.

Updating during a restart of the system before you start any of the normal systems avoids all of these issues because:

* None of the systems are running, so as long as all of the interfaces match up right before all the components have been updated and after all the components have been updated you're still good. You don't have to worry about any intermediate invalid states where the not yet updated controller expects to specify an gas flow rate but the updated engine expects an electrical voltage.

* Since none of the systems are running they don't have any state to save.

* Since none of the systems are running there's no dependencies to worry about.
[removed]
At least when installing things like Java (or Android Studio etc) the problem is the runtime environment. It has a lot of settings where programs and libraries can be found etc. The install programs change those to accommodate to the newly installed program. But it does not affect already running environments. So making a clean restart ensures that all programs use the same environment. 
It's easier.

Live patching is possible but it is far simpler to update a secondary kernel and simply set it to use that on next boot.


There is a push to implement live updates for critical server applications.  See kpatch:

https://en.wikipedia.org/wiki/Kpatch

Alternatively, it is often more robust to take a cluster approach where multiple machines process aggregate workload.  In this sense the individual machines still reboot but the service as a whole is not interrupted.
Computers and consoles don't _need_ to restart in order to upgrade. However, having a system that can upgrade without rebooting is much more complex than one that just reboots.

In fact, Ksplice was a company providing a kernel module for rebootless upgrades in Linux. It was acquired by Oracle, and now it is offered as part of Oracle Linux https://ksplice.oracle.com.
Typical OS have process running that may interfere with the new version. Take for example there is an update to Edge to version old to version new. Version old have internal processes that are running that are essiental to the functionality of the OS. What restarting does is shut off all old versions processes so it can be handed over to the  new versons processes without affecting the OS.
[deleted]
Usually it's because whatever the update is trying to change is actually being used by the computer/console all the time for proper function, and the only way to get the changes to take effect is to turn off so that the code can be reloaded. 
[removed]
It sounds like you're confusing [sidereal time](https://en.wikipedia.org/wiki/Sidereal_time) with [solar time](https://en.wikipedia.org/wiki/Solar_time).  A sidereal day is the amount of time it takes for the Earth to rotate by 360°, and is indeed 23h56m4s.  A solar day is the interval between two successive instances of the Sun crossing the [local meridian](https://en.wikipedia.org/wiki/Meridian_\(astronomy\)).  Since the Earth moves by roughly 1° around the Sun each day, the Earth has to rotate by roughly 361° for the Sun to cross the local meridian.  In [this image](https://en.wikipedia.org/wiki/File:Sidereal_day_\(prograde\).png), sidereal time is the difference between the blue circles labelled "1" and "2", where solar time is the difference between the blue circles labelled "1" and "3".
That's how long it takes for a 360° rotation of the earth. Since it's also revolving around the sun it requires more than a 360° turn for the same point to be facing the sun again. Therefore, a "day" as we know it is more than 360° and a lot closer to 24 hours than your estimate.
No, 23h25m4s is the length of a *sideral day*, which is the time it takes to face the same direction relevant to an object outside the solar system (such as the galactic center). A solar day (the time it takes for the sun to reach the same angle in the sky) is about 24 hours (give or take a few seconds on certain days, considering the earth moves at different speeds at different parts of its ellipse).

Leap years are a different kettle of fish. You see, the speed at which the earth rotates, and the speed at which it revolves around the sun are not connected, and the solar year is 365d5h48m46s (which is 11m14s less than 365.25 days) so we accumulate nearly a quarter of a day (to be specific, 24.219907407407407407407407407407%) per year, which would mean over time, our calendar would slip and we'd end up with summer in the northern hemisphere for christmas! over the course of 100 years, autumn would begin to creep into august, and spring into January, as the seasons shifted over by 24 days. So, we could add an extra day ever 4 years to fix it right? However, since the solar year is 11m14s shorter than that .25, we're now shifting 11m14s a year in the other direction. in 128 years, we'd be one day off. So we do better by NOT including an extra day on years divisible by 100. But we also knew we could do better, so we made an exception to that rule: If the year was divisible by 400 it WOULD be a Leap Year. Now it will take 3300 years to diverge for the calendar year and solar year to converge by a single day!
The 4 minute discrepency is our position relative to the stars, not the sun.  We measure days by the sun, not the stars.

So, one day is 1440 minutes.  One orbit is 365.25 days.  So every time we've completed one *solar* day - that is, the sun has returned to the same relative position it started in, we've moved further 1/365.25° in our orbit.  Leaving the stars 1440/365.25 minutes out of place - 3.94 minutes.

This "discrepency" doesn't need to be accounted for, because once it's happened 365.25 times, the stars are back where they started.

The leap year accounts for that 0.25 in our 365.25 day orbit.  So you can see where 4*0.25 becomes simple.
Your math is off, we add add the lap day because the solar year is actually about 365 1/4 days, we add the extra day every 4 years to account for that. We actually had to change it a while back to account for the fact that its slightly less the 365 1/4 

Heres a better explanation...

The exact length of a solar year is actually 11 minutes and 14 seconds less than 365 ¼ days. That means that even if you add a leap day every four years, the calendar would still overshoot the solar year by a little bit—11 minutes and 14 seconds per year. These minutes and seconds really start to add up: after 128 years, the calendar would gain an entire extra day. So, the leap year rule, "add a leap year every four years" was a good rule, but not good enough!

Calendar Correction, Part II

To rectify the situation, the creators of our calendar (the Gregorian calendar, introduced in 1582) decided to omit leap years three times every four hundred years. This would shorten the calendar every so often and rid it of the annual excess of 11 minutes and 14 seconds. So in addition to the rule that a leap year occurs every four years, a new rule was added: a century year is not a leap year unless it is evenly divisible by 400. This rule manages to eliminate three leap years every few hundred years.

https://www.infoplease.com/leap-year-101-next-when-list-days-calendar-years-calculation-last-rules
A lot of people are pointing out the difference between sidereal days and solar days, but there's an important piece to a proper answer that's (mostly) being neglected: leap days are to fix the disparity between how long it takes for the Earth to *revolve* around the sun once (8766.15 hours (+/-, depending on which definition you're using)) and how many hours are in 365 solar days (8760). That 6.15 hour disparity is what's corrected with leap days. Without Feb. 29, seasons would slowly (but noticeably) shift. Across a single lifetime, the winter solstice (in the north) could work its way to November. Leap days do *not* address anything related to how the Earth *rotates* on its axis (aside from the fact that that's how we count days).

If there really was a ~4 minute disparity between a 24 hour "day" and a 23h56m4s *solar* day, then you'd notice that the sun would rise/set earlier and earlier. In three months, you'd get sunrise at midnight and sunset at noon. Three more months, sunrise at 6pm and sunset at 6am.  
This clearly doesn't happen, and it's because a solar day really is (on average) almost exactly 24 hours. (See all the other answers about sidereal days vs solar days to explain where 23h56m4s comes from).

On a parting note, I want to actually answer "where does that extra [time] go?" Each year, it goes into the changing night sky. The constellations shift with the seasons *exactly* the way the sun would in my hypothetical scenario above. If you could see Orion even during the day, and you kept careful track, you'd notice that it rises and sets 36**6** times while the sun only rises and sets 365 times.
You are thinking of the rotation period of the earth. The earth takes slightly shorter than 24 hours for one revolution. 

However, after one rotation, it also has progressed on its orbit around the sun.  So after one full rotation, the sun is not in the same position. If you wait a bit longer (for a total of almost exactly24h), the sun is in the same position again.

The leap day has nothing to do with earth's rotation, it has to do with the time it takes to orbit the sun. The earth needs 365 and 1/4 (rounded) days to orbit the sun. The leap day keeps the year aligned with the orbit around the sun. Without the leap day, the seasons would slowly shift (at a rate of roughly 1 month every 120 years).
The sidereal day is 23h56m4s, which is the time for one 360° rotation of the Earth.
The solar day (from noon to noon) is almost exactly 24h, and is the only one that matters for our lifes.
If one day you see the night sky at 9pm, you will see the same sky the next day at 8:56, then 8:52 and so on. After one year, this difference will sum to one day, so you will returning seeing those stars at 9 (aproximately, because one year is slightly longer than 365 days)
Once you account for solar days vs. stellar days the *once-every-4-years* leap day turns out to be **too much**, actually.  Which is why we actually *subtract* leap days!

You know the leap days as once every 4 years, and yes, that's mostly true.  But did you know that every hundred years, we *don't* have a leap year?  Any year that is divisible by 100, and thus ends in 00 (1900, 1800, 1700, etc...) is **not** a leap year.  BUT, there's a second exception to that exception: Any year that ends in 00 AND is divisible by 400 (*i.e.* 1600, 2000, 2400) is **still** a leap year.
I'd like to congratulate you.
You took the base data, calculated the difference and came up with a reasonable question.
You were missing one variable, sidereal time, and you sought an explanation as to the variance of theory with observed data.
You learned, others learned. Science and logic FTW!
You're confusing a few different measurements here.

The 23:56:04 figure is for a *sidereal* day. That is, the time taken for the Earth to rotate once with respect to the background stars. But the calendar works by *solar* days, which are actually 24 hours (at least on average).

Notice how the difference (3 minutes and 56 seconds every day) is almost exactly enough to add up to one extra day each year. That's because the Earth goes around the Sun once per year, so the rate at which its solar days 'fall behind' has to come to one full day each year. (To illustrate, imagine if the Earth didn't rotate at all relative to the background stars. We would still get exactly 1 solar day each year.)

Leap day has nothing to do with the 3-minute-56-second discrepancy. It's due entirely to the fact that we insist on having the new calendar year start at midnight, and the discrepancy between the (non-integer) number of solar days in a year and the (integer) number of days on our calendar.
In 23:56:04 the planet completes one revolution on its axis. This is a sidereal day, and if you want to figure out where in the sky to find a star then knowing the local sidereal time is important.

During that time the planet also goes a little bit of the way around the sun. In 24:00:00 the rotation of the planet on it's axis plus the orbiting around the sun combine to get the sun to the same place in the sky (not accounting for seasonal variations). This is a solar day and is the most useful day for day to day life. 

In 365.24 solar days (or 366.24 sidereal days) earth completes one orbit of the sun. Our calendar approximates this as being 365 solar days most years or 366 solar days in leap years. 

Leap days are correcting for differences between the length of a year and the length of 365 solar days. The difference between a solar and sidereal day is correcting for the fact that over the course of a year the orbit of the planet "unwinds" one day. They're different things entirely. A different orbit could see a year of 365.01 solar days, needing a leap day only once in a century while still having a similar sidereal and solar day length. 
A day is 24 hours. If the day weren't 24 hours then after a period of months the Sun would be in the sky at midnight, or it would be dark at noon.

The year is not a full integer multiple number of days in length, however. And that would cause the seasons to shift their timing in the year over time. Which was what happened when the Julian calendar was in use. The Gregorian calendar adds or skips leap days in such a way that it takes about 3000 years for the timing of the seasons to be off by a day (which can be fixed with ad hoc leap days or skipping leap days, should civilization continue that long).

The difference you're talking about is the difference between the solar day and the sidereal day, or when the sun appears at the same place in the sky versus when the stars appear at the same place in the sky. What you'll find is that it is roughly 1/365th of a day shorter than a solar day. Over the course of a year the position of the stars relative to the solar time of day shifts over a complete rotation of the sky, because the Earth is in orbit of the Sun.
There are some pretty good explanations here, there's just one further that I thought would be interesting to add:

If you notice, the difference between the 23h56m4s and 24h over the course of a year is ~1 day. (Almost exactly if you use the exact length of a year and sidereal day, but still off by a little, hence leap seconds.) That is a clue that the difference is a result of orbiting the sun. In fact, if the earth did not rotate about it's axis, a solar day would be equal to a year, and the difference between a solar day and a sidereal day would again be 1 full day. That is exactly what happens if you are on one of the poles, where the sun rises and sets once a year.
This is a conflict between systems. Consider this, from wikipedia

"In astronomy, the Julian year is a unit of time; it is defined as 365.25 days of exactly 86400 seconds (SI base unit), totalling exactly 31557600 seconds in the Julian astronomical year."

That is an artificial simplification in which the daily discrepancies just get tossed away, they don't accumulate. That is the difference between our simplified system and the real world is fairly constant.

Leap days are used to adjust the Julian calendar in order to be able to keep Stonehenge alignments in synch with equinoxes and solstices and keep track of Easter.
 
The algorithm for deciding if a year is a leap year starts like this:

If the year is evenly divisible by 4 it is a leap year,

Unless it is also evenly divisible by 100 which means it is not a leap year,

Unless it is also evenly divisible by 400 in which case it is in fact a leap year.

The algorithm usually stops here because we change the calendar more often than the next step size.

This calendar was designed to keep the seasons from drifting.


 
Earth actually rotates around its axis 3,56 minutes less than 24 hours. If we would not be adding 3,56 min to get a 24 hours day, for everyday demand to get sun on its exact spot on every high noon, we would have on 21. december high noon sun on exact midnight hours (and/or viseversa).
Fun fact - the difference between sidereal and solar days means that the earliest sunset of the year is not necessarily on the shortest day of the year. As you near the winter solstice (shortest day) the difference becomes significant.

In the Pacific NW (for example) the earliest sunset is usually Dec 12 (ish). The days continue to get shorter until Dec 21st (or 22nd) because the sunrise gets later faster than the sunset gets earlier.

You are confusing the various times. A solar day is 24 hours which is the time from when a particular part of the earth is facing the sun, to when that same piece of earth is again facing the sun. The time you quoted is for a sidereal day, which is a similar definition but to a distant stellar body. The reason for a leap day is to account for the fact that these solar days don't align exactly with the earths orbit, which is a completely seperately defined length of time, at 265.2422 solar days. As you can see this number lines up far better with our leap system.
In fact if you want to take into account not only a full rotation if the earth, (360 degrees) and one day closer to one full orbit (365 days), you end up only having to rotate 359 or so to face the same position relative to the sun, which means 24 hours really is close. Close to a minute within 24 hours actually. Which is why if we loosely round to one extra minute each day, we end up with approximately six hours every year, thus the need to account for an extra day every four.
I'm pretty late to the party it seems, but 23h56m4s is the *sidereal* rotation rate (as others have said), meaning that each non-Sun star rises and sets every 23h56m4s.

But that's not why I'm here.

I'm here to point out an awesome Lewis Carroll piece about this.  Carroll wrote a bunch of humorous dialogues centered around conundrums of various sorts.  In one of them the big question is what is more useful -- a clock that's right twice a day or one that's right only twice a year?  Alice, of course, picks twice a year (but for the wrong reason) and her interlocutor points out that's silly, since the stopped clock is right exactly twice per day, and to know what time it is you just have to wait until the clock is correct, and then you know exactly what time it is...

Of course, a common sort of clock that's right exactly twice a year is a *sidereal* clock, and it's extremely useful for astronomy and stellar navigation.  
the length of a day is defined by measuring planets rotation relative to some other "fixed" astronomical object. For that we have one of two options, we can use the object it's orbiting, or we can use far away stars. 

In the first case you get synodic time, and since the earth is orbiting the sun, synodic time for our little rock is usually referred to as solar time. 

In the second case you get sidereal time. 

For the purposes of calendars and tracking seasons and blah blah all that every day use, we use solar time, not sidereal time. The mean solar day is defined exactly as 24 hours (or rather 86 400 SI seconds but whatever). However since a year is given by earth's rotation around the sun, we end up with a small time shortage on the calendar day since it takes about 365.24 solar days for the earth to complete one orbit. Since the calendar date is supposed to represent earth's position in it's orbit and thus the change of seasons caused by distance to the sun and blah blah etc  that .24 would cause the calendar to drift. Thus leap years. 

now sidereal time is mostly of use to astronomers and other folks who like looking at stars. It also gives us a "better" true day because the earths motion around the sun slightly extends the solar day by a little under 4 minutes in comparison to the sidereal period of motion. However this does not affect the calendar because our calendar isn't based off sidereal time. 
[removed]
You mean a sidereal day (time for the earth to rotate 360 degree arounds it's own axis).
Thr "normal" day is a solar day (time it takes for the sund to be at the same spot in the sky).
Here is a good picture:
[Thanks German Wikipedia](https://de.m.wikipedia.org/wiki/Siderischer_Tag#).
You see that the solar day (1-3) is takes a little bit longer than a sidereal day (1-2).
But the solar day is still not 24h, because of this there are leap seconds (unly if needed but approximately about every 18 months).
[Wikipedia](https://en.m.wikipedia.org/wiki/Leap_second)

But the year is also unperfect.
The "standard" year ([Tropical year](https://en.m.wikipedia.org/wiki/Tropical_year) is about 365,24219052 days long but there was a leap year every 4 years in the [Julian calendar](https://en.m.wikipedia.org/wiki/Julian_calendar) so this also doesn't work.
Because of this we now use the [Gregorian calendar](https://en.m.wikipedia.org/wiki/Gregorian_calendar).
It's mostly the same but:
There is one leap year if the year is dividable by 4
except it's also dividable by 100
but there is one if it's dividable by 400.
For example every year until now - 2099 is a leap year but 2100 isn't nor 2200 and 2300 but 2400 is one again.
For further information:
[And again Wikipedia!](https://en.m.wikipedia.org/wiki/Year)

Sry for my bad English skills.
I'm Austrian and never pay enough attention in school. Shit,   And have to make a English presentation by monday ...

Edit: Link repair
Edit: Wrote this edit ^
Aside from your math being off and confusing solar and sidereal days, an extra day every four Februarys is not the only corrective measure we use. 

There are also leap seconds which are applied when needed, and also century years are not leap years unless they are divisible by 400. (2000 was a leap year, but 1900 wasn't and 2100 won't be) 
23h56m4s is the length of a sidereal day, which is how long it takes the earth to rotate once relative to far away stars. For our calendars, however, it is much more convenient to go by our rotation relative to the sun. This type of measurement is called a solar day. Going by the solar day, the calendar is off by about 1 day every 4 years, which is why we have leap years.
One year is one rotation around the sun and not calculated by 360° rotation time. Earth completes one trip around the sun in 365 days, 5 hours, 48 minutes and 47 seconds. 23 hours and 56 minutes day called sidereal day (aka 360° turn) 24h day is considered 361° turn. 1° is gotten from earth movement in space  ( as far as i know, feel free to correct me ).

[removed]
[deleted]
People have already answered your actual question, but as a side note, the leap day by itself is an OVERcorrection for our orbit around the sun. That is why any year that is divisible by 100 will not be a leap year despite being divisible by 4 (i.e. The year 2100 will not be a leap year). However, this TOO is an overcorrection, so years that are divisible by 400 are leap years (i.e. The year 2000 was a leap year)
ok so now we all have devices which update their clocks all the time, does that updated time change according to the seasons? i've noticed my microwave clock has differed from my iphone clock by a few minutes in the last two months (last time we had a powercut and i reset it) but it didn't change much in the six months before that (when we moved house and i last set it) or does it just happen the same amount every day, and my microwave is knackered?
If the earth orbits the sun in an elliptical orbit and the south pole is farther away from the sun in the winter( trying to remember 6th grade science), will the orbit change ever so slightly that over time so that the north pole will become farther away from the sun in the winter?
[deleted]
This will necessarily be a very broad generalization. 

All else being equal, in the US, since the start of the pandemic, of all people ages 70-79 who have tested positive for COVID-19, about 8% die before recovering. In other words, 92% survive. 

There are many more variables than just age. Men with the virus are more likely to die of it than women are. Underlying conditions such as diabetes, heart failure, and obesity are linked to worse survival rates. Survival is better now than at the start of the pandemic because we have learned some treatments that are effective, and some other things that we should avoid doing. 

Also, the above percentage only refers to deaths. There are other possible long-term effects. Even a relatively mild case can cause permanent damage to the heart, lungs, and blood vessels. The most severe cases can leave patients ventilator-dependent or with brain damage from lack of oxygen. So, "survival" may not be the best metric to pay attention to.
For a non-Hispanic White adult over 65 the chances of hospitalisation is 290.3 in 100,000 (source: [https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/index.html](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/index.html))

Specifically for someone 74, somewhere between 8-31% of people around that age will require intensive-care (ICU): Source: [https://www.statista.com/statistics/1105420/covid-icu-admission-rates-us-by-age-group/](https://www.statista.com/statistics/1105420/covid-icu-admission-rates-us-by-age-group/)

If someone in the 70-79 year age-group reaches Intensive Care (ICU) then their chance of survival is approximately 31%. (source: [https://www.cebm.net/covid-19/covid-19-declining-admissions-to-intensive-care-units/](https://www.cebm.net/covid-19/covid-19-declining-admissions-to-intensive-care-units/))

There are many other factors though. Diet, health, underlying health conditions, obesity, race.
Spain data:

&#x200B;

6 may: 70-79 years old: 13,9% death ratio. So 86% survived

September: 70-75 years: 6,4% death ratio.  93% survives

&#x200B;

We treat better, faster, and earlier so death ratio is lower. 

At the moment here in Spain, the average age of death for second wave is 86 years old. 

&#x200B;

The sources:

&#x200B;

[https://www.rtve.es/noticias/20201001/coronavirus-causa-80-exceso-muertes-septiembre/2043513.shtml](https://www.rtve.es/noticias/20201001/coronavirus-causa-80-exceso-muertes-septiembre/2043513.shtml)

&#x200B;

[http://www.telemadrid.es/coronavirus-covid-19/mortalidad-edades-coronavirus-fallecidos-Espana-0-2229077086--20200506114130.html](http://www.telemadrid.es/coronavirus-covid-19/mortalidad-edades-coronavirus-fallecidos-Espana-0-2229077086--20200506114130.html)
Its not exaxtly what you were looking for, but the CDC says people in that age bracket are 90 times more likely to die compared to somone at the age of 18-29
[CDC](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html)
Due to the nature of the topic discussed, this post has spawned several political arguments. The mod team would like to remind you that /r/askscience is a place dedicated to science questions. Off topic comments will be removed. We also have a zero tolerance policy on racist content.

We expect users to answer questions with accurate, in-depth explanations, including peer-reviewed sources where possible. If you are not an expert in the domain please refrain from speculating.
There is a fixed amount of water available in the basin that varies only slowly over decade time scales. So if Ethiopia builds a dam close to the source of the water and stores it there this will have results downstream. A minor effect would be the evaporation from the lake which would be lost to the region (the recycling factor in the Ethiopian highlands is small). A major effect would be a quick fill which would temporarily cut off water supply to the downstream areas. A long term effect would be that in times of drought Ethiopia has control over the distribution and can keep more water for itself. All of these are negative effects for Egypt's water security. As for the claim that Egypt's waterflow is increased by reducing Lake Nasser evaporation, this is really a wry statement. It means that they might reduce the level of Lake Nasser by siphoning of more water upstream thereby decreasing the volume of the lake and the area from which it can evaporate. That might slightly reduce evaporation in Egypt which is what they could mean by "increased water flow" but I don't see how Egypt's total water budget would increase because of this.

That said, if Ethiopia's dam is properly managed it might increase the overall water security of the region, something that would also benefit Egypt. It all depends on the amount of irrigation Ethiopia is going to develop with this dam.
A lot of folks mentioning evaporation, which is correct, but another factor would be infiltration. Depending on the hydrology of the bottom of Lake Nasser and the newly-dammed lake, surface water could be lost to the ground at different rates in both locations. 

Of course, groundwater infiltration may be a good thing for the groundwater basin that is being infiltrated, however if you only look at surface flow, you may not account for such a benefit.

TL;DR: You are losing water both out of the top and the bottom of a reservoir.
The dam will reduce evaporation (deeper water with less relative surface area), but Egypt WILL get less water while the dam fills (could take a year or more depending drought), and then they are reliant on Ethiopia to let the water flow. I see no reason why Ethiopia would ever send that "saved water" to Egypt. 
This is essentially what occurs in California as well. With the vast majority of water flowing from reservoirs in the Northern part of the State to dry areas in the Southern parts, Los Angeles and San Diego. 

There has been a long standing fight to build additional tunnels in the Sacramento area to allow more water to flow down south. Massive interests on both sides fighting for billions of dollars in water rights. Anytime state legislation may impact water flow in California an army of lobbyists and attorneys descend on the capitol.

The Water Education Foundation has some great resources to learn more about water. Here's a good link to California's Delta water issues.

http://www.watereducation.org/aquapedia/sacramento-san-joaquin-delta
Liquid will only evaporate from the surface exposed to air, and so is dependent on and relative to exposed surface area. As you fill a container with liquid, the surface area exposed to air will by necessity increase at a slower rate than the volume being stored, if at all (in the case of vertical or narrowing container walls as height increases). So the deeper your container is filled, the slower evaporation will happen relative to the volume of liquid contained within. 
Its like temporarily moving the surface area of Lake Nassir upstream to Ethiopia, and then adding a lot more volume in depth. The water in the middle and the bottom does not evaporate, only the water exposed to the surface is in danger of evaporating due to the hot and dry air of northern Africa. 

It will not just be better, it will be a LOT better. The average amount of water that falls in the mountains of Ethiopia is fairly well set. It will not grow. It flows down the mountains to Egypt, and in the shallow Lake Nassir, much of it evaporates into the air. 

To keep as much of the Ethiopian rain in storage as possible, it must be stored in a deep reservoir in the cool and moister high Ethiopian location, then flow it down to Egypt as needed. 

The hot and dry-air Lake Nassir MUST be made smaller, and the reservoirs in the high, cool, and moist Ethiopian mountains MUST be made larger.

Egypt hates this because it gives Ethiopia control over a vital resource, and they do not trust each other. If Ethiopia and Egypt were one country, this would have been done long ago...
[removed]
The proposal would reduce evaporation from Lake Nasser by decreasing its size. However, the upstream reservoir would have higher evaporation than as a river. So to validate the claim, you have to determine whether the decrease in evaporation at Lake Nasser would cancel out the increase in evaporation in the upstream reservoir.

You would use [The Penman Equation](https://en.wikipedia.org/wiki/Penman_equation) for this.

First thing to notice is that the evaporate rate is an area rate, expressed in either mass over area and time or depth over time, meaning that a deep water body with little surface area has less evaporation than a wide, shallow water body of the same volume. 

The equation measures things like sunlight, wind speed, humidity, and temperature. Basically, as you would expect, a hot, sunny, dry, windy area is going to have more evaporation than a wet, cold, cloudy area.

The conclusion is therefore that storing the water in a reservoir in Ethiopia would therefore result in a smaller net irrigation loss storing it in Lake Nasser. 

Edit: posted before coffee, missed a major wording error. 

Also source (besides equation): Have master's degree in civil engineering with emphasis in environmental and water resources engineering. Certified EIT in Colorado and Idaho. 

2nd edit: I really need to not try to post technical things before my brain wakes up. Fixed another major wording error. For the *same volume*, wide and shallow has more evaporation than deep and narrow. Hot and dry has *more* evaporation than cold and wet. 

Although you could argue that deep has less evaporation than shallow even with the same surface area as the deep water would act as a heat sink. But that conclusion would come from the equation, not from the unit analysis. 
I think it's shocking nobody mentioned that this dispute isn't just about water, but the nutrients it holds as well. The Ethiopian mountains are rich in untouched silt and other such soil which are vital for the Egyptian agrarian economy. A dam would block most of this fertile soil and reduce Egypt's output significantly, which it has depended on for MILLENIA. Im not saying that Egypt has the right to this soil since it originates from Ethiopia, but I do believe that this will finally start a discourse over compensation...  
Personally, I think Ethiopia and Egypt should be more afraid of the public health implications of this dam. Dams can greatly change the ecology of a river, which allows disease-causing parasites to flourish. An example would be a dam in Ghana that allowed a specific species of snails (which carry a parasite called schistosomes) to flourish, which greatly increased the prevalence of the disease Schistosomiasis in West Africa. 
The project is focused on power production. The dams location on the border of Sudan/Ethiopian border, limits it as a tool for irrigation within Ethiopia. 

However, the dam will ensure the blue Nile flow steadily, which will allow Sudan to use a greater part of the water. So the dams effect would be similar to that of the Aswan dam but for Sudan, but unlike the Egyptian Aswan dam, Sudan does not control the amount released, limiting its usefulness compared to the Aswan dam. 

Drought is not a consideration. The region have a reliable monsoon season where the Nile stile overflows in Sudan and fill lake Nasser, which is slowly emptied during the year though the Aswan dam. As the blue nile will flow steady, the artificial lake will not get (over) filled in the monsoon season, but the steady flow into lake nasser will make it drain slower. ~~Due to it being a very shallow lake, there will be less evaporation over all.~~ So due to Lake Nasser being shallow, storing part of the monsoon water in the new lake leads and continuously filling Lake Nasser from it will lead to less evaporation overall.

This makes the response from Ethiopia correct. Assuming that Sudan does not take this opportunity to better feed themselves. All the actions of Ethiopia technically benefit Egypt. Therefore, I read the response as: Not our problem, make a deal with Sudan or explain why Sudan must see the water overflow their fields to benefit yours.

Considering that Nile stile drains massive amounts of water (around 1/5 of what reaches the Aswan dam ) into the Mediterranean sea. I think Egypt should learn to share.  

[removed]
Yea exactly . As another commenter said , if they were one country , that country would do this no question - because the country would save from the evaporation and also be generating Electricity and whatever other benefits .  
Some argue that draining [Lake Powell](https://en.wikipedia.org/wiki/Lake_Powell) would increase net water supply of the Colorado River. They state that the sandstone surrounding it leaches or wicks water from the lake, and quickly evaporates it.
This problem is exactly the kind of thing that you can rapidly simulate and evaluate using tools like GoldSim (just happens to be the one I have used.)

Mining companies use these tools to determine water management policies.

You can incorporate management options, black swan events and even stock market variability. It’s really fun to do!
Your third link is HTML encoded and so it doesn't work. ("%20" -> "-")

Here's the fixed link: https://www.al-monitor.com/pulse/originals/2018/01/egypt-world-bank-intermediary-ethiopia-renaissance-dam.html
[removed]
[removed]
As i understand it, building a dam would generate a lake just before that dam. 
Letting water from this lake through the dam would then generate power and when the lake is filled the rate at which the water is let through would be the same as before the dam was raised unless there has been evaporation from the dam in which case there would be let less water through.
(the size of the lake is constant. the water let through the dam is the lake inflow minus the evaporated water)

What i don't understand is how you can have a smaller lake Nesser. You would expect that this lake is formed so that a certain amount of water is needed for anything to flow on the the river Nile. If there is less water in lake Nesser there will be less water for the river Nile.
Is this correct? 
A distal injury won't affect another injury until it begins needing more resources than the body has to distribute - take burns for example. 

If you had a burn on your hand all sorts of plasma and proteins and immune related cells would be rushing to the site (some already there) causing both local inflammation and an immune response that ultimately results in a blister - the blister is full of immune cells that help to repair the damaged tissues by providing an ideal micro environment for healing. Now let's say there's a burn to a large portion of your body; depending on the degree and the inflammation response (3rd degree burns have a different response as many of the biological channels of cell repair are completely destroyed) while your body will send out all its required immune cells that it has it might simply not be enough - in this case bacterial infections can take hold in the blisters as they provide an ideal environment for certain infections to grow, this results in sepsis and eventually septic shock. Imagine that the bodies immune repair system is spread too thin to repair both burns - it doesn't have a very good system at establishing where it should send immune cells with regard to controlling sepsis beyond directing blood away from the extremities and towards critical organs as septic shock progresses. 

Ultimately it depends on the nature of the two injuries but yes they could affect one another.
A fun bit of trivia:

In polytrauma injuries involving a traumatic brain injury and bone fracture, the fracture may actually heal at a faster rate than the same fracture alone. The mechanism is currently not well understood, but some researchers speculate it's related to enhanced macrophage mobilization.

EDIT: For some more formal reading, check out some of work being performed in small animal models: [here](https://www.ncbi.nlm.nih.gov/pubmed/25682315), [here](https://www.ncbi.nlm.nih.gov/pubmed/26636276), [and here](http://globalprojects.ucsf.edu/project/murine-model-polytrauma-understanding-molecular-basis-accelerated-bone-repair-concomitant). Then, even more exciting, check out [this study](https://www.ncbi.nlm.nih.gov/pubmed/22323691) that looked at human patients with combined TBI + fracture injuries. Finally, to try to sum it up as best as we can, check out these lit reviews: [here](https://www.ncbi.nlm.nih.gov/pubmed/25873754) and [here](https://www.ncbi.nlm.nih.gov/pubmed/15710151)
Common to have multiple injuries, like in the surgical ICUs from traumas. Healing comes down to infection control, nutrition (usually the issue) and rehab.

With large body surface area burn patients, very hard to feed them enough, even with a feeding tube 24 hrs a day to meet calorie requirements to heal.

Edit for additional info: Acute critical illness can lead to catabolism exceeding anabolism, even in adequately nourished people. Obesity, is equal or more risk for wasting.
To answer the question simply, yes, absolutely. 

Different types of illness have unique calorie requirements. 

A polytrauma patient is different than a septic patient is different than a burn patient with regards to protein, carbohydrate, electrolyte, and fluid requirements. 

A dietician who works in the ICU could probably answer this question a lot more thoroughly than I can. I usually just write the order for a nutrition consult and leave it up to them. 

Source, am a doc, although this is definitely not my area of expertise. 
[removed]
I'm a pain management physician. There's a low likelihood that two separate injuries would significantly affect each other unless they are major (2nd/3rd degree burns, broken bones, etc).

Two injuries in the same area, however, if the second occurs before the first one is fully healed, or sometimes even after its healed, can predispose to longer healing times and ultimately decreased function and possibly chronic pain. There's evidence that prolonged pain and inflammation can cause neurological changes in nerves that can lead to chronic pain. That's why using ice, nsaids (such as ibuprofen), and most importantly, physical therapy, after an injury is important. 
There is an asksience threat from 2015 about (almost exactly) the same topic, maybe you'll find a satisfying answer there:

https://www.reddit.com/r/askscience/comments/3vx6mi/do_multiple_wounds_heal_slower_than_just_a_single/?st=j6h3fu31&sh=3398b910 

For the lazy: these are the two top comments:

* #1

>
Depends on how close they are too.
Wounds close mechanically as well as having regrowth. If you take a little biopsy from your skin, first off you have a scab, and you also have stem cells in reservoirs of stem cells around the wound that start to proliferate and these daughter cells migrate towards the centre of the wound where they meet, stop migrating and start to thicken up. Here is a picture where the green is skin (Keratin5) and red are stem cells, and the wound is to the right, so the skin you can see already starting to point to the wound centre. This wound is 4 days old.
Here is a wound that is totally healed. The scab is the big weird thing on top.
However, at the same time fibroblasts orientate towards the wound centre and start to muscularly contract the wound closed. A 1.5mm wound is basically invisible once the wound contraction is finished, a 3mm wound is less than half that size after a few days.
And so several small wounds in a close location would not optimally contract and might heal slower.
additionally
Wounds need blood flow, and that can be effected with multiple wounds. If you have wounds that are local, and interfere with blood flow, they can become chronic wounds. Wounds that affect skin in a complete circle around the arm or leg can be like that.

* #2 

> not necessarily. If one had three paper cuts of small size on different parts of their body they'd all heal at the same rate.
When learning about tissue healing, we are generally taught that there are three phases: Inflammatory response, Proliferation, Maturation/remodeling. In the inflammatory response phase, your immune system activates to fight off infection, when the wound is sufficiently "sterilized" the Proliferation phase begins. In this, the granulocytes start collecting near the wound area and new tissue is laid down in a messy "spaghetti-like" structure. In the maturation remodeling phase, the tissue is stretched and aligned along the lines of pull of the tissue and the tissue normalizes. That's a brief overview. The type of damage also plays a role. Incisions (straight edged cuts in skin) heal faster than lacerations (jagged edged cuts in skin, like tears). also the deeper the cut is, the longer it takes to heal cause tissues heal from deep to superficial layers. thats it in a sort of nutshell. If you have more questions feel free to ask!

[and further down]

> Your body has metabolic limits, you have limits on the resources you can take in through diet. With multiple wounds those resources are split. How split and how much that slows you down depends on all the usual factors - general health and diet primarily.

**this last part seems to answer your question**, albeit without providing any source.
I'm just here to say I love this sub for its great, well supported answers
There's a lot of great answers on here, and I've also seen several people ask if overall stress appears to affect healing. The answer appears to be yes. There's a famous study where students were given small cuts essentially and were found to heal more slowly closer to exam times.

Some reading on this subject: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3052954/
[removed]
I broke my left hand two weeks ago when a car hit me while biking. The nails on my right hand hand have continued growing and needed to be cut, however the nails on my left hand have stopped growing. Is this because healing the left hand is using resources before it reaches the nails?
[deleted]
When you have an injury, say, a paper cut, inflammatory cells (neutrophils and others than have already been mentioned here) “travel” to the site of injury to help the inflammatory response that will eventually lead to healing. 

With our bone marrow producing cells, we don’t have a “static” number of neutrophils, but for a minute let’s suppose we do have a standard amount circulating in our blood. So, if you have one cut, it’s easier for those guys to take care of it rather than 3,4,5 and more cuts. 

It alas depends on the type of injury and the magnitude of the initial inflammatory response. 

Hope I helped :)
[removed]
[removed]
There are generally a few reasons.  One of the biggest being that higher altitude means thinner atmosphere and less resistance on the plane. 

There's also the fact that terrain is marked by sea level and some terrains may be much higher above sea level than the takeoff strip and they need to be able to clear those with a lot of room left over.

Lastly, another good reason is simply because they need to be above things like insects and most types of birds.

Because of the lower resistance, at higher altitudes, the plane can almost come down to an idle and stay elevated and moving so it also helps a lot with efficiency. 

Edit:  Forgot to mention that weather plays its part as well since planes don't have to worry about getting caught up in the lower atmosphere where things like rain clouds and such form.
[removed]
Pilot here,

We can start by forgetting about piston aircraft that don't have any great benefits going above 10,000 feet compared to say 5,000 feet. 

Turbo-prop aircraft (Q400 or ATR-72) usually cruise around 30,000 since they have a benefit of the prop biting into a bit of a thicker atmosphere vs. a higher and thinner atmosphere

Jet turbine aircraft (737, 320, Cseries) leans itself out as the go higher: air:fuel ratio becomes most efficient. A rich vs. a lean engine in a piston aircraft can go from a 12:1 air to fuel ratio to an 8:1 fuel ratio in a few thousand feet and usually cannot get better than that. 

All other factors like greater fuel efficiency (fuel burns can be cut in half to 1/4 of lower alt. burns), drift-down time (Gimli glider), greater radio reception and radar guidance, obstacle avoidance, but mainly its turbine performance (concorde cruised at 60,000), not friction avoidance.

One misconception is the friction factor. A headwind of +5kt at a higher altitude will not outweigh the benefits of less friction at a greater altitude. Oxygen (atmosphere) drops off a lot after 12,000 ft.

I've changed cruising altitude from FL 19,000 to 13,000 ft to gain another 30 kts.


[removed]
Thinner air actually makes an engine less efficient, but this is offset by increased airspeed in a turbojet engine due to an increase in ram air. A high bypass turbo fan or turboprop still loses efficiency due to the thinner air. Efficiency is primarily gained by the much much colder air temperatures at higher altitudes, which more than offsets the reduction in thrust due to less dense air. I can’t recall exactly why this is but the lower temperature is the biggest reason turbine engines are best at high altitudes. 

Also, because of the thinner air, for a given indicated airspeed, true airspeed (airspeed through an air mass) and subsequently groundspeed, increases as your altitude increases. In the end you go faster for less fuel as you get higher, up to a certain altitude. Then the temperature stops dropping and you run into increased costs to keep the cabin pressurized to below 10,000 feet. IIRC, this is somewhere in the 40,000 feet range. 
1. Stable air and weather avoidance. Less turbulence makes for a smoother ride and it would be cost and time prohibitive to fly around all the storms and wind shear at lower altitudes. 

2. More efficient flying. Less strain on the engines, better aerodynamic performance, and the ability to catch a favorable air current (it's called the jet stream for a reason).

3. More altitude is better in terms of troubleshooting any problems. 

4. The view is spectacular. 

EDIT: removed extraneous words
u/stoplightrave us partially right. However, one reason no one has mentioned is that most want to travel as fast as possible. The higher you go the less drag and thus the faster you go with least amount of effort.
Others are giving some right answers, but also some wrong or misleading ones. Here are the reasons. 

1.	As mentioned, thrust required to cruise decreases with altitude due to reduced drag forces on the aircraft, which is a product of reduced air pressure/density.
2.	The thinner air is easier to work for the compressor, resulting in reduced maximum temperature in the combustion chamber (or as someone else stated, one may trade reduced temperature for increased compression ratio leading to reduced fuel consumption). This reduces stress on components, and therefore maintenance costs. About 35,000 feet is the sweet spot- any higher and the compressor has to work harder to supply the desired pressure. 
3.	Fuel consumption is inversely related to airflow through the engine.  This doesn't sound quite right but I'm looking at the equation to justify this;  I'll check the theory and get back to this if possible. Note: *Thrust* is directly related to airflow. 
4.	Less birds/air traffic. 
5.	Noise. 

I don't know why there are comments referencing pressurization of the aircraft.  Pressurization relies on the engine for air and power, so it's the engine that matters. There is less pressure differential on the structure below 8,000 feet, as the pressure inside is the same as the outside air- so you're not reducing stress on the airframe or anything. 


(edited because I’m a silly)
Fun fact: Certain high altitude air currents such as the Jet Stream play a role in the altitude pilots sometimes fly at. If you’re flying into one it can add a lot of flight time to your journey, so you might ask ATC (air traffic control) for a higher or lower cruise altitude. Same in reverse cuts your flight time.
Fun fact 2: Aircraft flying in generally opposite directions are assigned ‘odd’ or ‘even’ cruising altitudes to reduce the risk of collision. So heading west you’re assigned 33 thousand, but east is 32 thousand. 
I'm late to this thread but figured I'd throw my two cents in...

I'm a flight dispatcher. Nope, not an air traffic controller. I work for an airline and make the flight plan. I plan the route, fuel load, and... altitude.

95% of the reason you fly at the altitude you do is due to efficiency. At higher altitudes the air is thinner and there's less drag (air resistance) on the fuselage of the plane. The engines are also at their maximum efficiency at higher altitudes. 

Most passenger jets are going to be cruising at 30,000-41,000ft. The reason you won't see airliners going above 41000ft very often is that the airplane isn't designed to go any higher. Air gets progressively thinner the higher you go. The difference between the high pressure air in the cabin and the thin air outside above 41000ft could cause structural damage to the fuselage. There's also an aerodynamic problem you run into at higher altitudes called the "coffin corner". https://en.wikipedia.org/wiki/Coffin_corner_(aerodynamics) Some private jets can go up to 50,000ft.

So all else being equal, I want to plan my flights as high as possible to save my company as much fuel as possible. Basically that means 41,000ft. But I very rarely do that for the following reasons...

1. Weight. If the plane has a decent payload or lots of gas, it's probably not going to have enough power to climb up that high. So rather than 41,000ft we have to settle for a lower altitude. Being heavier also lowers the altitude at which the coffin corner becomes a problem. On very long flights they do what's called a "step climb" where climb a little higher throughout the flight as you burn off fuel and get lighter. So on a flight from New York to Tokyo, the airplane might level off at 30,000ft. By the time it reaches the halfway point it might be at 34,000ft. By the time it starts it's final descent into Tokyo it might be at 38,000ft. This is all to ensure that the aircraft is close to it's most efficient cruise altitude for it's weight the entire flight.

2. Regulations. In the U.S. westbound flights are supposed to fly at even altitudes and eastbound flights fly at odd altitudes. 

3. Weather. Flying above weather isn't a concern since we're already trying to get as high as possible. If a flight can't get above it, I'll plan a route around it. The are cases where you might fly at a lower altitude to fly underneath some turbulence or strong headwinds. If there's lot's of turbulence along a route I'll either set the altitude beneath it or give the flight some extra gas so the pilots can hunt for better rides at less efficient altitudes. If the core of the jetstream is at 39,000ft it might make sense to duck down underneath it if you're flying into it. Or if the jetstream is lower it might be a good idea to fly lower and take advantage of the tailwind.

4. Length of flight. There's no sense in climbing all the way up to 41,000ft just to start your final descent five minutes later. Climbing burns more gas than cruising. 
We have a saying:  Speed is life, altitude is life insurance.  If my engine dies at 1k feet above the ground, I can glide 2 miles.  If my engine dies at 10k feet, I can glide a little over 20.  You'll be hard pressed to find somewhere that you can't locate a suitable spot to land within 20 miles.  On the east coast of the US, 20 miles will likely give you a handful of actual airports to choose from.
Airplanes fly as high as they can because the air is thinner and jet aircraft become more efficient the higher they go.  
All problems of dense, turbulent air minimize as you head up.  A jet may fly 400 MPH at 10000 feet but can reach 600 MPH at 33000 with the same or less effort.   That saves fuel, which adds up per trip.   It reduces the strain placed on the airframe by random air currents as they are not as dense.   It also allows better progress against the curvature and rotation of the earth.   There are also extremely fast air currents at high altitudes that can be utilized for a "tail wind" effect that pushes the relative ground speed of an aircraft even higher.   There have been times our Global Expresses or 650s will report groundspeeds above Supersonic, even though at their altitude it's only Mach .98


TL;DR
-Saves fuel. 
-Reduces Turbulence. 
-Increases Speed. 
-Reduces air route crowding (our Gulfstreams fly way above commercial aircraft).  
-Great Circle Routing

Finally something I know!

I am currently studying to become a commercial pilot (ATPL theory) and I am a little bit more than  2/3 through. There are a few reasons, amongst them that I can think of right now are;

Commercial airplanes generally fly at the tropopause since this marks the “top” of all weather. The tropopause varies from day to day but generally lies at 36050ft in ISA (International standard atmosphere) conditions.
While flying at this level the fuel consumption also decreases since the air density decreases and the fuel:air ratio can be decreased. The aircrafts true airspeed also increases due to the decreased density which means that for the same indicated airspeed (which is measured by the amount of “air particles” going into the pitot probe) the aircraft will be flying a lot faster whilst up high. Mach number also increases, this is the effect of an increase in true airspeed and a decrease in temperature. 

Apart from said efficiency reasons there is also other benefits like noise abatements and reduced risks of bird strikes etc. Longer glide distances Incase of engine failures and probably some more stuff I can’t think of right now. 

If anyone has any other questions just comment and I’ll see if I can answer them! :) 
One reason: it’s cheaper. 
If you look at all the reasons already listed they all come down to saving more money. 

That’s why the commercial sector designs airplanes. They care more about the bottom line. They want to squeeze every penny out of every drop of gas, and every butt in the seats. 

More butts in seats : more $
Less fuel burned : more $
It is a mix between engine efficiency, the design of the wing and it’s most efficient altitude and weather.  It’s not just about drag... 

1.  Engine efficiency is the primary reason. This is a thermodynamic cycle. 

Jet engines are way more efficient at higher altitudes for a few reasons. 

Jet engine power is measured by the ratio of exhaust pressure/temp compared to ambient press/temp.  As you climb the ambient decreases faster than output from jet engine. Sea level is 15 deg C, 30K is -56.5 deg C.  It is non linear but follows a basic profile. 

The air is thinner and the temp is lower, the (max temp - ambient temp) / max temp is the equation for efficiency. The engine adjusts fuel based on air density (less air, less fuel) and there is a decrease in output but it is outpaced by efficiency. At some altitude the efficiency will stop increasing and power will continue to decrease, determined by the design specifics. That is max altitude for that engine. 

2.  Does drag reduce with altitude?  Yes but... you get drag at any altitude. It’s about power required to make the lift you need to stay airborne. 
Dynamic viscosity decreases with altitude up to about 30k. It is not linear meaning the difference between 0-10k is more than 20-30k.  it is constant to something like 80k.  So if you can get into the 30-40k range you have a semi constant viscosity to deal with.  Most airliners fly in this range. 

It takes more lift to fly the same airplane in less dense air. So you need to speed it up.  Lift and drag are related so it will increase the drag as you increase lift. 

In general for aerodynamic reasons we can fly slightly faster at higher altitudes. And thanks to our more efficient engines that doesn’t necessarily cost more fuel. 

An airliner wing is designed to get lift both at sea level and at cruise, so it’s compromised design will determine the max altitude for the airplane design itself.  Look at a wing that is efficient at low speed/low altitude vs. an airliner wing vs. a jet fighter.... they are all VERY different. 

Could you design an efficient wing for 10k? Yes but the speed would be lower because you need less lift in more dense air.

3.  Weather and winds.
Winds at lower altitudes are influenced by geography, surface temp, and local weather. Winds aloft (25k-45k) can be more uniform. The jet stream runs in this altitude band. Most storms tend to be 30k and below. Planes fly over that stuff. Sometimes large thunderstorm systems can reach 40-60k. Planes go around those. They can use the jet stream to their advantage or purposefully fly outside of it when going westbound. 

That was more difficult without pictures. I am an airline pilot.  I’m flying right now... at 34000’
  The higher you climb, the less dense the air gets. The less dense the air is, the less particles are present in a given space of air (e.g cubic meter). When there is less particles per space, there is obviously less friction in the air (drag) which will in turn, slow the A/C down. Travelling at lower altitudes have more drag acting on the aircraft compared to higher altitudes due to the amount of drag experienced (density). 

  There is also something to do with engines. Just like in cars, they require a certain mixture of air to fuel to burn efficiently. Starting off at sea level, the ratio (let's use 14 parts air to 1 part fuel) the higher you get, the less dense  the air gets. Therefore, the air in the ratio tends to drop a bit(i.e 13:1) In turn it enrichens the mixture. To make the ratio balance out, leaning of the mixture helps to restore the ratio. So therefore you're using less fuel in less dense air and traveling faster due to less drag
Generally the higher you fly the less fuel you burn. Within reason.. Unless you're flying into the jet stream then your flight will be at a lower altitude. Payload is also a factor. As a flight dispatcher there are a lot of factors that go into flight planning. 
When an airplane is flying at high altitudes, it requires less fuel than the amount that is required at lower altitudes. ... Moreover, lower air density at high altitudes also increases flight efficiency by decreasing the skin-friction drag produced by the interaction of air molecules with the surface of the aircraft.
Too many responses but it is also part of air traffic control. I believe along with the other things said they put planes at different altitudes so that they won't have a chance of running in to each other too. One may be at 35000 another may be at 30000 to help avoid any possible conflicts. 
I haven’t seen it posted yet so I’ll chime in. The most important reason is $$$. Jet engines are horribly inefficient at lower altitudes.  The higher you go, the colder it gets, and the thinner the air gets, which means the jet engines become super efficient and burn less jet fuel. Mind you jet fuel runs about $4 avg per gallon. Jets don’t fill up in terms of gallons but rather pounds. About 5,000 lbs per hour are burnt on a 737 at cruise, that’s about 750 gallons. 

Also to the $$$ factor is weather. Most weather systems top out at about 26-28,000 ft meaning they fly above the weather making for a better ride for customers. Further to this below 10,000 ft there is a speed restriction to 250kts or less, meaning slower flight and more fuel burn. And to add one more point, jet wings are inefficient of generating lift at lower air speeds, which is why on takeoff and landing you see pilots deploying flaps / slats to change the shape of the wing to generate more lift.
Another consideration is the higher altitude of commercial flights allows room for general aviation to take place. You will never see a Cessna 172 at 30,000 but 15,000. You want to avoid haveing multiple aircraft in the same airspace at the same altitude (for obvious reasons). So haveing commercial flights operating much higher can lead to more benefits than if they were lower (amoung other answers too).
It mainly to do with the efficiency of the engines. Colder air is denser and therefore more efficient to burn. As you go up, the temperature decreases fairly linearly, so in terms of temperature it's more efficient the colder it is.

However, as altitude increases density decreases, which is less efficient. As we go up the decrease in density is fairly linear also.

The effect of altitude reducing the efficiency is less than the effect of temperature increasing the efficiency, until we hit the edge of the troposphere/tropopause. At that boundary, the temperature stops decreasing at the same rate, and can actually start increasing again  causing a dramatic drop in efficiency.

That boundary is roughly 30k-35k ft.

The most complex part is the engine, by operating them as efficiently as possible as often as possible means they last longer costing the airline less in servicing, repairs and replacements.
Most people mentioned the wind, which is a good reason.  I'd like to add safety.  If you are at 1000ft you aren't really gonna have enough time to react if your engine fails.  If you are at 330000 ft, you'll have more time.
As someone who flies privately (father has his private license takes me when we have free time in four seaters), it’s nice that the commercial airlines fly that high because we fly at 2-4 thousand feet (sometimes 3-5). We don’t have to worry about their traffic almost ever.
There's a number of benefits, but one that hasn't been mentioned is barometric pressure differences. When planes cross 18000 ft they must set their altimeter to read 29.92 inHg (1013.3 millibars). Basically, the air pressure around the world is not standard and so if someone is flying along and doesn't calibrate their altimeter they won't be flying at the altitude they say they are which can lead to collisions. When everyone sets their altimeter to the same reference number, we all make the same deviations in altitude. We don't do this at lower altitudes because we actually need to know our height above the ground so we can clear terrain and other obstacles. Air pressure also needs to be known so the crew can calculate landing distances and other performance
Most people touched on how at higher altitudes, the planes are more fuel efficient but however the exhaust is actually worse for the environment so there is a trade off because if the planes flew lower, the pollutant would be more contained and less harmful but the planes efficiency would be decreased. 
In a side note, depending on direction, planes fly at different altitudes
Planes burn less fuel the higher they fly. You can get higher than most clouds at altitude. Traffic is another reason, at lower altitudes you have a lot of other prop planes going places that can’t get to 35000 feet so it makes it easier on the controller and the pilot to be above most traffic (some private jets fly higher than that). If you lose an engine it is better to have 35000 feet to play with rather than 15000 feet to use.
Besides fuel economy, there's the fact that there are more meteorological hazards at lower and middle levels than at the altitudes you bring up. Most turbulence doesn't reach that high, and if it's there it's more easily forecast and charted. Also, in the lower and middle levels is where much of the densest cloud cover is; therefore, at, temperatures below freezing, there's the danger of icing. Clouds at the upper levels are much less dense and easily avoided.

Source: Nine years in the US Air Force as a meteorologist, in many cases actually drawing the forecast hazards charts used in providing weather briefings to military pilots. 
Funny enough I didn't see anyone talk about speed, but you need to understand that the speed of sound is dependent upon air density. You can fly faster when flying higher with the same airfoil. It is not that important than other aspects but it is still one thing you need to account for 
Well the problem is in the overgeneralization of the question.  "Airplanes" don't necessarily need to fly that high, only "jet aircraft" do and that is primarily to save fuel due to the fact that the thinner atmosphere creates less drag.  

Also by flying at high altitude jets can take advantage of the "jet stream", which are higher speed currents of air, which again assist with fuel efficiency.

Propeller driven aircraft don't fly at such altitudes unless the cabin is pressurized, and though this is not unheard of, it's not a common practice among propeller driven aircraft.
Only jet engined aircraft need to fly so high. The reason is largely to do with fuel efficiency, jet engines use significantly more fuel at low altitudes. This is so much so that in the UK jets will ascend to 25,000ft even if they’re only going from London to Scotland. As an added bonus true airspeed and therefore to some extent ground speed increases as you go up too. 

This is the kind of thing you’d put on a pilots pub quiz 

(Bonus fact Flybe use prop aircraft because climbing that high takes extra time and therefore fuel, so running turboprops is cheaper and more efficient for their uk operations.) 

Edit: Flying above 10,000ft gets you out of most of the weather so flying above that is beneficial, one of the reasons prop aircraft fly that high. 
the primary reason is fuel efficiency and true airspeed.  as a plane gains altitude three main things happen.  the air becomes less dense since there is less air above to press down on the air where the plane is (think of pushing pin pong balls into a box and as you go up take one out every ten feet, pretty soon you have very few left) as the air thins the affect on the plane is less oxygen to mix with the fuel, fewer molecules to push out of the way as you go forward, need to go faster so you have enough molecules to go over and under your wing to give you lift.  also the air becomes colder (this means that the exchange for heat can be greater).
these changes in density, temperature and oxygen available in a give amount of space will affect various planes up to different altitudes.  a piston plane will be best off between 5,000 and 10,000 feet normally, a turbo charged piston might be good up to 16,000 or even 18,000 if you have your own oxygen to breath or a pressurized hull, a turbo prop plane finds the best fuel burn at about 30,000 feet and a jet or turbine jet is around 41,000.  
also considered is fuel burn to get to altitude and winds aloft.  i have done some ferry flights in pistons between hawaii and the mainland at just 3,000 feet others at 8,000 to 12,000 and in turboprops about 18,000 but once again a lot of the choices can very with finding the best winds.
Planes with the same indicated airspeed (the speed at which they move through the air in their immediate vacinity) have a higher ground speed (the speed at which the plane moves over the ground) at high altitude. This is because the air is much less dense high up.

It's kind of hard to explain, but commercial jets travel around 250-275 knots (nautical miles per hour) indicated air speed, which means they're cutting across the local air at that rate. At sea level, that means they're traveling at 250-275 knots over the ground. 

But when you go up to 30,000 feet, the plane still sees itself as travelling at 250-275 knots indicated (the pressure tubes you use to measure airspeed would be measuring teh same pressure), because the air pressure is much lower and therefore the plane can cover more actual space while flying through the same number of air molecules. Because of this, the actual ground speed is more like 550-600 knots.

Essentially, the plane can travel at a certain speed IAS - against the air - and the less dense the air is, the higher the same IAS is in terms of actual ground speed. In simple terms, a plane's top speed at sea level will be about half of what it is at 30,000 feet.
 The movie The Aviator explained it really well. Before planes did fly low. Turns out there's a lot of turbulence from weather at low altitude. Also a lot more birds. Howard Houghs figured  out if they pressurized the cabins and flew really high the flights would be a lot smoother.
2 things,

1. Altitude = time
If anything goes wrong, the more altitude, the more time you have to solve a problem and keep the souls aboard alive.

2. Safe separation
Also there's thousands of aircraft in the air, and hundreds of airports. So to separate safely the planes that are "passing by"/"passing over" versus the aircraft that are "landing"/"taking off" is to have en route aircraft fly high.

Also everyone else stating the efficiencies are totally true, but here are some disadvantages. Like oxygen and pressurized aircraft are things that are needed before being able to utilize the "smooth sailing" winds aloft.
Air is thinner, you can fly faster, much less turbulence, can get around weather, etc.  If you ever had the displeasure of flying commercial in a puddle-jumper turboprop airplane, you'll understand why flying higher is always better.
It all boils down to aircraft performance and getting the best "miles per gallon". There are some generalized equations for computing the range and endurance of an aircraft. The goal is to stay close to the altitude that maximizes the range when flying to a destination. The target altitude changes as weight changes due to fuel burn. Alternatively, there are equations for computing the endurance.  When staying in the air as long as possible (think Die Hard 2), the goal is to stay close to the altitude that maximizes endurance.
Source: am rocket scientist
More info: http://s6.aeromech.usyd.edu.au/aerodynamics/index.php/sample-page/aircraft-performance/range-and-endurance/

There are a lot of great answers on this post.  The short answer is that the engine choice dictates the flight altitude and speed regime.

Sure air density is lower at the higher altitude but airlines would not operate at that altitude if the  engines were just wasting fuel. The main factor is due to the engines of choice: high bypass turbofans. Turbo fans operate most efficiently at around 35,000ft in the Mach 0.74 to 0.85 range. Older low bypass turbofans and even older turbo jets prefer to operate at lower altitude with more dense air and at higher speeds. Boeing 707's used to cruise at near Mach 0.9!
There are a lot of right answers that all contribute to the full answer. The only thing I have to add is that it always comes down to saving money. 

The plane is more efficient where the air is thinner because of reduced drag. That’s true. But also the fuel and the air have to mix in the right proportion to burn correctly. Less air means less fuel at the same optimal ratio. That saves money. 

At higher altitudes you can gain efficiency flying a curved path around the curvature of the Earth. That makes the flight shorter, burns less gas, and saves money. 

There are lots of little reasons like that that contribute. Changing altitudes because of tailwinds/headwinds is common, because wind pushing you is free. 

Most airliners have to be more than 95% full to break even on the flight (that’s what I read about a decade ago in grad school at least. May be different now).  Everything they do is either driven by the bottom line or an FAA regulation. 

For the most part, the altitude they fly at is the profit maximizing solution based on the cost of getting to an altitude and the money saved by being there.
I studied aerospace engineering. It's all about efficiency. Propellor engines are more efficient at lower altitudes (more dense air to use as thrust), and jet engines are more efficient at high altitudes (less dense air is easier to compress for combustion). I can go into more detail if you want.
[removed]
Oh that’s an easy one.....MONEY! Fuel burn, speed, efficiency...jet engines perform better at high altitude...I can do 700 knots across the ground at FL450 versus 1000 feet, plus at altitudes of FL450 - FL510 I can get over 99.9 percent of all Weather, storms etc... lots of other reasons, however, those are some of the more relevant... 
Are there any currently accepted theories that will be eventually discarded as a result ?
A wave is typically measured by frequency and amplitude. What aspects of gravity do these two properties affect, and are these aspects explainable/understandable to non-physicists?
For anyone still confused about what exactly gravitational waves are, [Piled Higher and Deeper (PhD Comics) has a fantastic video](https://www.youtube.com/watch?v=4GbWfNHtHRg) explaining what they are and how we can detect them. 

In short, gravitational waves are produced whenever masses accelerate, changing their distortion of spacetime. Anything with mass/energy can create these waves, but since gravity is very weak only the most massive of objects produce detectable waves. We (currently) rely upon the fact that the speed of light is constant to detect gravitational waves on Earth. If a wave passes between our detectors, it will either stretch or compress the distance between two points, thus changing the total traversal time for a laser beam.

The detectors themselves are [laser interferometers](https://en.wikipedia.org/wiki/Interferometry) and are large L-shaped constructions with each arm extending for 4km. The US-based LIGO project has two facilities near Livingston, LA, and Richland, WA. The detector takes advantage of the phase change a gravitational wave will cause in a laser beam. 
This event apparently released 3 solar masses worth of energy.  

If that sentence sounds weird, remember E=mc^(2), which means energy and mass are interchangeable.  So to figure out how much energy that is, you have to take 3 times a solar mass (2&times;10^30 kg) and multiply it by the speed of light (300,000,000 m/s) squared, which is an awfully big number:

* 5.4&times;10^47 Joules
* 1.3&times;10^41 kg or 66 billion solar masses of TNT equivalent (**A typical galaxy made out of TNT**)
* 2.2&times;10^34 kg or 11,000 solar masses of thermonuclear explosive
* 5000 Type 1a supernovae
* 100 hypernovae

A sphere of lithium deuteride thermonuclear explosive that massive would be 36 million km across, and isn’t even capable of exploding because it is so heavy it would immediately collapse into an 11,000 solar mass black hole.

But this was a release of gravitational energy, not light, so we never saw a thing, just felt the slightest ripple when it distorted spacetime as it passed by.

I posted this on Facebook last night, and will leave it here in case anyone finds it helpful:

---

Gravitational waves are one of the last major, unconfirmed predictions of general relativity, a theory which does a pretty amazing job of explaining gravity. General relativity describes gravity as a result of spacetime being warped due to matter. Gravitational waves are the ripples in spacetime that happen when you shake matter around. They are to the gravitational force what light is to the electric and magnetic forces.

But because gravity is much weaker than electromagnetism, we can see light all the time (just look around!) while we need to construct enormous lasers and incredibly (absurdly) precise detectors just to have even a hope of measuring gravitational radiation. Rumors are flying that LIGO, just such a system of lasers and detectors, has found a gravitational wave signal, probably coming from two black holes orbiting and falling into each other (because that's the sort of seismic event you need to make gravitational waves large enough for us to detect).

This would most likely confirm what we fully expect is there, rather than reveal something new and shocking about the Universe. Think the Higgs boson a few years ago. It would be a much bigger surprise if this radiation had turned out *not* to be there: general relativity has worked extremely well so far, and we have had indirect but extremely strong evidence for their existence since the 1970s, which won the 1993 Nobel Prize in physics. LIGO's *direct* detection would undoubtedly be Nobel-worthy, too; the only question is whether it would happen this year.

This is exciting because a) it's direct, rather than indirect, confirmation that these things are there, and b) they'll open up a whole new window onto the Universe. Pretty much the entirety of astronomy is done by observing electromagnetic radiation, from visible light to X-rays, the ultraviolet, microwaves, what have you. Starting now we'd have a whole other type of radiation to use to probe the cosmos, delivering us a brand new and pristine view of some extreme events involving ultracompact objects like neutron stars and black holes.

So all this will probably be announced at the press conference tomorrow, ushering in a new era of astronomy and physics. Or they could just be fucking with us.
Is there any concept of directionality with gravitational waves like there is with e.g. light and sound waves? If LIGO detects a disturbance, will it also be able to tell us where that disturbance originated from or are we dependent on other detectors to get that sort of information? 
> For more information on the expected spectrum of gravitational waves, see here.

Wow, I didn't expect the gravitational waves to have such large wavelengths for some reason!  Does that man that LIGO is looking for waves a frequency on the order of 100Hz?  And that future detectors will be looking at frequencies measured in minutes, hours, and days?  And that some of those waves in the CMB end of the spectrum have had only a few full oscillations since they were created?
EDIT: Haha, I work at MIT and the professors who have to teach classes right now are really pissed, they want to watch the announcements!

As expected, there’s already quite a bit of confusion and misinformation in this megathread, so I’ll try to clear things up:

&nbsp;

**What are gravitational waves?**

In order to understand gravitational waves, it’s first important to have an understanding of how forces and fields work. We’ll first take a look at something more familiar - the electromagnetic field. (note: this is simplified to avoid writing a textbook):

The electromagnetic field actually consists of 2 fields: the electric field (E) and the magnetic field (B). The electric field is generated by particles with *electric charge*, such as electrons (-1 charge) and protons (+1 charge). Here’s a picture of the electric field generated by 2 such charges:

&nbsp;

https://en.wikipedia.org/wiki/File:VFPt_charges_plus_minus_thumb.svg

&nbsp;

As you can see in the picture, the convention is that electric field lines come *out* of positive charges and go *into* negative charges.

If you already have an E field and you put a charge in it, the charge will feel a force according to the simple relation (from Coulomb’s Law):

F = E*q

where F is the force, E is the electric field, and q is the charge. This force has a direction - for positive charges it points in the same direction as the field lines, for negative charges it points backwards (since q is negative).

Now, what does this have to do with gravitational waves? Well, let me ask you the following question: what happens if I have 2 charges, A and B, far apart from each other, and I suddenly move charge A?

1) The E field changes instantly, with charge B immediately “seeing” the new position of charge A.

2) The E field changes first at charge A, and then the change propagates outward until charge B “sees” the change a while later.

&nbsp;

http://i.stack.imgur.com/gA6FS.gif

&nbsp;

As you can see, the answer is 2. Changes in the electric field *radiate* outwards at a constant speed - the speed of light. In fact, this radiation IS light - our eyes are actually super-sensitive E&B field sensors that pick up these ripples and translate them into images of the world!

&nbsp;

Most importantly for our purposes, this principle of changes *radiating* outwards at the speed of light is **universal** for all fields and forces in the universe. Including gravity. The caveat being that the gravitational field is incredibly weak compared to the E&B fields, so you need to have incredibly **huge** masses moving around extremely violently, and incredibly **sensitive** detectors to pick up their movements.

The biggest masses in our universe are Black Holes. The most violent events in our universe are black holes colliding with each other. And LIGO is the incredibly sensitive detector designed to detect them!

We’re almost done! The last question is: what do we expect the gravitational waves to look like? And as a corollary, why do black holes collide in the first place?

Well, most of the black hole collisions are going to happen in binary star systems - systems where you had 2 huge stars orbiting each other that at the end of their lives become black holes, still in orbit. But why would they collide? Why not just keep orbiting forever?

Well, massive objects orbiting each other radiate gravitational waves. Those waves carry energy, and that energy has to come from somewhere - in this case, it comes from the orbital energy. So over a very long time, the orbits slowly collapse, the objects slowly orbiting closer and closer to each other. As this happens, the *orbital frequency* increases - the time it takes to complete an orbit gets shorter and shorter. This is a universal principle of gravity - if you look at our Solar System, you’ll see that Mercury orbits much faster than Earth, while Pluto is much slower.

So, as 2 black holes spiral towards each other, we expect to see a **chirp** - gravitational waves increasing in frequency and intensity, rising in a final shriek as the black holes collide and merge.

&nbsp;

**What is LIGO?**

Even with black holes colliding to make gravitational waves, the ripples produced are still incredibly weak, requiring the ability to detect changes on the length scale of 1/1000th the diameter of a proton or less. So a very amazing detector is required.

LIGO is basically an extremely sensitive distance-measuring device, called an interferometer. The way this works is the following:

You start with a laser beam, then you split it into 2 equal beams (typically using a half-silvered mirror that reflects 1/2 of the beam and lets the other 1/2 through) and send them down tunnels at 90 degree angles. When they get to the end of the tunnel they get bounced back by a mirror. When the beams return to you, you recombine them into a single beam and they *interfere*. Depending on how far each beam travelled, this interference can be either *destructive* or *constructive* - meaning the beams can either cancel each other out, or they can reinforce each other and get even brighter.

At LIGO, they designed the beams so that the interference is completely destructive, meaning that no light arrives at their detector. But, when a gravitational wave comes in, it distorts spacetime, changing the lengths of the beams, and they no longer perfectly cancel out! Thus, a light signal appears at the detector.

I strongly suggest you watch the following video, which has a good visual representation of the process (around 1:30):

https://www.youtube.com/watch?v=RzZgFKoIfQI

&nbsp;

**Why is this so damn exciting?!**

So many reasons! The incredible technical achievement - measuring changes down to 1 part in 1,000,000,000,000,000,000,000. The long-awaited confirmation of gravitational waves, which is a HUGE deal in itself. Perhaps most of all, the fact that this opens up an entirely new method of astronomy, one that ultimately will allow us to probe the most extreme densities and energies that exist in our Universe! Finally, this result gives us some confidence that we’ll eventually be successful in measuring the gravitational waves of the Big Bang, and learning about the fundamental origins of the universe.

tl;dr - There are no real tl;drs in science, and why would you want one? It’s worth it to try and understand cool things like this!
Since gravitational waves are technically waves, are they affected by cosmological redshift? 

And if they are, would that mean that the actual event was even shorter? Anything under a second is mind-boggling as an astronomical timescale to me.
The *Physical Review Letters* website has crashed.  Is the first time demand for a scientific paper has been so high that it has crashed web servers?
They kept this really hush hush.

I have a friend that works at LIGO on theory side. He had no idea that they had measured it in September. I even asked him about it in January when whispers started up.

First confirmation he heard was a few hours ago.
"Where do they come from?" should be in the faq.
So..kind of a dumb question here: I've read posts drawing analogies between LIGO's laser setup and the detection of tiny changes in the length of a rod, so I'm just going to frame my question in those terms. By causing ripples through space time, aren't these gravitational waves not only changing the length of the metaphorical rod, but also *everything around it* including the very frame of reference of our reality? In that case, how did LIGO manage to detect the alteration in spacetime?

For example, looking at relativity and how gravitational fields affect time, there's a small difference in the passage of time between, say, a person on earth and a person in a satellite, but each of them perceives- and measures- that the same amount of time has passed, because their respective frames of reference are also affected, right? Why isn't something like that happening here?

I feel like I'm doing a very poor job articulating my question here, but I hope one of you learned folks understand what I'm trying to say and answer my query.
What would someone (or some planet) nearby experience as these gravitational waves passed by them?

I mean, we detected a very faint signal from this distance. But how are the effects when you're close to the source? This event released massive amounts of energy, what are the effects when you're close by?
What is the absolute guarantee that the waves detected this time are gravitational waves and all other interference issues were accommodated for ?
[removed]
How are they able to make the determination that it came from two colliding black holes? How can they determine the distance to those black holes?
How does this work tie in with what the LISA Pathfinder is measuring now that it's in L1 orbit? [Link to LISA's Site](http://sci.esa.int/lisa-pathfinder/)
When every thing in the universe is exerting gravitational force on every other thing it seems almost impossible that any waves could be distinguished from any others. This is very exciting news and the implications are enormous. 
It's easy enough to think of gravity as a 3D object such as a sphere pushing into a 2D plane creating a distortion and gravity waves as being the ripple effect as the magnitude of the gravitational forces fluctuate slightly. Can you help me understand how this would look like in 3D space time rather than just a 2D plane?
How do they know that it's two black holes specifically? (Low data so I couldn't watch the kids, sorry if it was mentioned there)
They did it!
Whenever there's a scientific breakthrough, this question:

> What's the practical application?

always comes along and I hate it. So many things have been discovered and created at a time no practical application was possible and now we can't live without.

With that being said, possible practical application for this gem is marvelous. From LIGO:

> In conclusion, we will never be able to commercialize or weaponize gravitational waves themselves.  However, they will carry information to us about some of the most extreme environments in the Universe which we can use as a laboratory for environments we cannot create here on Earth.  This information can tell us more about how the physics around us works in subtle ways that can have profound implications.  What those are are yet to be seen.  That's the exciting thing about science - you never really know the full potential of new discoveries until after the fact.


EDIT: Sorry, folks. I've meant to say people that ask this question in a derisive manner. Of course, curiosity as to its practical application in real life is, of course, welcome.
Can two waves of equal and opposite magnitude cancel each other out? 
This is an incredibly sensitive detector. What kind of things do they have to correct for? I seem to recall a story about how they were confounded by the gravitational signal of clouds passing over the detector --- is this the case?
Will this help between any link with gravitational waves, gravity waves and gravitons? Could the detection of waves help with the idea of wave-particle duality when it comes to the graviton?
Have we learned anything other than that gravitational waves exist, i.e. does the wavelength and frequency tell us anything new about the attributes of gravity and gravitational waves in general, or at least about the object that emitted those waves?
The masses of these black holes strikes me as strange: 36 and 29 solar masses.  As far as I'm aware, most black holes are thought to be more like 1-3 solar masses.  Do we have any solid ideas for how such a strangely massive pair could form?
Do gravity waves become weaker the further they are from their origin?
But how do you know it's not just the natural stretching/compressing of the system, the metals, all that stuff?

Because the wave is so small, a thousandth of a proton, wouldn't individual molecular vibrations throw everything off?
What implications does this have for holography? Shouldn't gravitational waves have entropy proportional to the volume of the space, rather than the surface area?
^(Proper out-there speculative question incoming. )

What, *if any,* advantages might gravitational waves offer an advanced alien species wishing to communicate over vast distances?

Would they propagate better than radio or lasers? Don't worry about what it takes to generate them for now. As we advance from detecting gravity waves for the first to measuring them more precisely, perhaps we would stumble on the galactic chatter than SETI has failed to find because it was focused on a less suitable medium. 


The detector for these waves shows one leg of a tunnel being slightly shorter than another indicating a wave.  Is this the gravity causing an elastic deformation in the earth or is it something more complicated? Also how do you separate it out from seismic signals which may also deform the earth?
Where does this place the notion that gravity, as a force, should have an associated force carrier particle?
Here's the series of 11 papers announced on arXiv tonight:

* [Observation of Gravitational Waves from a Binary Black Hole Merger](http://arxiv.org/abs/1602.03837)
* [GW150914: The Advanced LIGO Detectors in the Era of First Discoveries](http://arxiv.org/abs/1602.03838)
* [GW150914: First results from the search for binary black hole coalescence with Advanced LIGO](http://arxiv.org/abs/1602.03839)
* [Properties of the binary black hole merger GW150914](http://arxiv.org/abs/1602.03840)
* [Tests of general relativity with GW150914](http://arxiv.org/abs/1602.03841)
* 	[The Rate of Binary Black Hole Mergers Inferred from Advanced LIGO Observations Surrounding GW150914](http://arxiv.org/abs/1602.03842)
* [Observing gravitational-wave transient GW150914 with minimal assumptions](http://arxiv.org/abs/1602.03843)
* [Characterization of transient noise in Advanced LIGO relevant to gravitational wave signal GW150914](http://arxiv.org/abs/1602.03844)
* [Calibration of the Advanced LIGO detectors for the discovery of the binary black-hole merger GW150914](http://arxiv.org/abs/1602.03845)
* [Astrophysical Implications of the Binary Black-Hole Merger GW150914](http://arxiv.org/abs/1602.03846)
* [GW150914: Implications for the stochastic gravitational wave background from binary black holes](http://arxiv.org/abs/1602.03847)

Happy reading!
I understand that the discovery was made by measuring infinitesimal amount of distance (1/1000th of a proton's width). From a physics/scientific standpoint, can we know if that difference results from the dilation of space, as opposed to the dilation of time, or both?

Bonus question: couldn't the effect of the gravitational wave have been measured by using extremely highly calibrated timepieces of some sort? If spacetime is distorted by the waves, then there would presumably be a notable difference using precise enough equipment.
I don't remember seeing an explicit time term in the Einstein field equations as a physics undergrad. I would expect that the team is measuring a distortion in distance as a function of time as the gravity wave passes by. I'm curious how you can solve the field equation with this information, or are you relying on some explicit solution of the equations (which might involve large assumptions, like the Schwarzschild metric, for instance)?
From the Nature article:

>This is the first black-hole merger that scientists have observed. 

Were they viewing the merger through a telescope or something? If not, how did they know this was happening? 

Is LIGO just constantly switched on and if they see those wiggles on the charts then they know 2 black holes have collided? Or do they somehow figure out when one is going to happen and then they turn it on?

Thanks!
How quickly are these things orbiting their center of gravity?
Presumably there are a lot of gravitational wave sources, each with a different frequency and phase.  Unless Earth happens to be much closer to one or a few of them than to the rest, it seems like it would be difficult to detect individual sources above all the background and interference.  How does LIGO account for this in the data processing?  Or are the sources rare enough that this isn't a big issue?
[deleted]
Sorry this is a very idiotic question but what are Gravitational waves? I don't know what it is but I am excited and want to know more also does this hold the key to the origin of life and universe? And how much our technology will be advanced through this? 
Are these the type of waves that can be modeled with standard wave equation or are they some other type of wave?
Congratulations on this amazing achievement! 

My question pertains to the timing of all this. I was reading through the timeline of events, it seems that LIGO was constructed in 1992 and they detected a binary black hole system in September 2015. Were you actually looking for such a system for so long or someone just happened upon it? I would guess that such a system would take "years" to decay, and (probably) detecting it would make it harder. At what point of the event, do you actually realize that a gravitational ripple has been caused? If there is such a specific "point of time" how is it ensured that it is statistically significant? 
How long, roughly, would the event of the two black holes merging take place? 
For people that are interested in the ongoing research on gravitational waves there is a project being conducted at the University of Wisconsin called [Einstein@Home](https://einstein.phys.uwm.edu/).  The project is using data from LIGO gravitational-wave detectors, the Arecibo radio telescope, and the Fermi gamma-ray satellite to search for weak astrophysical signals from spinning neutron stars.  The long-term goal of the project is to make the first direct detection of gravitational-wave emission from spinning neutron stars.  So far the project has discovered 50 new neutron stars, and with the new observations of gravitational waves, the project has new data to work with that will greatly help their project.  

Whats also neat about Einstein@Home is that it is a [BOINC (Berkeley Open Infrastructure for Network Computing) project](http://boinc.berkeley.edu/).  To process all the data for the ongoing project, Einstein@Home utilizes BOINC to send packets of its data to volunteer personal computers to help process the data and send it back to the project.  

If you are not familiar with BOINC, it is software that you can download from the BOINC project website that will allow your personal computer to help scientific projects (like Einstein@Home) with their data processing.  Once downloaded to your computer, you are given a choice of various projects to participate with.  When you choose the projects you wish to help, data is downloaded from the project servers to your BOINC client.  When your computer sits idle, the BOINC client will then use your computer's CPU and GPU to process the data.  Once complete, the data is sent back to the project's servers and new data packets are downloaded for processing.  Basically, BOINC allows the processing of large amounts of data through a network of volunteer personal computers around the world.

To learn more about BOINC and Einstein@Home you can check out their websites at the following links:

https://einstein.phys.uwm.edu/

http://boinc.berkeley.edu/download.php

I'm having trouble grasping the concept of gravitational waves. Is a gravitational wave just the periodic change in the force of gravity that we feel from those distant merging black holes? Similarly, are our ocean's tides caused by a gravitational wave with a 27 day period caused by our orbiting moon?
But why.. Why those specific black holes though? Shouldn't we detect something closer and massive like the Sun?
Sometimes I am saddened by the state of our world at the present time. With the current political and religious celebration of the obtuse, I fear we as a species are doing our best to move back into the caves. 

 The shear magnitude of this discovery is almost unfathomable.  When Galileo invented the telescope, only a few people on the entire planet understood what he did, and what it meant. We have just witnessed the invention of the telescope. This discovery is of the same caliber of Galileo's and hold the same amazing possibilities of discovery.
What I need to tell myself when I am saddened by the obtuse of the world, is that millions of people understand what has just happened. And much of those, if we made a correlation back to the time of Galileo, are the same as the peasants of that time (myself included). Those who created the amazing mechanism to measure gravity waves will be celebrated and held in the highest esteem during their lifetime, and not persecuted and murdered as Galileo. Those who want to drag civilization back to the caves are the very small minority. We have come a long way, and will always move forward. 
If space time is warped by these waves to the degree that there was a 7ms delay of one reading to the next, wouldn't carefully calibrated atomic clocks in different locations be able to sense perturbations in space time such as this? Or would each clock minutiae fluctuate it's own time then for example the next clock fluctuating 7ms later? Could we go back to the event on September 12th and detect the waves in other instrument records? 
I understand what gravitational waves are and understand the *explanation* of how LIGO works. But I'm still confused on how it's actually physically possible.

One of the videos says that you can't put a line on the "floor" and a scale next to it and watch for the line to stretch because the scale will also stretch. I understand this much and the argument makes sense.

What I don't understand is how the space-time stretching some how affects only the space part of the equation when it comes to light travelling and doesn't impact the time part. As in, why doesn't time stretch (slow down) for the light wave when the space-time stretches? Gravity causes time dilation, but somehow gravity waves have no impact on time?

If both of space and time stretch, then the time taken for the light to travel a squeezed LIGO tube would be the same as the one that's not squeezed and we won't be able to detect the gravitational waves because there won't be interference.

I'm confused why time isn't affect. Someone please help me understand.

Is it possible that using GravWave detectors to do observational astronomy will allow us to drop "Dark Matter" and "Dark Energy" as lazy place-holder terms, and actually be able to name and describe what's causing those effects?

[deleted]
[deleted]
Vibration specialist here, I saw someone mention the gravitational was caused be a neutron star shaking. Is the shaking from the angular velocity of the star passing the star's first natural frequency?
Are you aware of the [LISA Pathfinder](http://sci.esa.int/lisa-pathfinder/) mission from the ESA? They will be beginning the experiment fully next week with the release of the test masses and should give results soon after.
What is the time window that waves coming from binary black holes would pass the earth? In other words, If the collision of these back holes had happened few years earlier, they would miss it? If so, does it make this a very lucky catch or these binary collisions happen frequently?
why was the detection 7 ms apart between both detectors? seems like a long time to me
Since these gravitational waves stretch and compress space, does that mean you could in theory disrupt the event horizon of a black hole with gravitational waves and "safely" enter?
Is it really possible to get an accurate handle on the progenitor system for these waves? Given that they may have been traversing the Universe for > 1 Gyr, these waves may presumably have interacted with other gravitational waves in a constructive or destructive manner?
What is the name for a scientist that studies gravitational waves?  Now, that they can be detected.  Gravitational Physicist?
It sounds like we just can't create the conditions here on Earth, but could we ever use this for communication\transmitting of information?    I'm thinking simple Morse Code, but we wouldn't have to set up wires or worry about as much about what got in the way...
Is the gravitational wave your measuring here different from meteorological gravity waves?
Amazing.

Congratulations to those involved with LIGO!  
Now that this phenomenon has been directly observed and gravitational waves confirmed, what would be the next logical step in terms of designing instruments that can detect these waves on broader scales (not just unique and powerful events)?

I guess what I'm asking is:
-What could we learn if we could detect more subtle signals?
-How might we go about building such uber-sensitive detectors?
-What will it take to make that a reality (funding, time, tech advances)?

Congratulations to the LIGO team! 

As a social scientist I envy the ability attain such astoundingly high statistical significance in results.
I heard several time that these gravitational waves could enable us one day to "see" (detect gravity waves from) the big bang. I thought all matter in the universe came from a single source exploding, meaning the wave traveling at light speed should always be ahead any matter in the universe ? unless matter initially traveled faster than the gravity wave (impossible ?). I also read that these waves go through anything without being affected so the big bang gravity waves can't simply bounce somewhere and then get to us right? Besides if it's always ahead of all matter it can't bounce on anything anyways.
ELI5: A 'binary' black hole?
I find it more interesting that we actually observed the merger of two black holes.  Has this ever been done before through other means?

As a rough estimate how often does an event like this occur in a given galaxy or in the observable universe?   Is this a once in a lifetime event or would it happen every few months?

How does LIGO discriminate the source of these distortions from simple seismic disturbances?

EDIT: Actually I see in the stats an estimate of blackhole mergers at  2-400 Gpc-3 yr-1
So if that's per cubic Giga parsec and the one we obsesrved was 410 Megaparsec away then that means we should be able to detect events within the volume of a cubic gigaparsec around us.  2-400 events a year.
What's the difference between "gravity" and gravitational waves?
Are gravitational waves just the way the gravitational field expands?
What I mean is that a mass M moves and so it's gravitational field change accordingly, we don't get "gravitational waves" because usually this movement is too slow, when there is an asymmetrical mass spinning we get a wave because the asymmetry implies the strength of the gravitational field in a specific point changes over time and since it's spinning fast enough, that rate of change is not too slow, so we can measure it. Is this even remotely correct? Or do the waves come from something more profound?

Edit: grammer
Are gravitation waves linked to spooky action?
Are there any events where we will be able to correlate EM waves from various observatories with gravitational signals? On a related note: Are black hole mergers the only thing that will produce a signal with sufficient intensity to be detected?
> * Final black hole spin α = 0.67+0.05-0.07

Holy shit, that's a lota spin!

In all seriousness what is black hole spin measured in?
If space time contracts or expands due to gravitation wave, what effect does it have on the matter? Do the nuclear forces grow weaker and perhaps disband from each other, when the space expands or vice versa when it contracts? or is the matter there, not "feeling" a thing when the wave passes? 

I assume that the energy or amplitude grows stronger and also the expansion/contraction when we go closer to the gravitational wave source. If I were close enough to feel significant expansion/contraction, would I be a living entity anymore after the wave has passed?
The sad and somewhat ironic thing about this is that it is extraordinarily profound achievement, however, in order to truly appreciate and be fascinated by this, one must have understanding of the subject matter. You can tell this to your buddy Joe Average he will just shrug it off. Two black holes collided over a billion years ago and some apes from a random planet detecting it. So magnificent and profound, yet so irrelevant to the majority.
After this discovery, are there any other aspects of General Relativity that remain unproven?
Is there a way to determine the "elasticity" of space time? Or the tendency to propagate gravitational waves? 
Does the LIGO team know where in the sky these two black holes merged?

If yes, follow-up question:

Since gravitational waves travel at the speed of light, were we able to watch this event unfold on the visual spectrum?
Hi Folks,

can anybody please explain to me, why there is a significant difference between both locations in terms of the plots which were measured and calculated? 
Thanks 
So the gravitational wave was detected short after the detector was turned on. Does this mean that detectable occurances like colliding black holes happens often, or can we detect several waves from the same collision, or were we just extremely lucky?
If the earth is staying in orbit with the sun because of gravitational waves, is the reason I return to the earth when I jump due to gravitational waves the earth is giving off? (much smaller compared to these ones detected) or do gravitational waves only result from a change?
What is the difference of what they do at LIGO and Michelson-Morley Experiment aside from LIGO being about 2000x bigger in size than Michelson-Morley's interferometer?
Great percentage ~5% of final mass was radiated as energy. Does it contradict the rule that nothing can leave the black hole ? They both lost mass without actually shedding matter to outside world. 
How is a measurement like this validated or reproduced to rule out a false positive? 
[deleted]
So I see the magnitude of the space time strain was 10^-21. How big would it be if we were 1AU away from the event?  Is it an inverse square law?
Hey, sorry if this has been asked already, but can someone explain to me, how we know exactly how far away this gravitational wave started. In advance I'm sorry for my lack of knowledge but I find this really fascinating.

So, what I have gotten so far is that there is three relevant factors to find, when and how these waves were created. First tere is  the speed of light which is constant, then there is the power of the waves at their origin, and the time, when those black holes merged. But from that I don't understand how we can determine those factors. If we want to know, how powerful those waves were initally, we need to know, how far away that was, and if we want to know how far away it is, we need to know how powerful the gravitational waves were initially in comparison to know...

What am I missing? Again if that is really stupid, I am really not very informed (yet :) )
I was reading an article on the NY Times app, and at the end of it it says "only three days before the black hole chirp...antenna readings were plagued by radio interference."  and earlier was mentioned how a group known as Bicep "claimed to have detected gravitational waves...They later acknowledged that their observations had probably been contaminated by interstellar stardust." 

So could it be possible that LIGO's readings were polluted by radio interference? 
(I'm not too knowledgable on these things)
Thanks! 
This might have been answered already but I'm struggling to find it. Were the scientists expecting to find these specific waves at this specific time due to visually observing the orbit of the binary system? And if these gravitational waves move at the speed of light, if they were observing the system did they "see" the black holes merging at the exact second the LIGO system received the data?
Are there objects in space which can magnify  gravitational waves? Just like lenses can magnify emw.
Congratulations LIGO! 

I have a question tho: the energy released in this event was truly humongous. What would happen to an observer on a planet, say, 5 light years away from the source? Would they notice anything? 
[removed]
As a flood engineer i wish i came here sooner to answer a few questions. 

But the main thing is that a number of vectors will remove the water due to gravity.   Namely sewers, ground drains, good old evaporation, lay of the land, local ground geology, tidal differences to name a few.  Once the hurricane has passed though then levels will drop fairly quickly.

The biggest problem will be both the environmental (mobilised sewage etc...) and physical property damage that has occured.

Lots of standing water means mozzies and vermin.  Contaminated waters means polluted ground water sources, damaged agriculture infrastructure, dead livestock, crops wiped out and quarantined/destroyed  (certain crops absorb harmful nasties). 

Infrastructure wise lots of work gutting out the drainage system and clearing hurricane damage will take place. Damaged electricity/phone/gas infrastructure will need checking too.  

Now let's talk about the houses that are damaged. 

There are typical damage assessments that can be done but there are 2 typical factors that ultimately determine damage costs and severity - the depth of water and the time water stays around (ok and type of building materials used)

Anything typically above 400mm for a day or 2 means that the lower floor needs gutting and requires a complete rework.

Now think of this in terms of demands locally.  Every homeless person will  need rehousing, every house will require skilled trades to repair them(and they WILL be in  demand). It will take years for all affected properties to be repaired by all trades.

But as a starter for then why not google what happened during hurricane katrina or the 2005 flooding carlisle in the UK.  Some of the stats are just mindblowing and heartbreaking.

Ps also google the lake levels rises during hurricane Katrina due to the low air pressure alone.  It makes for amazing reading.

Source : i am really a flood engineer 😊

Edit: Thank you for my first ever gold(s!!!) and all your messages.
I'm a semester away from graduating with a degree in soil science, so I'll answer this as it pertains to soil.

The most important concept to understand before I delve into this is that everything moves through soil via water. This includes nutrients, pollutants, minerals, etc.

The large scale scope of a flood like this is pollution and soil erosion. When soil is flooded to the point that water can no longer infiltrate, it begins transporting the top 2 layers of soil. This is the organic layer (such as leaf litter) and the surface layer (topsoil). These are the two most nutrient rich layers of soil, and often the most polluted (by pesticides, foreign chemicals, or trash). This is not good for a few reasons:

1. The main source of nutrients have now been stripped from the plants.

2. The sub-surface layers are exposed, and from what I understand about Texan soil, this is mostly clay.

3. Pollutants are now being transported and dumped into waterways or varying soil environments. 

Some of the nutrients will be returned to soil elsewhere, but most of these nutrients will flow to waterways and into the ocean. This will not be good for the Gulf, as it is already hypoxic (lack of dissolved oxygen due to nutrient runoff. Essentially, the large amount of nutrients results in algae blooms which deprives the water of oxygen). The reduction of plant matter will also reduce the stability of the soil, meaning it will be more likely to erode in the future. 

The second portion of this is that clay now becomes the surface layer. Because clay is so small, it takes water significantly longer to move through the soil profile, thus lowering the hydraulic conductivity. This means water runoff will now become a large problem in the affected areas for months or years to come.

The third portion is pollutants. Chemicals and trash are now being transported to different soils and waterways. Not good.

As for the smaller scope impacts, soil horizons will become disrupted. Soil stability will be negatively impacted. Nutrients will be forced through the horizons quicker, leaving a deprived soil. Many soil microbes (the most important indicator of soil health, depending on who you ask) will be killed off.

So, yeah. It's disastrous for soil and the overall environmental health of the area.

EDIT: Also, as for plants, many can survive being submerged for up to a weekish. Many cannot. The flooding will inhibit root nutrient uptake, which will result in decay. Water-logged roots are also more susceptible to organisms that specialize in root-rot.      

EDIT2: /u/BadBadger42 asked a question about highly expansive clays. These are what we call shrink-swell soils. The clay fraction is expanded in area when water-logged, which causes extra stress on infrastructure. This is the leading cause of flooding basements, and why many buildings in Texas do not have basements. Taking a look at [this soil map of Texas](https://www.nrcs.usda.gov/Internet/FSE_MEDIA/nrcs144p2_000979.jpg), most of the area effected is Vertisol. I'm sure you've all noticed a vertisol before, when it dries it looks like [this](http://geotechfoundation.com/wp-content/uploads/2013/04/cracked-soil-texas.jpg). The "swell" of the soil is caused by the micropores between the clay particles becoming over-saturated - and when it dries ("shrinks") those cracks are left behind. Vertisol is the shrink-swell classification of soil. I would imagine that the effects of shrink-swell on infrastructure will be exacerbated by the flooding.  
I'm a Professional Engineer in Pennsylvania who consults in stormwater management.

Q1: Water will recede just like any other storm, it will just take MUCH longer. In an urban setting like Houston, it will flow into storm drains along the curb, into small pipes like 18” diameter, then those feed into larger pipes like 48” or even concrete box culverts which might be 15’ wide by 4’ high (the possible dimensions are endless). Those large pipes/culverts drain into the streams and rivers. Everything eventually keeps draining downhill to the ocean, bay, bayou, etc. 

Q2: In general, this probably won’t change much regarding the landscape. It will look like the same place as it did before, except structures around main drainage channels may have washed away.

Q3: No permanent lakes or rivers will be made. Everything will drain naturally in a matter of days. If there is a low spot that doesn’t drain by gravity to a storm sewer, channel, stream, etc., that depression could hold water longer but it will eventually infiltrate into the soil and/or evaporate.

Up here in Pennsylvania, we design pipes/culverts to carry up to the 100-year storm which would be like 8” of rain in 24 hours. Not sure what they design for down south, but their 100-year storm is probably like 12”+ of rain in 24 hours. This hurricane is dumping 4+ feet in some areas so there’s no way to drain that much water very quickly. Just have to wait.
Water will recede in basically same way as in the normal, non-flooding rain. 

There are two (three) types of [runoff](https://water.usgs.gov/edu/watercyclerunoff.html) (= outflow/discharge). We have direct runoff and basic runoff. 

One part of direct runoff is surface runoff: water is no longer being soaked up by soil, because soil is fully saturated, therefore water can only flow on the surface. 

Other type of direct runoff is hypodermic runoff - water is [infiltrated](https://water.usgs.gov/edu/watercycleinfiltration.html) to the soil, but above groundwater level, only in soil capillaries, where it's flowing away.

About basic runoff, this is the [groundwater runoff](https://water.usgs.gov/edu/watercyclegwdischarge.html), where water in zone of saturation(=aquifier), beneath the groundwater level, flows off.

When you have flood, all soil is fully saturated even in aeration zone, above groundwater level, so soil is not able to soak up water and water is not able to infiltrate into the soil, so it stays on the ground. Or there is precipitation so strong, water cannot infiltrate into the soil in time. But, in time, all that water flows away, because groundwater is basically connected system of water bodies, so excess water is distributed to other areas via basic runoff and via basic runoff, especially surface runoff, it flows back to terrain depression, rivers and other areas with low altitude. In this area it's the gulf of Mexico for most of this excess water.
I Googled this yesterday cause I was afraid to ask on here. I am curious of the actual paths it takes to get to the gulf. The only thing I can imagine is how a tsunami rapidly recedes back to the ocean but I know that isn't how this works. 
Dams, levees, and storm water drainage systems are usually piles of dirt covered in specifically chosen breeds of grass and clay  that form a really durable root system that acts as a self-healing water barrier. Or they use concrete. 

Unless pumped, water will always flow down hill. Houston sees predictable amounts of rain, so every street and development has a drainage system that plugs into a municipal system. Everything points downhill and gets there eventually. 

However, this all only works if the "end" of the system is lower than everything else. Right now, that isn't true in Houston. Where ever their collective wastewater is supposed to be going is now higher or equal to what drains into it, so the water doesn't flow. 

If you have 2ft of water in your house, when that drainage basin lowers by 2ft your house will no longer be flooded. Kinda. 
I would like to piggyback on this question if I could...

Would draining the 2 reservoirs they're having problems with (the Addicks and Barker) before the hurricane hit not have kept the water out of the Bayou and out of those neighborhoods close by?

Why would they not have done this beforehand?
Something similarly horrifying (albeit part human error) happened with the [Salton Sea] (https://en.m.wikipedia.org/wiki/Salton_Sea). 

it's got quite an errie vibe. Hot In SoCal, deserted trailers from the 50's halfway buried in sand. A law chair, somehow, still planted where its long dead owner once placed it. The Dunes don't deserve a second blink, untill your get close enough to get it in between your toes. Its not sand, but rather decades of stocking fish in a toxic lake caused the entire sediment layer to be a mass grave. 

cool shit. 
They'll recede the same way normal rain does... through any natural "drain" feature to a lower elevation. It will just take longer since there's more water.

If you dump water slowly into your tub with the drain open, the tub will not collect any water. If you dump it in fast enough, the tub will fill/flood/overflow. But it will still drain, and once you stop adding water it will eventually empty. Until it finishes draining though, the flood will remain standing.

> Will permanent rivers or lakes be made?

No matter how much water you add, or how fast, you won't ever permanently fill the tub, unless of course the water being added is so forceful that it changes the landscape, i.e. it collapses your tub and causes broken rubble to fill in the drain hole.

Likewise, if there is some closed container where water collects after the hurricane, then it also would have collected there after a normal rain. Because it's a closed container. Not because of how much water fell.
If you want a good example of a change that occurred due to a storm, look at the inlet at Ocean City, MD.  While it was made permanent due to human intervention, its creation came about due to a storm that hit Maryland's eastern shore.
After the Great Flood of 2016 in the Baton Rouge area, many areas remain fully saturated today because of broken culverts and debris in canals. Doesn't help, we depend on the water flow to filter South, where flooding is now occuring (which can cause a wave of water to flow up...think of waves coming and going) Harvey's passing through now has left standing water now that is not fully draining, but not really flooding.  Most homes (estimated at 60%) are not fully finished due to lack of qualified contractors and lack of building supplies.  Of those 60%, only 30% are living in their not fully finished homes, leaving the rest displaced and looking for housing in a market with low inventory.  Hurricane season is no joke and more serious than taken.
[removed]
[removed]
[removed]
[removed]
The sediment portion of river bottoms exist in a state of equilibrium with deposition and removal rates.  Generally, as the flow of water increases it will expand upwards and/or flow faster, whichever requires the least mechanical energy.  

As the speed of water flow increases, its competence, or ability to carry sediment increases.  Erosion rates will increase, and deposition rates will decrease on a section of river.  Thus, during an increased flow event, the river will carve downwards as well as along the banks.  

Flow rate competencies differ by py particle size.  Silt sized phyllosilicates are the most easily eroded.  Clay particles, which are even more fine, tend to resist erosion more up to a certain threshold, but they stay longer in suspension and so travel farther.  Sand, pebbles and boulders all require greater competence to move, but they often move by skipping along the bottom, increasing erosion more than water alone would.  

When flow rates start to decline, deposition rates will increase relative to erosion rates, but with the new channel shape, some of the removed material may be replaced due to slower flow rate with the normal amount of available surface water.  

As to the geomorphology component of the question, when water overflows a river bank, it slows down.  When this happens, it's competence is reduced, and sediment starts to fall out of suspension.  The larger particles fall out first, overall nearest the bank of the river.  This creates a slope leading away from the river into the flood plain.  This slope is known as a natural levee.

Over very large spans of time, rivers tend to flow fastest at the outer bend of a meander, and slowest at the inner side.  The effect of competence causes the inner point bar to grow, while the outer portion of the meander erodes the cutbank, which expands.  Sedimentary basins, when viewed from above, tend to be littered with old meander scars.    
[removed]
Civil Engineer here, storm sewers will/are flowing at full capacity towards their outfall (pond, creek, river), and depending on where you live and what your municipal standards are, there are storm drainage systems that will naturally infiltrate rain water back into the soils water table. But seeing the capacity of the flood, and everything flowing at full capacity there could be problems of the amount of water in the soil causing the storm sewers to settle & could potentially crack/rupture. 


There's a lot of climate science being discussed by those who I suspect are not climate scientists. I'm not one of them either. But I do have some questions for both climate scientists and civil engineers if any see this and are willing to answer- 

1) Is the urban landscape of Houston contributing to or lessening the effects of the flood? I.e.- is the freeway system, neighborhood structure, sewer systems a player here? Is there a optimal way to build- besides putting everything on stilts, that could abate this kind of flooding?

2) Whether or not Harvey can be attributed to AGW, are there local climate effects that exacerbate or mitigate these kinds of storms, such as the heat island effect?

Thanks!
I can say this as I lived in Houston for many many years and was there during Allison. One of the biggest and hardest things I remember to recover from then was the tunnels in Houston. 

Downtown Houston is interconnected by a series of tunnels that go from building to building all over downtown. This is where a majority of us ate , occasionally shopped and used to get from building to building all around. 

The amount of mold , stench and rot that covered downtown for probably a solid year if not two. The entire downtown had pumps and huge fans constantly removing water and trying to air out the tunnels to try to dry them. The smell , the damage and rot was unbearable at time and I can't remember exactly how long it took them to rebuild this. 

The damage done to houses and buildings and hwys is only complicated by the fact Houston sits at I believe 100 feet above sea level. There is no place for the water to "run off" too so it's a slow drain that allows for standing water to continue to fester.  
One issue that is more prevalent in Louisiana is when cemeteries are flooded, the coffins come up. There are many small family cemeteries, some centuries old.  

They found coffins months after Hurricane Rita just piled up in some rural wooded areas. 

[story](http://www.nytimes.com/2005/10/25/us/nationalspecial/coffins-and-buried-remains-set-adrift-by-hurricanes.html?mcubz=3)

The way waters recede is two fold, the first is the simplist to understand, the ground is porus and the water is absorbed into the porus ground. The second is the water flow, water always flows to the lowest point. So as the water either is absorebed into the ground or into lakes or rivers that then flow into the gulf. As that water is moved the rest of the water follows. 

And while we had some rough patches here in houston, the fact taht the ground was not saturated with water made everything a whole lot better as the floow waters mostly stayed while it was raining but receded fairly quickly after the rain stopped.
[removed]
Cardiovascular physiologist with extensive exercise physiology experience here.

Couple of things first - cardiac muscle is fundamentally different compared to skeletal muscle. Although certain contractile proteins are similar, in terms of energetics (energy production and consumption), cardiomyocytes (heart cells) are very different. 

The main reason why cardiomyocytes are so resistant to fatigue is because they contain almost twice the amount of mitochondria. Mitochondria are the **aerobic** cellular powerhouse. We know this by looking at the content of [citric synthase which tracks very well with mitochondrial content.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4121645/#!po=35.0000)

The heart is very metabolically flexible in terms of fuel. [It consumes glucose, free fatty acid and **lactate.** Yes.. you read that right. It consumes lactate (so does skeletal muscle).](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2290415/) This is especially pronounced at high exercise intensities.

Finally, cardiomyocytes are very well vascularized and since they have more mitochondria are incredibly good at extracting oxygen and using it for aerobic respiration. In fact, even at the rest heart muscle pretty much extracts most usable oxygen from blood which means the only way for the heart to improve oxygen delivery is to improve flow (as it cannot improve on extraction).

These are just some broad concepts but I recommend taking a look at some exercise physiology texts that will help.

Obligatory edit - Woah! this blew up. RIP my inbox. 

Thanks for the kind words. Going to try to answer the most common questions asked here.

**One common question is - what would happen if we replaced all muscle in the body with cardiac muscle?**

**Answer** 

Lots of bad things. Cardiac cells talk to each other through a structure called the intercalated disk which allows all cardiomyocytes to beat synchronously to produce an effective beat. Further, cardiomyocytes are self-excitatory i.e. they contract even without nerve supply (that's why a transplanted heart with its nerves cut still beats - a lot faster rate too because the heart needs the vagus nerve to rein it in). Obviously both of these would be very bad for skeletal muscle as these are incompatible with voluntary, purposeful movements. First, it's precisely because muscle fibers in a motor unit are isolated from others that we're capable of effective movements otherwise a contraction that started in one fiber would rapidly spread to others. Secondly, you can only imagine how bad it would be if skeletal muscle cells started to contract by themselves without any nervous system input. Ninja edit - oh and one more thing, the [metabolic rate of cardiac tissue per unit mass is almost 35-times greater](https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.2000.tb06470.x?sid=nlm%3Apubmed) than skeletal muscle (440 kcal/kg per day vs 13 kcal/kg per day). If we replaced skeletal muscle with cardiac muscle our daily energetic needs would skyrocket given that skeletal muscle is a substantial percentage of our body mass/weight.

**Another common question - some variation of; does the heart only have a set number of beats and if I speed my heart up with exercise am I draining it?**

**Answer**

Not true and please don't stop exercising. If anything, exercise revs up your heart (esp cardio; weight training has very modest cardiac effects) during exercise and that's a good thing (if your heart rate doesn't go up that's bad and called chronotropic incompetence). That's because over time you get what's called eccentric hypertrophy so your heart can pump more blood out per beat (increased stroke volume). Further, regular cardio also increases vagal tone (the vagus is the nerve that slows the heart rate) and this in combination with the increased stroke volume means you get a very nice resting heart rate. [Low resting heart rates](https://heart.bmj.com/content/99/12/882) and [high cardiorespiratory fitness] (https://mayocl.in/2WzpBok) VO2max (or VO2peak) are associated with significantly lower risks of heart disease and all-cause mortality.
As another commentor mentioned, the heart doesn't make lactic acid.

In fact, the final enzyme in the creation of lactic acid, lactate dehydrogenase, works *in reverse* in heart cells! It actually *consumes* lactic acid.
The heart contains a different type of muscle tissue - cardiac muscle tissue. While cardiac tissue has the same organisation as skeletal muscle, its fibers are much shorter, so the force the heart has to exert is far less than that of skeletal muscle. 

Cardiac muscle cells contain a much greater number of mitochondria per cell.  These provide energy in the form of ATP, at a much greater rate than skeletal msucle.

Being as close to the heart (because it is the heart), cardiac muscle also has a constant fresh supply of blood, providing vital oxygen and nutrients. This along with the factors mentioned above, make the heart much less susceptible to fatigue when compared to skeletal muscle.

Edit: grammar
Although the reason was already mentioned, you could also consider, that the heart takes breaks and the muscles of the heart relax during diastole. The breaks usually take up the same time as the contraction(sistole) (if the heart doesn't get much workload, though) so the muscle has enough time to rest.
If your arms are tired of beating, stop beating for a few days.

Also vast difference in force between the coordinated contraction of many individual cardiomyocytes to generate a beat, and the contraction of long multinucleate skeletal muscle fibres pulling on sections of your skeleton. And cardiomyocytes have the highest density of mitochondria, and high vascularisation delivers plenty of oxygen while clearing any lactic acid (to give you some idea just how much the body values that organ being in the best shape it can be to keep beating).

&#x200B;

*I apologise if this post was entertaining as admins have warned me I shouldn't be.*
[removed]
The heart also prefers beta fatty acid oxidation as an energy source over carbohydrates. In layman's terms, the heart prefers to use fat for it's main energy source but if that is not readily available it will use carbohydrates. That, combined with the fact cardiac muscle is better adapted to contract, relax, and contract over and over makes the heart not "tired" compared to your arm or leg muscles
[removed]
Several correct answers here, but a different perspective or way of explaining :
 
There are three types of muscle, skeletal, smooth and cardiac. Skeletal muscles you use for voluntary movement and exercise, as you described arms/legs. Smooth muscles are the involuntary type like the intestines, blood vessels, respiratory tract, etc. Now cardiac is it’s own specific type of muscle that is involuntary but acts quite differently.  Skeletal muscles are the only ones you feel soreness from after overexertion- and as another commenter mentioned, from the lactic acid release.
[removed]
[removed]
See [here](https://redd.it/7ogg5t) and [here](https://redd.it/26gzjf) and [here](https://redd.it/63wd1j) and [here](https://redd.it/p4a8z) and [here](https://redd.it/2an6h1) and [here](https://redd.it/5h4zve) and [here.](https://redd.it/3qahnh)
[removed]
Critical care specialist here. Your heart is okay at lower rates or faster when training during exercise. However it definitely can get "tired" if it was to beat excessively fast for prolonged periods. For example in patients who have irregular heartbeats that are sustained in the 100s, a lot of them will develop heart failure. The fancy medical term is 'tachycardia induced cardiomyopathy'.  Once you slow the heart down, the heart failure usually resolves within a few months.
[removed]
In addition to what everyone else said there is one point that no one brought up: The shape of a human nose helps infants breathe while suckling. Other primates have protruding jaws that allow them to breathe while suckling. If we didn't have our noses we could be more prone to smothering. 
The nose is actually an organ that performs many functions aside from just being a conduit to the respiratory and olfactory system.  The nose warms up and moistens  air before it enters the trachea in the winter and has an air conditioning effect in the summer.  The nose is the first line of defense before foreign particles can be inhaled into the respiratory tract – large particles are captured by hairs and smaller ones get caught in mucus.  In that way, it can be thought of as an organ of the immune system, since it is responsible for draining the sinuses.  The endings of the olfactory nerves go through the roof of the nasal cavity; if there was no protection and just open orifices on the front of the face, you would basically have very little between raw exposed cranial nerve endings and the outside world.  Lastly, the shape of the nose changes vocal resonance and affects your voice.

Edit:  Wow, leaving for a few hours, you wouldn't expect a little comment about the nose to blow up like this, or else I would've elaborated more.  I did a little lower down, going into more detail on theories of racial/climate-related differences in nose shape and comparative anatomy of other animal noses (carotid rete).  I also agree that noses are not usually classified as immune system organs, but I just meant that it has functions with innate immunity.  And good point on helping to keep liquids out of the respiratory tract!
Protection from rain.  Many animals have heads that point down, but with our large brains, we're not designed for that.

https://www.theguardian.com/environment/shortcuts/2015/oct/05/sneezing-monkey-walking-fish-200-new-species-himalayas

Locals have told scientists the monkeys are easy to find because, owing to their upturned noses, water gets into their nostrils when it rains and they start sneezing. On rainy days, they sit with their heads tucked between their legs to stop the water getting up their noses.
[removed]
[removed]
In addition to what everyone has said, the nose plays a big role in taste, too. We can smell our food better with the nostrils right above our mouth and better detect potentially harmful (rotting or poisonous) food. 
The shape of your nose, with wide openings that narrow down as they travel up your nose towards your lungs, condenses the air that you breath in, while at the same time your nose hairs filter out particles and the air gets warmed up.  All 3 of these things make the air easier to accept by the lungs 
[removed]
ooh this is a good question. The nasal structure actually prevents particulates from being inhaled (unless you're a mouth breather... hence the derogatory term). It's also a drainage and filtration system. Key to the immune system. 

Key bumps are a recent cultural phenomena that may damage the nasal cavity but nevertheless, our schnozes snozberry on.
The nasel cavity needs to be long to modulate humidity and temperature, and filter incoming air. Many other mammals have a snout that gives length, but humans have an unusual flat face so our protuding nose provides the length.
So because the air is moistened by your nose it makes sense that places with warmer humid environments have inhabitants with wide noses. Conversely in colder climates humans have longer more pointed noses because the nose needs to warm the air as it is inhaled. The maxillary sinuses are 30% larger in Europeans with thin noses compared to Africans or Malaysians with wide noses.

Actually, the human nose is not all that unique. As cattarhine primates we share a downturned nose with old world monkeys and the other apes. In this sense, we can see our nose as a derived trait shared with our closest ancestors. That said, human noses have evolved among different populations through time to deal with climactic variation (for example growing relatively long to help moisturise and heat cold, dry air among populations living in high latitude environments).
Need some distance for the hairs to catch the incoming pathogens. Also, the downward angle for the holes is less likely to accidentally be penetrated than if they were just flat on the front of your face like that.
When we say "nose" we're actually talking about many tissues working in unison with each other. The holes are the ... well in my language they're "coanas" but they are only the end point of the nose.


The structure of the nose, including the wings of the nose, the internal turbinates and the sagital septum are all in this shape to give stability to the structure while also providing the necessary utility (Heating air/catching undesirable elements/etc.).

If we simply had 2 holes as the question says , we would run into a vast myriad of problems, both simple and complex, for an example of each,a simple problem would be dust would be far more threatening as opposed to just a minor nuissance easily dealt with by a combination of structures and tissue lining of the nose (Nasal mucosa/humidity/nasal bellosidad), for a more complex problem, the upper respiratory airways are delicate, having no form of mitigation to physical stimuli like from temperature, humidity (too high, too low) or again, dust, would be devastating towards any tissue, especially mucosa, in direct contact with the unmiitgated assault.... chronic inflammation of the sinus, abrasion and erosion would be serious possibilities.

To be honest, I don't know for certain, I had not stopped to consider what the effects would be of straight up breathing through coanas...
In addition to what has already been said, the shape of the nose assists in olfactory localization. This is generally under appreciated in humans because we're not as good at scent tracking as dogs, but put your nose to the ground and you'll be impressed at how well you do. 
There is a documentary related to this topic, called Waterbabies. The theory is that humans developed a nose bridge so that when diving into water, the bridge causes water to be diverted from entering the nostrils. I watched it in the 80's on HBO and always remembered the different monkeys featured, and that humans probably evolved from species with nose bridges and not apes.
Humans are long distance runners who cool our bodies by copiously sweating. We evolved as social pack hunters on the hot African savannahs chasing down our prey over long distances. Because we also have flat faces, unlike most other animals, we need a way to prevent the aspiration of sweat into our lungs, which can cause serious health problems. That's why we have protruding noses with nostrils that face down. 
The shape of the nose allows a breastfeeding baby to both suckle for long periods gaining necessary nutrition while still being able to breathe because it pokes the breast away from the area just above the upper lip and this creates an airway 
Oh! Oh! I know the answer to this one!

Google 'Sneezing Monkey"

It's also known as the Myanmar Snub Nosed Monkey. It has no nose just two slits in its mush. It's an evolutionary cul-de-sac that explains why we have protruding downturned nosed.

"The species is known in local dialects of Lisu people as mey nwoah and Law Waw people as myuk na tok te, both of which mean "monkey with an upturned face".[4] Rain allegedly causes it to sneeze due to the short upturned nasal flesh around its nostrils. People from the area report that it sits with its head directed downwards, hiding its face between its knees when it rains"
I got taught (grade 12 high school) that our noses started protruding at an angle to allow our brains to work at a higher temperature (like a cooling vent) so our brains didn't over heat and the hot air could escape / cool air could enter. That's why ancestors from hotter climates vs cooler climates have different shaped noses. 
[removed]
Time to shine! I am a medical student going to 2nd Year right now, but on 1st Year we learn all the basics of Histology and also (your question): Embryology.

The human embryo in the 4th week of development develops the facial primordia in which the main structures of the face are going to appear and form until the 8th week in which they only grow in size.  This facial primordia is located (obviously) on top of the embryo, and it is going to grow even further longitudinally and faster than on width. For that reason the embryo is going to suffer folding of its tissues. Which is important for the following point

The facial primordia is composed of 5 prominences. [A single Frontonasal, paired Maxillary, paired Mandibular.](https://pocketdentistry.com/wp-content/uploads/1008/image003645.jpeg) We want to focus on the Frontonasal as it is the one that forms the Nasal placodes; which are the thickening of that part of tissue, and hence the firstprecursor of the nose. The underlying tissue just beneath the surface is going to proliferate and produce:

- Stretching of the placodes (making them near each other)
- Depressions in the middle (which later will form the holes)
- Primordium of nasal cavity (which is the complete inside of our nose)

After these, the nasal cavity and oral cavity are going to be only one. Later the maxillary prominences will start to grow towards the middle causing the formation of the palate and hence separation of the 2 cavities. Also, it is important its growth upwards [so it can reach the olfactory nerves for the sensation of smell!](http://imgur.com/a/KUEGW) Once it is formed, it is only matter of time for the growth and thickening of the skin and subskin layers that will form the nose. 

Hopefully I will have answered your question detailed enough but without getting too technical. There is a lot of terminology that has to be explained in order to make the explanation easier but I think that for an ELI5 it is enough.

Edit: URL formatiing and minor text tweaks
[removed]
When we presumably shifted from treetops to forest floor and open terrain, we literally faced more wind, particles from sand and many small insects such as flies. Our noses are very aerodynamic, and also allow anything that is gravity fed to bypass our nose so we don't breathe it in. 

Something that many people are failing to mention is our nose shape allows tears to bypass our nostrils. This is very important because our tears carry away pieces of dust and dirt and we unlike every other primate have our nasal cavity connect to our oral cavity. We can choke very easily on things and can not breathe while something is in our throat, so to prevent things from entering our noses is very important. 

Also to a lesser degree our noses are very small and don't detract from our mouths and eyes which are more important from a communication standpoint. 
This is what I always thought: imagine falling face first... with a nose, the worst you could get us a bloody/broken nose falling on a flat surface, but usually nothing else, because the cartilage will break the fall. Without one, falling face first would mean falling forehead first, because there is nothing to break the fall, and smashing your skull against concrete isn't fun. Oh and the nose is also there to distinguish is from aliens :p
[removed]
I have experience here. I had a bad nose job. I had the bridge of my nose narrowed and I discovered that even that tiny change caused extreme discomfort in my life. On top of that there is also some scar tissue in my nose that although minor makes me uncomfortable all the time. The tissue covers part of the inside of my left nostril so I lose the sensation of breath from that side. If your nose works I don't recommend changing it at all. The form and shape is very delicate. 
The most obvious reason that comes to mind is so that things don't fall into our nostrils.  Rain as an example.
Also, it likely enhances our ability to smell the things we are about to ingest which would be a good mechanism to protect against bacteria and poison.
> Fred Hoyle and his colleague 
Chandra Wickramasinghe further eroded enthusiasm for panspennia by suggesting that outer 
space brought us not only life but also many diseases such as flu and bubonic plague, ideas 
that were easily disproved by biochemists. Hoyle — and it seems necessary to insert a 
reminder here that he was one of the great scientific minds of the twentieth century — also 
once suggested, as mentioned earlier, that our noses evolved with the nostrils underneath as a 
way of keeping cosmic pathogens from falling into them as they drifted down from space. 

> .....

*Bill Bryson: A Short History Of Nearly Everything*
It keeps the rain out! that's useful. Also no one wants to see your boogers.

Just think if it was the other way around a sudden sneeze would blind you. Keeping snot out of your eyes has to be useful, or at least those with inverted noses all died out because of gummed up eyes blinding them when sabertooth was around.
Everyone here is correct, and please correct me if I'm wrong but I've been told that it also has something to do with our evolutionary past. Primates often have large or visually noticable buttocks, since that's what is at eye level and therefore serves as the sexually attractive visual signal. It's my understanding that when humans began to walk upright, certain parts of our body began to look like genitalia in order to remap those same visual signals to something we were now likely to see. That's why humans have flat, red lips, which don't look like any other animal but do visually resemble a vagina, and large breasts, which don't resemble any other animal but do resemble a butt, and large pointed noses, which don't resemble any other animal but do resemble a penis.
It allows you to smell something as it is entering your mouth. If whatever you're about to consume is rotten, toxic or putrid, you will surely be able to tell, as your nose is the last gate/checkpoint to discern if something consumable would be harmful to you or not.
Taste is a sort of defense mechanism, and it is a way our bodies determine what it needs to maintain itself.  Smell assists in taste;  therefore the downward nostrils smell food as it enters the mouth.  If food is rotten, our nose can sense it before it enters the mouth.
In accordance with the aquatic ape theory, our noses are shaped the way they are to deflect water away from going up our noses as we swam.  The only other apes with this feature is the proboscis monkey of Borneo, also a good swimmer. 
Well, if you look at other primates, you see they have more "open" nostrils... and where do those other primates spend most of their time?  That's right.. in trees.... when you're tree-bound you spend a lot of time looking down.  And when it rains and you're looking down you don't have to worry about water going up your nose.

Savannah-bound homo erectus, on the other hand, spent a lot of time walking around upright... and when it rains and you're walking upright, you need a bit of protection to keep rain from going up your nostrials.

Soooo.... needless to say the females (and vice versa) of homo erectus had no choice but to breed with the only males (and vice versa) still left alive after a hard downpour.... eventually the protective bits evolved into a full blown nose (so to speak). 
There are large areas in the middle of the major oceanic basins called the subtropical gyres. These could be considered the deserts of the ocean in that the biomass (total mass of all organisms) density and biodiversity are low. This is because the ocean circulation doesn't replenish the nutrients available in these areas for algae to live off, which are the base of the food-chain.

In general, you tend to find much more biomass and biodiversity closer to the coasts. But you can get hotspots of productivity in other places such as upwelling regions and also near seamounts [(shameless plug of my paper on this)](https://www.sciencedirect.com/science/article/pii/S0079661116300702).

Vertically, there is much much more life in the surface 100 m or so as this is where algae can photosynthesise. But even in the ocean deserts (subtropical gyres) you can find all sorts of life in the deep ocean and buried in the mud at the bottom of the ocean. Just not as much as near coasts.
[removed]
[deleted]
Teeming is a relative term. Typically terrestrial and marine biologists both use Chlorophyll-A concentrations as a proxy for measuring life abundance over a large scale area. This is because the producers in an environment fix carbon via photosynthesis, and the pigment Chlorophyll can be measured by satellites as a substitute for plant biomass. NASA satellite imagery of Chl-A can be seen [here](https://eoimages.gsfc.nasa.gov/images/imagerecords/4000/4097/S19972442003273_lrg.jpg)

Notice, most open ocean areas have low Chl-A while coastal zones and select parts of open ocean have higher Chl-A. This is all due to the physical mixing properties of the ocean. Water is not unlike air, in that the warmest (aka least dense) water will float to the top, and the coldest, most dense water will sink to the bottom. Generally, in the open ocean this means the difference in density between the water upper layers and the icy depths is too great, and water can not upwell from the deep to the surface. To circle it back to the original question, when nutrient-rich water from the depths can not upwell, the top water columns of the ocean can not re-cycle nutrients, meaning the nutrient poor-water can not support high densities of primary producers (phytoplankton in the oceans, grass/trees/etc. on land). With low densities of phytoplankton, zooplankton growth will be limited, then small fish will be limited, then big fish, and so on.

**Additional information edit:**

You may notice that the areas of high open ocean productivity coincide with the areas on this [map](https://www.researchgate.net/profile/Jochen_Kaempf/publication/307434835/figure/fig11/AS:601718358306830@1520472244610/Locations-of-significant-coastal-upwelling-regions-in-the-world-ocean.png). These are major upwelling areas of the oceans, generally caused by the wind (although density-driven upwelling does occur in select places). In these areas, nutrient rich waters from the depths rise up to supply vast amounts of ocean life, a prime example being the fishing grounds of South America's West Coast. 

As a side bit, I'm sure you are familiar with open ocean water generally being pretty clear and blue. This is in fact due to the low levels of phytoplankton, as high concentrations will cloud the water. Look at some of the famous examples of clear water beaches (Greece, parts of the Caribbean, Pacific Atolls). Even as coastal areas, they are still low in productivity and concentrations of phytoplankton and as a result their water is clear.
[removed]
The vast majority of all sea life is in the shallows. Looking at a [depth map of the ocean](https://d32ogoqmya1dw8.cloudfront.net/images/eslabs/corals/ocean_depth_colorscale.jpg) gives you a rough idea of where you probably won't find much fish. 

The ecology of the ocean is all about energy. Within 200 metres of depth the sunlight is still able to reach the bottom which results in photosynthetic organism forming the base of a food chain. 

In the middle of the ocean with 2 km of depth beneath you, there simply is no energy to sustain a complex biological environment compared to the shallows.
Since you didn't specify how big, the fact is, if you picked a random spot, the odds are extremely good you'd find it teeming with organisms at least as big as plankton.

The reason is that there are approximately [400 dead zones](https://science.sciencemag.org/content/321/5891/926) in the world's oceans, and their combined area is roughly 245,000 sq km. The area of the [world's oceans](https://www.ngdc.noaa.gov/mgg/global/etopo1_ocean_volumes.html) (see table 1) are 361.9 *million* sq km.

It's terrible that there are so many dead zones, and their size is staggering and depressing. But they're dwarfed by the comparitively massive area of all the world's oceans.
It's all based on food. So the food chain relies on nutrients in the water to feed cyanobacteria and plankton. Then you get other microorganisms feeding on them and up the chain you go. Nutrients in the deep ocean generally come from two sources (any experts feel free time chime in or correct me here). The first is surface runoff from land being carried by currents far out into deep areas. The outer is upwelling of nutrient rich waters from the depths to the photic zone. 

Water found very deep is often capable of carrying much more nutrients as there is no light to allow photosynthetic organisms to convert these nutrients to biomass. So often when a deep water current experiences upwelling you can get very remote areas of ocean able to support much larger amounts of life than the normal nutrient deficient areas.
There's a dead layer in the middle of the ocean where the suns energy doesn't reach and life is rarely present. Bellow this layer there are ecosystems that draw energy from the earth's core, separating them entirely from every living thing on the planet which kicks its food chain off with the sun. BBC planet earth covered it beautifully
The main limiting nutrient in the ocean is iron so if you are to far from land or any upwelling points the phytoplankton will not grow and the water will be to nutrient poor to sustain very much life this is why places like the coast of Morocco are such rich ocean areas because they have upwelling and iron from all the desert dust,the least productive area of ocean is the middle of the South Pacific  or near any land
There are probably more scientific answers above me but I spent a few more than a few years on a submarine and I can tell you sometimes you hear dolphins chatting and whales farting all over the place and other times only hear a few carpenter fish pitter pattering away.
[removed]
There’s an awesome video on this by Atlas Pro, he makes great educational videos. The link is here, describing the ocean as a desert and talks in detail about where you find life in the ocean (near the coasts mainly).


https://youtu.be/MT28gm9CNuI
When sailed home from the PI back to home port near SF, it looked pretty dead. There’s lots of things floating around out there, and little spires poking up out of the water. It felt pretty sad to me, no dolphins, birds or flying fish for days. They made us sail back at 10 freaking knots. The admirals ride, a carrier, passed us like we going backward. Much resentment over that. I wish I could find an old Notice to Mariners, that chronicles everything they know of on weekly(?) basis.
I’ll break this down as simple as possible. All food webs/chains begin with plants. Plants need sunlight and typically grow from the ground. You’re most likely to find these plants that start up food webs/chains closer to the shores of continents and relatively shallower parts of the ocean that cover our planet. Out in the middle of the ocean, yes, there may be phytoplankton towards the upper levels of the water, but that’s about it. So to answer your question, the ocean isn’t as teeming with life as it might appear. The most diverse life to exist are in shallow tropical waters such as coral reefs and other areas of shallow water. Some larger animals my potentially migrate through the open waters, but they won’t stay there for long or try to stay there since their food source is closer to the shores instead of the middle of the ocean.
Not much lives in the open ocean. Stuff needs a surface to grow on and animals need growing things and or other animals to eat. If you dropped a piece of rope in the open ocean, as long at it was there a tiny ecosystem could attach to it, but once broken down or removed, that would go away. You’ll find things in shallow water where there is solid surface and sunlight, at the bottom of the ocean where there is heat, around islands, and around atolls(really old eroded islands)
What I really want to know: if I’m sailing across the Pacific, (1) should I expect to catch fish when I can’t see land and (2) should I be worried about sharks if I go swimming, eg the Oceanic Whitetip, which I have heard describes as an “open ocean” shark?
The ocean is classified as the biggest dessert on earth because the measure used to describe a dessert is the ratio of surface area or volume to biomass.

There is life everywhere, and as some others have pointed out, in the ocean it mostly consists of planckton, but it is relatively scarce.

So, the simple answer is; it is mostly empty with some areas where there is abundant macroscopic life like hydrothermal vents, guyots, and costal waters!
So, technically it will most likely be teeming with life, just microscopic life. However there are “dead zones” with little to no oxygen caused by pollution. I’m pretty sure the microscopic life will not be found there (but I cannot confirm, as I focus more on cetaceans [dolphins and whales]). If you discount the microscopic life, then yes there are “dead zones” of fish. There are many such zones along the ocean floor, as they need to stay where the food is
[removed]
[removed]
[removed]
Here’s a handy chart from Randall Munroe (XKCD): https://xkcd.com/radiation/

You may notice that cell phones and other tech are not on this chart. This is because the radiation emitted by these devices is so weak, they are not capable of altering your cells (non-ionizing radiation). Bananas, on the other hand, do emit ionizing radiation (just a very, very, very small amount. You do not need to be worried about bananas). So you might explain to your parents that bananas are more dangerous than cell phones, and ask them if they know anyone who has died suspiciously after eating a couple bananas
No, it is not.

Phones and other devices that broadcast (tablets, laptops, you name it ...) emit electromagnetic (EM) radiation. EM radiation comes in many different forms, but it is typically characterized by its frequency (or wavelength, the two are directly connected).

Most mobile devices communicate with EM signals in the frequency range running from a few hundred megahertz (MHz) to a few gigahertz (GHz).

So what happens when we're hit with EM radiation? Well, it depends on the frequency. The frequency of the radiation determines the energy of the individual photons that make up the radiation. Higher frequency = higher energy photons. If photons have sufficiently high energy, they can damage a molecule and, by extension, a cell in your body. There's no exact frequency threshold from which point on EM radiation can cause damage in this way, but 1 petahertz (PHz, or 1,000,000 GHz) is a good rough estimate. For photons that don't have this much energy, the most they can hope to achieve is to see their energy converted into heat.

Converting EM radiation into a heat is the #1 activity of a very popular kitchen appliance: The microwave oven. This device emits EM radiation with a frequency of about 2.4 GHz to heat your milk and burn your noodles (while leaving parts of the meal suspiciously cold).

The attentive reader should now say to themselves: Wait a minute! This 2.4 GHz of the microwave oven is right there between the "few hundred MHz" and "few GHz" frequency range of our mobile devices. So are our devices mini-microwave ovens?

As it turns out, 2.4 GHz is also the frequency used by many wifi routers (and devices connecting to them) (which coincidentally is the reason why poorly shielded microwave ovens can cause dropped wifi connections when active). But this is where the second important variable that determines the effects of EM radiation comes into play: intensity.

A microwave oven operates with a power of somewhere around the 1,000 W (depending on the model), whereas a router has a broadcast power that is limited (by law, in most countries) to 0.1 W. That makes a microwave oven 10,000 more powerful than a wifi router at maximum output. And mobile devices typically broadcast at even lower intensities, to conserve battery. And while microwave ovens are designed to focus their radiation on a small volume in the interior of the oven, routers and mobile devices throw their radiation out in every direction.

So, not only is EM radiation emitted by our devices not energetic enough to cause direct damage, the intensity with which it is emitted is orders of magnitude lower to cause any noticeable heating.

But to close, I would like to discuss one more source of EM radiation. A source from which we receive radiation with frequencies ranging from 100 terahertz (THz) to 1 PHz or even slightly more. Yes, that overlaps with the range of potentially damaging radiation. And even more, the intensity of this radiation varies, but can reach up to tens of W. That's not the total emitted, but the total that directly reaches a human being. Not quite microwave oven level, but enough to make you feel much hotter when exposed to it.

So what is this source of EM radiation and why isn't it banned yet? The source is none other than the Sun. (And it's probably not yet banned due to the powerful agricultural lobby.) Our Sun blasts us with radiation that is far more energetic (to the point where it can be damaging) than anything our devices produce and with far greater intensity. Even indoors, behind a window, you'll receive so much more energy from the Sun (directly or indirectly when reflected by the sky or various objects) than you do from the ensemble of our mobile devices.
[removed]
No. Most evidence says it isn't true.

Radio waves are a form of electromagnetic (EM) radiation and the term "radiation" scares a lot of people, but EM radiation also includes the heat from the sun, visible light, radio waves, and x-rays. Some of these are more dangerous than others. In a lot of cases people in medicine and physics talk about "ionizing" and "non-ionizing" radiation. "Ionizing" radiation means that it can knock an election lose (making an "ion") and break a chemical bond. Since our DNA is a large molecule this type of radiation can change the structure of DNA and create errors in the DNA which can cause cancer. Non-ionizing radiation can't do this. Radio waves, and visible light are all lower energy non-ionizing radiation, while ultra violet light and x-rays are ionizing radiation.

Now, having said that, obviously enough heat will cause burns. Microwaves are non-ionizing, but enough of them can boil water and cook meat, so it isn't enough to just say that "non-ionizing" means that it's safe. Now a microwave oven produces 500-1000 watts of EM radiation, while a cell phone produces at most 0.7-1 watt (some old ones could go up to 3 watts, but in practice cell phones try to send a weaker signal to conserve battery life, so even the 0.7 watt is very rare). A 1000 watt light bulb will really hurt your eyes while a 1 watt bulb isn't enough to read by, and it's a similar for cell phone -- very low power doesn't appear to be a concern.

Now, one other problem is that In addition to the strong chemical bonds (e.g. ionic and covalent), there are also weaker bonds (e.g. hydrogen bonds) that don't actually require ionizing radiation to break them. Now breaking a hydrogen bond in DNA doesn't change the structure, but it can unzip the DNA, and cause different genes to be expressed. Every cell in your body has the same DNA, but your liver and your skin cells read different parts of the DNA so they make different proteins, which means they look and act differently. Some non-ionizing radiation can affect this (e.g. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2633881/) and while this is the same general range as the airport body scanners, the scanners probably don't provide enough energy or for long enough to show this effect. However, some of the new 5G cell phones, and some high-speed wireless (that some phone companies are rolling out to simplify connecting new houses, not the normal WiFi), also use similar EM energies and they would involve much longer exposure times so should probably be studied better.

For a long time there was concern about living near high voltage power lines, since they also produce EM radiation (of the non-ionizing type). It turns out that people who live under power lines get cancer more often, but when this was analysed more carefully, the problem is that poor people tend to live under power lines and poor people tend to get cancer more often. We still don't know exactly why poor people tend to get cancer more -- it could be don't know if this is this is due to stress, food, chemicals from clothes, or less access to health care, but we're now sure that it isn't due to the power lines).

Nothing is absolutely, provably, safe in all cases, and it is worth continuing to examine new technologies, but as far as wee can tell there's more danger from not getting enough sleep due to staying up playing on your phone than there is from the radiation from the phone.
[removed]
The short answer is no.

Cellphones emit non-ionising radiation only, therefore they are unable to damage your DNA in a way that would cause any cancers or other serious health issues. 

After a number of formal studies on cellphone radiation, no serious danger has been demonstrated. More telling, however, are the results of the informal experiment that we are all taking part in every day. Cellphones have been frequently used by a significant portion of the population for decades now, more than long enough for any associated pathology to make itself known. Although it is true that there may be further developments as time goes on and we see the effects of a full lifetime of use, it would appear that instances of things like brain cancer and dementia have not risen in any appreciable way that tracks the proliferation of cellphone use, nor does there seem to be any increase in risk as dose increases. 

Given all this it is pretty safe to say that there is no need for concern but if you are still worried then you could take some basic precautions like using a wired headset or speakerphone when making calls, keeping your phone in a bag or hip holster if you are worried about your gonads and, of course, you could try to limit your usage.

[EDIT]
P.S. You shouldn't take my word for all this though, I'm neither a doctor nor scientist, the writers of the blog linked below, however, definitely are. Rather than linking a single article or firing off a bunch of URLs, I have provided a link to a search of the topic on SBM (Science-Based Medicine) so you can choose which ones to read for yourself.  
As a small aside I couldn't recommend SBM more highly, they are a fantastic bunch of writers and they really (like, really really)know their shit. They blog about more than just medicine too btw so you'll find a few gems covering other areas of science too.

https://sciencebasedmedicine.org/?s=Cellphone+radiation&category_name=&submit=Search
I know you've gotten great information already, especially from Rannasha. This is an [article with a pie chart](https://www.epa.gov/radiation/radiation-sources-and-doses). I'd like to give you a few numbers you can quote.

Of all the radiation you're exposed to in a whole year, give "consumer electronics" a value of 1. You get almost that much again from a chest X-ray. INSIDE your body from dietary minerals and bananas and stuff you eat, 2.5. Radon contamination in your basement is about 12. The safest, most radiation-free space in your entire world is between your cell phone and your ear. So please quit warning me about my cell phone and get your basement inspected.
No. They are just trying to keep you away from your phone. 
There has been a famous test that was as follows:
They asked people who said radiation emmited by radio towers, TVs... had bad effect on their health to move to a location where there is absolutely no radiation. They moved there and said that they were feeling much better. Later they found out that there were no radio towers there because a gigantic radio tower was built there that was used to send signals to space and they didn't want any interference from the sorrounding commercial towers. This towers emmited way more radio signals so it turned out it didn't matter how much radiation there was. People were just lying.
When I worked at a service desk at a uni I had a concerned mother call asking about her daughter’s exposure to wifi and mobi signals. My simple answer was she’ll probably get more harmful (solar) radiation walking from her student accomodation across a couple of sports fields to uni than from any wifi or mobi. 
I feel like the whole picture hasn't been conveyed in the top comments as certain studies have found a slight correlation between the non-ionizing radiation emitted by cell phones and certain types of brain tumors. The majority of studies have shown no such correlation or a statistically inconclusive correlation. The problem with any research on this issue, as far as I understand, is that the span in which people have been heavily using cell phones is relatively short in comparison to a human lifespan. In any case, the issue isn't done and dusted and a great deal of research will be conducted on the subject as cell phone users age.

Here is a somewhat technical source that does a very good job of summing research into the issue, and also links to the few studies favoring increased risk: [https://dceg.cancer.gov/research/how-we-study/exposure-assessment/cellular-telephones-brain-tumors](https://dceg.cancer.gov/research/how-we-study/exposure-assessment/cellular-telephones-brain-tumors)

Here is a less technical fact sheet on the issue with some Q&A your parents might appreciate: [https://www.cancer.gov/about-cancer/causes-prevention/risk/radiation/cell-phones-fact-sheet](https://www.cancer.gov/about-cancer/causes-prevention/risk/radiation/cell-phones-fact-sheet)

Edited for word choice on the controversial bit

Also, I certainly wouldn't stop using a cell phone over this - just wanted to point out an applicable research area that hasn't been pointed out by other commenters. 
Every thing on this planet emit radiation. Even carbon in your body has some c12 in it which is radio activity.  
One thing you need to remember is 
"it's not the substance that is dangerous but the quality it is taken in".  
Any radiation in low quantity from cell phone can't even kill microbes in air. Forget about doing anything to body cells.  
This was a popular click bait in newspapers and in TV news   
They say "Radiation is harmful".  
Then "Cell phone emits radiation".  
Both of which are technically correct because radiation in higer quantity is harmful. And cell phone do emit radiation. But it's not enough to harm
[removed]
It's been answered very scientifically above, the tl;dr version is that "radiation" means different things, and typically people hear "radiation" and think Chernobyl and uranium but in the case of devices it's a different kind. Light is "radiated" from a light bulb. We are constantly bathed in EM radiation - if you turn on your radio and hear a radio station, that's because electromagnetic RADIATION is reaching it's antenna from the broadcast station.

No. *All* electronics emit radiation in the form of electromagnetic waves but it's not dangerous unless it's **ionizing** radiation. Ionizing radiation is capable of damaging cells and can cause cancer but for radiation to be ionizing, it has to be a much higher frequency than what electronics emit
EM waves have a scale, from the largely harmless microwaves, to IR (which causes molecular bonds to oscilate, which we sense as heat), to visible light (which can cause cis/trans transformations in molecules), to UV (which can create radicals - ozone is made by UV), to x-rays which can ionize atoms further, to gamma which can cause serious damage.

Either way, your average desklamp is 1000x as powerful and dangerous as a phone or any such electronic device, and your parents do not have even a hint of a technical education.
It just comes down to Ionizing vs Non-Ionizing radiation. There are no RF frequencies being produced in consumer electronics that have ionizing radiation. You start getting ionizing radiation with X-rays and higher frequency technologies 
[removed]
[removed]
Every phone sold overseas has a way to find the SAR value. People do this regularly before they buy the phone. ..." Any smartphone at or below this SAR levels is “safe” to use. You can check Radiation level in terms of SAR of your smartphone by dialing a USSD code \*#07#, if results shows SAR below 1.6 watts per kilogram (1.6 W/kg) then it is OK otherwise you are advised to change your smartphone immediately...." 
Non-thermal effects of non-ionizing radiation are those not directly related to an increase in tissue temperature (the thermal effect), but rather to other changes that occur in the tissue as a result of exposure to electric or magnetic fields.  Such changes may be caused by a sequence of biological processes and may affect the living organism at different levels (from the molecular to the cellular, the organ, and the whole living organism) activating various mechanisms that affect health.

Epidemiologic studies pointing to an association between non-ionizing radiation and the development of cancerous tumors led to a working hypothesis that living cells in the body are able to 'sense' non-ionizing radiation and react to it without undergoing heating. To verify this hypothesis, the effects of different frequencies of non-ionizing radiation on intracellular processes in tissue cultures of various cell types were studied. 

As expected, non-ionizing radiation did not have a significant effect on the DNA sequence or on its structure.  Conversely, slight but reproducible effects were observed on intracellular mechanisms of signal transduction, including of free radical formation, phosphorylation and protein breakdown. These findings showed that cells are able to 'sense' non-ionizing radiation; however to date there is no evidence that the 'sensing' leads to physiological changes, including cell proliferation or death. 

In order to reveal the mechanisms whereby radiation affects living cells, additional research on more sensitive systems (such as mutation -bearing or DNA-deficient cells) is needed.
Another user gave a wonderful answer already, but it lacks some of the very specific data.

There are two kinds of ways that "radiation" or electromagnetic waves can hurt you.

The mild way is with heat transfer. We all own and operate a cool device that performs this in a contained box: the microwave.

When a photon collides with an atom, it is either reflected away or it is absorbed by the atom's electron(s). When the electron absorbs the energy, it speeds up and bumps to a higher energy level (where it is unstable and imbalanced), and then falls back down while releasing that energy as heat, light, etc.

This can warm you up as you absorb that energy. The hydrogen atom in the water in your skin might catch a photon, release the energy as heat, and cause the molecule to wiggle a little more than normal, which you'll feel as heat if it happens enough.

With enough heat transfer, you can cause some fun chemical reactions, most notably combustion, or some physical changes like melting or vaporization.

This can definitely harm you, but only in extremes. Your phone battery and circuitry will generate much more heat than you could ever absorb from it's radiation. 

So how else cab radiation hurt you? In a process called **ionization**.

Ionization occurs when the photon has so much energy that when an electron absorbs the photon, it speeds up so much that it actually escapes it's orbit of the nucleus. By losing an electron, the atom now has an imbalanced charge and by definition becomes an **ion**.

Therefore this atom was ionized. 

The ionization energy requires is different for each element and electron energy level on that element, but the simplest is the ionization level of hydrogen, which only has one electron. It requires 13.6 electron volts (a measurement of energy) to ionize a hydrogen atom.

Anyone who's taken some chemistry might start thinking now "hold on, what if the electron that leaves was part of a bond?"

Electrons are what allow atoms to form molecular bonds. If one suddenly gets  blasted away, we have a problem: The bond will break.

That breaks the molecule. For simple molecules like water, nbd. It'll find a new hydrogen soon enough.

The real danger is for massive long chain molecules like DNA.

When you break a bond in a DNA molecule, it doesn't always get fixed properly. When it doesn't get fixed properly, the cell either dies, or it can "glitch out" and continually reproduce. That's the fast and dirty definition of what cancer is.

Well the good news is that most electromagnetic radiation doesn't have photons with enough energy to even do that. You only get to that amount of energy when you're in the UV,  x-ray, and gamma radiation range.

Radio, microwaves, and visible light simply do not contain enough energy per photon to cause ionization.
I’m usually a lurker but I didn’t see this in the comments, so here we go: 4 years ago Veritasium did a good video about cell phone usage and brain cancer risk. 

https://youtu.be/wU5XkhUGzBs

tldr; these days there’s a lot more phones out there than a couple decades ago, but the rate of brain cancer didn’t increase. We can conclude that not only phones don’t increase brain cancer risk, but also that nothing did. 
[removed]
Im working on a residential renovation and the client had us protect all high voltage cable with a material called "G-Iron"  we also wrapped all exterior walls in an aluminum mesh.  
No cell phones or wifi devices in the house.   

They are concerned with EMF emissions.  

We are also testing the home once all the furniture is in for VOC's from the memory foam and fabrics.   

How much harm this stuff does,  i can't say but it's a hell of a market.  

But this is in New York City... step outside and in one breath you can taste it all
There is no scientific evidence for harmful effects from cell phone signals. There is, however, a lot of hype and emotion-driven hand wringing in some circles online - not different from all the conspiracy theories you see online all the time.

The transmitter on a typical cell phone makes about 1 W of electromagnetic radiation. I am a HAM radio operator. Me and my buddies build and use transmitters up to 1 kW (1000 times more powerful) or more, at all kinds of frequencies (including those that are close to cell phone frequencies) and with all kinds of modulation (including digital transmissions like the ones used by cell phones). This hobby is over one century old now. No one in this hobby is dying of any mysterious illnesses. Case closed.
Yes, they do. It is a similar type of radiation emitted by the sun, lightbulbs, and certain types of deep sea fish. Concentrated doses in the for of beams can easily cause severe burns. It is even used to shape diamonds, one of the hardest materials know to man. 

Most people call that type of radiation light. 
Thanks for this. I've been considering bluetooth headband to listen to music while I sleep but feared 8 hours of low wifi level for such a long period. Do you think length of exposure on a concentrated spot (my dome) makes a difference, even at low wattage?
our current networks and cell phones might not be too much to worry about but the upcoming 5 gigahertz network has been linked to cancer undoubtedly

https://www.wired.com/beyond-the-beyond/2018/03/5g-smartphones-cause-cancer-big-wireless-doesnt-want-know/

https://www.latimes.com/business/la-fi-cellphone-5g-health-20160808-snap-story.html


to put it simply, no. we have no long term studies of their effects, so we have no proof, but there is no reason they would and there is also no proof the universe didnt come into existence last thursday with memories of what seems like the past
I always understood this to be a carry-over from the  days of the old cathode ray tube TVs which did emit small, mostly harmless, amounts of X-Rays, but you were told not to get too close to them to reduce exposure.  Like many other pieces of good advice, it didn't age well with advanced in technology, and is now an old wives' tale.
Probably not, but there was a study released a bit ago that found there was a definitive increase in both heart and brain cancer of rats subjected to radiation similar to what mobile phones put out.

It pays to be sceptical of people giving you a hard answer one way or another, as there is definitely not a consensus in the scientific community on this. 

https://www.theweek.co.uk/fact-check/95290/fact-check-do-mobile-phones-cause-cancer 

https://www.healthline.com/health-news/sorting-out-the-science-can-cell-phones-give-you-cancer
On a related note, the wifi in my house goes to crap when I strike an arc with my TIG welder with the HF function enabled. Is this due to back EMF in the house's electrical system or broadcasting EM radiation from the arc?

This is especially confusing because I'm welding outside but the welding machine (with the HF stuff) is inside of the house. Am I ruining the whole neighborhood's wifi when I melt aluminum?
As others have said, the radiation is so weak but if you are still worried: The closer you are to the masts the safer you are. This is because the receiver in mobile devices doesn’t have to use lots of power to try to maintain/pick up the signal. 
There was a recent study on mice about if radiation produced from smart phones can cause cancer. 
The result was yes, phone radiation does cause cancer but the conditions of the experiment was overkill; 9hours of exposure per day with the phone next the the mice head the whole time.
Yes, they do emit radiation, but so does your furnace, your home, your lights, and almost anything in your home that uses electricity. 

   Now, is it dangerous? If you strapped it to your head for years on end, it may do some odd things, but again, your dealing with radiation of sorts from everywhere in your live so often there's a better chance you'll get your death dose outside from the sun. 

   Ask them about the radon system in there home, and if their afraid of that radiation, or if the ultraviolet radiation from the sun terrifies them, or if the thermal radiation from their furnace keeps them up at night. Chances are they either didn't know or didn't realize that they deal with way worse radiation in their lives then phones daily. 
Cellphones emit non-ionising radiation only, therefore they are unable to damage your DNA in a way that would cause any cancers or other serious health issues.

After a number of formal studies on cellphone radiation, no serious danger has been demonstrated. More telling, however, are the results of the informal experiment that we are all taking part in every day. Cellphones have been frequently used by a significant portion of the population for decades now, more than long enough for any associated pathology to make itself known. Although it is true that there may be further developments as time goes on and we see the effects of a full lifetime of use, it would appear that instances of things like brain cancer and dementia have not risen in any appreciable way that tracks the proliferation of cellphone use, nor does there seem to be any increase in risk as dose increases.

Given all this it is pretty safe to say that there is no need for concern but if you are still worried then you could take some basic precautions like using a wired headset or speakerphone when making calls, keeping your phone in a bag or hip holster if you are worried about your gonads and, of course, you could try to limit your usage.
Yes, they emit radiation.

Depending on which study you read (most bought and paid for by the same companies making the stuff that pump radiation into the surroundings), the results are anywhere from "this stuff will cause cancer in your brain in a few years" to "there's no problem whatsoever with radiation coming from a device and streaming through your body".
[removed]
Yes. It's true. They're called "photons", usually in the visible light spectrum. It appears that when emitted in particular sequences, and with certain regularity, they transform promising young people into unemployable slobs
[removed]
There is evidence of tumor growth in rats caused by cell phone radiation in a massive study conducted by the National Toxicology Program in the USA. You can pick apart the study however you want, but it is the best data currently available. It also did not get great media exposure when released, and I hear from those that conducted the study that the telecom folks were extremely upset.

Link to study highlights https://ntp.niehs.nih.gov/results/areas/cellphones/index.html

The risk of using tech and mobile devices on the move comes almost entirely from inattention to the environment. In that regard it is deadlier than sharks or lightning.

Depending on your age, heavy use could also impact your vision.

So as for radiation, tech does emit some, and intensive use has not been widespread or existed long enough that we can tell the damage (if any) for sure. In fact, only in the past 5-10 years or so have mobile phones, tablets and WiFi been widespread enough that very young children have access to them. To this day we do not know for sure (and it wouldn't be ethical to test) the long term impact of mobile phones on infant and child health. All we know is that as far as we can tell, it doesn't kill people in the short term, just like tobacco or eating too many sausages won't. 
- These last two have been proven to cause DNA damage in the short term on animal models, by the way, whereas for this particular radiation, it wasn't so clear cut, so any impact might take even longer to appear. -

Animal model studies make the radiation out to be mostly safe (although enough of them find a correlation with some form of cancer to be more than just pure chance), but can't emulate 40 years of exposure in 5 years. I visited a mobile phone radiation testing lab (where they only measured the radiation, not its effects) about 15 years ago and they were concerned enough then that all cellular phone models for sale in that country have been required to display radiation levels for years (France).

Now, it it highly unlikely you have the particular cocktail of genetic anomalies that would be triggered into cancer by this kind of radiation. However it is not impossible. This said, it IS way more likely that such a device would kill or seriously injure you or someone else near you (such as children you might have) simply by distracting you. 

Now if only I had time to go and link sources for all these statements!
edit: added note about relative tested safety of confirmed cancer-causing long-term health impacting behaviours.
[removed]
[removed]
Anything with a light emits radiant energy... Anything with a screen emits "dangerous" radiation.

An oscillating fan creates dangerous "buffeted air" that can cause genetic mutations and nodding your head kills brain cells.

The key to living a happy life is knowing when to pick your battles... I'm not convinced that "cellphone radiation" is a hill I'm willing to die on.
Okay but...
Am I harming myself by carrying my phone around in my boobs?

Serious question btw.

I have a phone case with a card holder on the back, so it holds all of my important things. I just tuck it into my bra in lieu of carrying around a purse.
Am I putting myself at an increased risk of breast cancer?

I generally have wifi/data/location turned off to save battery. I usually mean to put airplane mode on if I'm not expecting a call, but often forget.
Mostly untrue.

Phone do emit radiation, however the radiation it emits is non-ionizing radiation and is quite safe like the other have mentioned.

However, there are situations that this type of radiation becomes dangerous. The radiation emiited by your phone is quite similar to that of a microwave. In some rare cases when surrounded by hundreds or even thousands (more calculation is required for extract number) of phones, this type of radiation can become dangerous by heating the human body similar to that  food inside a microwave. Of course this almost never happens.

Another case is the older generation of display technology that utilizes accelerated electrons for display (CRT). Ionizing radiation can be emitted in that case. However, you need to travel back in time to find such technologies.
[removed]
The Mariana trench is about 11,000 meters below sea level. If you built a cofferdam down to that depth, as u/SirHerald suggests, the pressure would be about 3.2 atm. Divers experience a similar pressure at a depth of around 22 meters, so humans can survive these pressures in the short term. In the long term, I found [this chart](http://www.hq.nasa.gov/pao/History/conghand/fig15d3.gif) from NASA suggesting you would have to worry about mild [nitrogen narcosis](https://en.wikipedia.org/wiki/Nitrogen_narcosis), making you feel slightly drunk, and mild [oxygen toxicity](https://en.wikipedia.org/wiki/Oxygen_toxicity). So you probably wouldn't want to live at that pressure, but a short trip down wouldn't be any worse than diving.

But, your question was about draining the oceans. The oceans cover 72% of the surface of the earth, and the average depth of the ocean is about 3700 meters. That means a large portion of the atmosphere will sink into the ocean, resetting 1 atm of pressure to a depth of something like 2600 meters below sea level. So that would make the Mariana trench a bit more tolerable at around 2.5 atm, but it would also mean some high altitude cities would become nearly uninhabitable. 

**edit** Fixed the bracket on the link. Wish I had caught it before I left this morning - wasn't expecting this to blow up so much.
The oxygen would re-distribute, and as such some of the higher elevation places would see a dramatic decrease in air thickness. Think akin to how on higher elevation mountain the air is thinner which affects blood flow and your brain. Hypoxia occurs when not enough oxygen is distributed among the body, it can be general (throughout) or localized to a specific part. Since the Oxygen re-distribution would re-scale altitude, people would suffer Altitude Sickness. We know the extreme baseline for this is 6000m, at which and beyond, human sustained life is not possible. Essentially those effects would now be on higher elevated cities. So anything currently exceeding 1000m would, under the new distribution be at the threshold where our current 6000m altitude sickness baseline is. BASICALLY we'd just have to move to more coastal situated cities. 
Instead of draining the ocean, what about a cofferdam down to the ocean floor? Something like a giant tube all the way down so that you were at the elevation of the bottom of the ocean.

Are you interested in whether we could breath there or if the pressure would be too high?
[removed]
Randall Munroe explores the scenario of draining the Earth's ocean onto Mars in his book "What If - Serious scientific answers to absurd hypothetical questions". It's a great read and might quench your nutty knowledge thirst
Could I append to the question what the atmosphere at present day sea level would be like? Would I just have to follow the high altitude baking instructions or could there be real hazard because the air is too thin? 
Well... hold on a second. Just draining the ocean would change the topography of the earth, and we'd have the same amount of atmosphere. So I guess we'd HAVE to move down there, or learn to breathe the thinner air way up here.

If you're replacing the water with air such that the pressure stays the same where we are now... I imagine it would feel strange breathing in the thick soup of air down in the Marianas Trench or whatever. But we've already got many kilometers of atmosphere, so I don't know that it would be dangerous to add a few more.

Couldn't you do the experiment by just going down a 5 km shaft or something?
http://blogs.agu.org/martianchronicles/2011/08/01/9800-feet/
The pressure at the bottom of the sea would be roughly what we have at sea-level today.   
That's because we're draining the ocean, not adding more air, and air pressure is determined by the amount of air *above* you. 

The problem would be at what is now sea level, which would instead get air pressures equal to about 3700 meters.  At that height, altitude sickness can become a problem. 
The deepest part of the ocean is the Challenger Deep which is 10,994 meters below sea level. If we calculate out the pressure at sea level of 1 atm at 70 degrees F, then the pressure at the Challenger Deep would be 3.14 atm. 

As for temperature and oxygen levels, in not sure how that would factor into the equation to determine if one could survive in those conditions. 

EDIT: this is roughly 46 psi vs the 14 psi at sea level so it would be really tough to survive in that environment. 
Poster here.  Going to state this here as well since I replied to somebody else with this.

This is ignoring obvious facts that drained ocean would wreak havoc and we'd obviously all be long gone.  This is an environmental question.  

It's nearly impossible to answer, which is probably why it bugs me so much.  

For this question, I guess there are two assumptions that need to be made.  Either A) Assuming current pressures/temps/gas levels remain unchanged at sea level, what would they be at Challenger Deep and Mariana Trench?  Then B) ok, ocean magically disappears...what happens to current temps/pressure/gasses at sea level?  Do they fill void that water once occupied?  Does current sea level environment move to bottom of the ocean floor?  If so, what happens at current high altitude levels...do they get even more extreme?
If the water were replaced by enough air to keep (former) sea level pressure the same, then the bottom of the Mariana Trench it would be about 3.2 atmospheres. Not immediately lethal, but I think you'd have issues with nitrogen narcosis. Most of the ocean floor is much shallower than this, and would have a pressure around 1.5 atmospheres. No biggie.

However, if the water just "goes away somehow", then the existing air will fill that space -- and it's a lot of space to fill. About 70% of the Earth's surface is covered by oceans, and the average oceanic depth is about 2.3 miles (3.7 km). There are a lot of factors to consider, not the least of which is what happens to the atmosphere without all that water, so I'm not going to attempt the math, but a ballpark estimate would be everywhere on land would end up with an atmospheric pressure equivalent to around 9000 feet higher than it is now. That would be high-altitude cooking, but [bigger issues would occur](http://what-if.xkcd.com/103/).
[deleted]
theres about equal amounts of air and water, 1.4 billion~ ~~square~~ cubic kilometers of both, the air would drop into the place occupied by the oceans, the thermosphere would drop on our heads and we would all suffocate.
[deleted]
Followup question: If we drained the ocean and kept the same amount of air in the atmosphere, how dramatically would the atmosphere change? Would the areas above sea level have a significant decrease in oxygen levels? Would the different levels of the atmosphere drop? Would the treeline and snowline change as well?
I guess we are also assuming the biomass of life in all the oceans is figured into the mass volume of water, or should that be calculated and removed from the equation and left for dead on the new earths surface...
The oceans' volume is a small portion of the volume of the atmosphere, so the pressure difference would be pretty small, a couple of percent at most. But there would be less water *in* the atmosphere, so subtract the partial pressure of H20, which is another couple of percent. So less than ten percent overall.

Normal variation of atmospheric pressure due to weather is on the order of ten percent, and people can't tell that's happening without something to amplify the effect.

So removing the water from the planet wouldn't make a difference in pressure that by itself people will notice.
What do you call this question? I'm curious what you would call a question that has a lot of complexity to it on an unrealistic situation. I REALLY love trying to answer and read about these types of questions so can anyone tell me what you would define this question as?? 
I think the volume of water is just slightly less than the volume of breathable  air, at 1.4 billion cubic KM v 1.6 billion cubic KM for breathable air [need a fact check]. So based on that information it would be probable that the land we are living on now would become much harder to live on as it would be considered at much higher altitude. I don't expect there would be any issue living at the deepest point as this would have standard air pressure. Source: I'm some guy guessing his way through the question.
Not strictly related to the question but to add to the catastrophic scenarios depicted by others, if all the oceans would disappear instantly without being magically replaced by additional air, then the movement of atmosphere falling down into the basins to fill the void would be another great issue. I don't know how to exactly calculate the resulting average horizontal wind speed, but an approximate guess is that it will be far greater than 300km/h. 
Optometrist here. It should cause no permanent damage to vision in an adult. Many elderly people have dense cataracts (which, if left long enough, would act pretty much the same as covering the eye). When these cataracts are removed, vision goes back to normal, assuming that no other diseases are present. 
[removed]
I’m on mobile, not sure how to cite but Hubel and Wiesel actually won a Nobel Prize for answering that question in cats. No permanent effects would be apparent in adults, they would just have a hard time adjusting to light at first. However, if a newborn child (in this case kitten) were to have its eyes sewn shut for a year, the visual system would not develop correctly and they would basically be blind

http://thebrain.mcgill.ca/flash/capsules/experience_rouge05.html
[removed]
[removed]
TL;DR: No unless the brain is still developing.

I just got my degree in Psychology and can answer this in terms of brain development.
Permanent loss of vision has not been observed in adults, as u/Moorgan17 stated earlier. Permanent loss of vision has been seen however in young animals. A study was conducted years ago where one of a kitten's eyes was stitched up pretty much right from birth. I believe the eye was kept closed for about a year (at least a couple of months, but I'll double check that). When the eye was opened back up again, the kitten was permanently blind in that eye. I'll try find then start and link it. I'm on mobile right now.

This has to do with the Occam's razor effect that developing brains have. This behaviour transcends species and basically means "If you're not using it, get rid of it". It's a way of the brain not putting effort into something that doesn't work anymore. It's even been shown that the part of the brain associated with the loss of sight ends up working on some other part of the body (kind of like how blind people have slightly heightened other senses). This is known as brain plasticity.

Side note: brain plasticity also works in the opposite way. A young girl had half of her brain removed in an attempt to relieve her of her constant seizures. This left the young girl paralyzed on one half of her body and with several cognitive difficulties too. Over time, however, the half of the brain that was left began to compensate for the lack of brain mass and she began to regain movement in the side of her body affected by the surgery, her language and other cognitive skills. 

Basically brains are cool. 

Edit: a video on Jody, the girl with half a brain https://youtu.be/VaDlLD97CLM

Edit 2: for those interested in the kitten study, see Hubel, D. H. & Wiesel, T. N. (1964). Effects of Monocular Deprivation in Kittens

As it turns out, the kittens' eyes were sutured for only a few weeks, not even months.
[removed]
[removed]
[removed]
Though there would be no permanent visual damage, some studies suggest that this may promote the entanglement of other sensory nerves with the visual cortex. The blind have been proven to develop some form of echo location with practice, and further study has shown that the visual cortex is partially activated during such exercises. So in not seeing for a year, your vision won’t change when you go back to it, you may just build neural pathways to “see” sound in some capacity as well. 
[removed]
[removed]
[removed]
There's a book you might find interesting, by neuropsychologist Oliver Sacks.
It's the one about hallucinations, I don't have the title at hand, I'll edit it in. Many people whose vision becomes impaired, experience visual hallucinations. Didn't answer your question directly, but you already have plenty of answers and I thought this might be interesting to you, as it's kind of a related topic.

Edit: The English title is, in fact, 'hallucinations'.
The German title is 'Drachen, Doppelgänger und Dämonen. Über Menschen mit Halluzinationen.'
Depends on the stage of development.  It's not only vision that could be affected- it's the ability to perceive.  Kittens exposed to only horizontal lines couldn't see vertical lines and vice versa.  People that had their vision restored who lost it at a very early age had a great deal of difficulty understanding the signals that were coming into their brain.  You could explain to them what they were seeing, but they had no frame of reference with which to process the images, although [this article](http://www.sciencemag.org/news/2015/10/feature-giving-blind-people-sight-illuminates-brain-s-secrets) seems to indicate that the ability to perceive can be gained in as little as a year and a half.
[removed]
Selective “pruning “ would rob a developing brain. And adult brain could have? Comes. A study in 1978 with students wearing glasses that reversed their vision resulted in a “flip“ after one week of wearing glasses, this took a week to reverse. I suspect that your “volunteer“ may have some more or possibly  worsened results
[removed]
[removed]
[removed]
This post has attracted a large number of anecdotes. The mod team would like to remind you that **personal anecdotes and requests for medical advice are against [AskScience's rules](/r/askscience/wiki/rules)**. In particular **this is not the right place to share what your earliest memory is**.

We expect users to answer questions with accurate, in-depth explanations, including peer-reviewed sources where possible. If you are not an expert in the domain please refrain from speculating.


It’s not that we forget our earliest years, it’s that we don’t form memories in the first place. The term for this is infantile “amnesia”, but this is not actually a form of amnesia — that would require forgetting. As infants grow into toddlers, their brains grow fantastically quickly. So much so, that any pathways that are deemed unimportant/weak are “pruned”. Pruning is the technical term actually. By age three, pruning calms down to the point where toddlers can start forming the memories they’ll remember for possibly the rest of their lives. However, usually the very earliest memories are traumatic or notable in some way. Pruning continues for some time into childhood. Maybe age 4. 

Edit: this has gotten more traction than I anticipated, and I have to clarify: when I say “we don’t form memories in the first place” what I’m saying is that we form memories that we will NOT be able to RECALL after our brains grow and pruning occurs. Toddlers can remember things, but, as we’ve all experienced, they forget an exceptional amount of their day-to-day experiences. As another user pointed out, toddlers actually are learning a metric ton of stuff (motor, language, etc.). 

Edit 2: another user pointed out that it is the delayed development of the hippocampus (learning/memory/encoding center) that contributes to infantile amnesia. 
What you are taking about is autobiographical memory.  Infants easily form other types of memories, including procedural memory (eg learning to walk), and semantic memory (learning what things are).  Infantile amnesia is limited to autobiographical memories.  There are lots of ideas as to why this is.  One is a lack of self awareness - the child hasn't yet learned that they are an individual and that there are other individuals.  The second is language development.  Autobiographical memory is often descriptive, eg. "I went to the store ".  Encoding this memory may rely on language skills that have not yet developed.  These are all related to the underlying neurophysiological development.
Check out this article, it's essentially saying that the brain isn't really concerned about memories at that age, it just needs to get a grip on how reality works.

https://www.livescience.com/32963-why-dont-we-remember-being-babies.html

This article covers a much wider range of subjects including the gap in memory varies by up to 2 years depending on where you're born, the article says one theory on that is some cultures don't value early childhood memories and that may affect how hard we try to hold onto them. The article has no definitive answer but covers what appear to be reasonable theories such as the brain isn't "recording" episodic memories or *can't* record them yet due to a still-growing hippocampus.
http://www.bbc.com/future/story/20160726-the-mystery-of-why-you-cant-remember-being-a-baby

This article is more focused on the explanation being proposed that the brain is still developing neurons in the region where memories are formed. 
https://www.scientificamerican.com/article/why-can-t-you-remember-being-a-baby/

I am absolutely not an expert in this field but I've read enough to know that memories are the connections between neurons in specific parts of the brain. There is also an element of flexibility, called [plasticity](https://www.brainhq.com/brain-resources/brain-plasticity/what-is-brain-plasticityhttps://www.brainhq.com/brain-resources/brain-plasticity/what-is-brain-plasticity), in the brain that defines how easily connections are made or broken. 

When we are very young the brain is very flexible, we have a lot to learn. Connections are made and broken quickly. 

It would seem plausible that driving force for not being able to recall specific memories while the brain is still adding neurons to the memory-related parts of the brain(the bbc article mentions this) and while the brain is still extraordinarily flexible, those connections that make memories may not last very long. New neurons would disrupt those networks of connections and new memories could be making use of connections involved in other memories until there isn't enough left of a given episode's network to recall the memory. 

I wouldn't be surprised if culture played into that as well, if you try to hold onto a memory your attempt to access it can strengthen the connections to the episode over time and make it harder to forget(no idea if that affects accuracy of the memory). If no one in your culture gives a shit about a specific type of memory why bother trying to recall it? 

I know just enough about this stuff to get myself into trouble so check the sources and don't quote me on jack shit.
This might be tangential, but for those of us that do have some memories before the age of three, what causes this difference? (As far as I can tell, they weren't traumatic or impactful memories either; just a couple of specific moments.)
2 Memory systems are in place, subconscious instinctive learning, which is active even earlier than described, responsible for recognizing patterns much quicker than a cognitive approach, such as potentially dangerous situations or markers of danger, speech tones, etc. and is very resistant to change once a pattern is learned, which also has the potential to be problematic later in life when our mind is reacting in a non-beneficial manner. "Unlocking the emotional brain: Eliminating symptoms at their roots using memory reconsolidation" addresses much of the research surrounding this, as before 2007, research had largely concluded it was so difficult to change subconscious patterns once formed that the focus became cognitive behavioral therapy of trying to create entirely new pattern recognition on top of already learned responses.

The second memory storage is the conscious one, which is much more plastic and changeable. Memories are being recorded again at very early ages, but the issue with this system is that the catalogue and recall system hasn't developed until around 3 years, so there isn't a way to recall memories beyond that point.

The difference in these 2 systems can be very distinctly seen when memories of a traumatic event fade or are blocked from the mind, but the pattern recognition still causes anxiety when faced with a similar situation

Subconscious pattern recognition is also much faster than cognitive thought, consisting of learned knowledge and truths, such as being able to recall that fire is hot and not to touch it or phobias, the event where we learned it doesn't need to be recalled for the situation to be assessed very quickly by instinct built over our lifetime
Pruning continues to go on throughout your life. For instance, you can probably remember what you had last night for supper,  but do you recall what supper was a week ago from last Monday? Unless it was remarkable in some way, probably not.
I have a related question (hope this is allowed):

What is the science on remembering feelings/emotions during this time? For example, I’ve read that relationships with parents are very important during this time, and emotional development is heavily influenced by events during this time even if they aren’t remembered (such as feelings of connection or love and on the flip side, threat or abandonment). Can anyone speak to the existence of that from a scientific perspective?
Infants form many memories such as learning how to crawl and walk and eat as well as emotional memories such as who is their favorite caretaker and what they look and sound like.

They do not fully form episodic memories because the hippocampus isn't fully formed until age 3.
[removed]
An interesting related phenomena is called "source amnesia". As you've pointed out in your question, some people seem to recall very early childhood memories, for example how their infant mobiles looked like. As many different studies have shown (i.e. Julia Shaw's book "the memory illusion" is a good read on this) people may actually belief to remember things that other people have told them. When, for example, their parents told them stories about them as a toddler and they think about these multiple times when telling others, it might happen that they actually think they remember these facts from them being a baby and not from their parent's stories.     
Our memories are sometimes way more fallible than most of us think.
There are a lot of things that you learn in your first few years. Most neuroscientists would call those 'memories.' What I think you're talking about is 'narrative recall.' That's when someone tells a story about what happened to them in the past: "I was 4 and in the living room and the cat fell off the sofa, It was so scared that it ran right in to me. It was so traumatic! I'll remember it forever."

Narrative recall requires a lot of acculturation. You need to have a concept of narrative and the narrative self as well as narrative composition skills. All of that cultural learning take a long time. You just don't have the psychological facility to lay down and rehearse self-narratives until you're a few years old. 

EDIT: typos. 
You shouldn't equate moments of critical development (underlying neural connections) with "memorable" moments. You seem to be thinking of **episodic memory**, which is likely a result of communication between networks in the hippocampus and cortex. These pathways might not be developed at a young age, and so you might not have distinct episodic memories. Nevertheless, important neuronal growth was occurring at the time.
I see pruning mentioned here and I'm not sure that's really the only reason why there's something like infantile amnesia.  I think the development of language and formation of hippocampal structures and white matter tracts play a role.   

In addition, there's a cognitive reason that people only remember things after a certain age. Another way to look at it is the development of the sense of self.  If you understand memory formation (explicit memories that is), it is tied to various things like emotional valence, personal relevance, etc.  This helps form autobiographical memories. One theory is that there's a shift in memory when a child develops a sense of self where there is a personalization of memories.  

some references: 
https://link.springer.com/chapter/10.1007/978-1-4615-9364-5_7

https://books.google.com/books?id=w1YrFypvmn8C&pg=PA67&lpg=PA67&dq=Development+of+Higher+Brain+Functions:+Birth+Through+Adolescence&source=bl&ots=J0mUh1YEFR&sig=R27kae-gbp0g_QEKSfOH13Ve34E&hl=en&sa=X&ved=0ahUKEwimye78pNjYAhVfHGMKHYB9CF4Q6AEIKDAA#v=onepage&q=Development%20of%20Higher%20Brain%20Functions%3A%20Birth%20Through%20Adolescence&f=false

https://www.researchgate.net/profile/Mary_Courage/publication/278857485_The_emergence_and_early_development_of_autobiographical_memory/links/55c9dda008aebc967dfa3c74.pdf


disclaimer; clinical neuropsychologist here.
Memory requires having a frame of reference and context. 
It's kind of like asking why there were no functioning apps on your conputer before you installed the system software. You can have all the peripherals hooked up for input and output... camera, keyboard, mouse monitors, speakers, but until that system software is written to memory, loaded, and up and running with the computer properly booted, none of that is going to be committed anywhere. You could even have a live internet connection, a home network, and a camera with a passthrough to the monitor to view the feed in real time, but until that system software is there, nothing is gonna save your video and upload it to youtube. The difference between the computer and us, is that we write our own system software, bit by bit, and build our own context ourselves with our nurturing and with hardware build by our genetics.
It depends what kind of memories you're talking about. Implicit memory stands in contrast to explicit memory and it is memory that is not consciously recalled (implicit memory is unconscious memory). These memories can begin even before birth (prenatal memory).


[removed]
Finally something I can answer! Being a psychology minor I took Psychology of childhood and infancy last semester and was fascinated when I learned the answer to this question. This is commonly referred to as childhood amnesia and the reason this occurs has everything to do with both language and memory. In short children need to form memories to remember correct? But how do we form memories? By language! If you cannot understand the language fully yet than it is very difficult to develop long term memories. Children between the ages of 1-3 can form short term memories of course but not long term. For most people the language skills don't fully develop until ages 4-5 which is when you begin creating long term memories and why you can remember events from when you were 4 or 5 but not necessarily when you were 3 or younger. 
[removed]
Interesting article suggesting how memories are remembered in narrative/storytelling cultures eg Maori and cultures with extended family structures eg Italy at younger ages compared to the norm.

https://www.thecut.com/2016/07/italians-maori-and-divorce-kids-have-earliest-memory.html
I read once that our earliest memory is almost always a moment that makes us realize that we are an individual and have a separate consciousness, experience, and identity to others.

Apparently countries where children are generally respected as individuals, educated from a younger age, or given more independence have an earlier average “first memory” like 3-4 years old, but countries where kids start school later and dont have as much independence etc have earliest memories of more like 5-6.

Having lived in China for 8 years confirmed this to be the case based on my questioning of friends, students etc. 
Infant brains are fairly undeveloped. In fact, newborn's sensory cortex isn't fully developed, so all of their senses blur together - music has color and a taste, for example - called synesthesia. Here's a fun read on it: www.scientificamerican.com/article/infant-kandinskys/
[removed]
[removed]
[removed]
[removed]
This is very interesting. Are their studies by doctors on how best to capitalize on such a small scope of time? 
Is there some sort of stimulation that how’s best results for analytical problem solving and improving hand/eye coordination or sharpen reflexes?

I don’t have children, but lots of my friends do and I’d love to help them out as new parents besides reading Hungry Hungry Caterpillar and making animal sounds to make them giggle.
During the ages 1 to 4, our brain has not yet developed fully, it is the age when the brain has to process lots of data like faces, voices, languages etc.. at that age if a language has to be learned by a child then it has to look for connections between different people and objects. Unlike now, we learn a language with translations and synonyms. I wouldn't say that the storage space is less but I'd say a developing brain wouldn't multitask. ( read and store at the same time )

It doesn't mean we don't remember anything from that age, emotional memories like pain can be recalled often.

Memory is not actually you remembering an event. It's the recalling act in which you recall the "last time" you recalled that memory. 
So, the childhood brain does store memories, lots of them.  However, the part of the brain that keeps track of where those memories are stored isn't fully developed before age 4.  The memories are there, but you just can't access them.  It's like having a library that has lots of books but hasn't catalogued them so it's impossible for the librarians to find any particular book.
Sorry I am so late to the party. I know Paul Frankland http://www.franklandlab.com/ has very neat leading hypotheses on this question. The hypothesis is that memories are formed during neurogenesis, and that the rate of neurogenesis governs the rate of forgetting - and thus produces infant amnesia.  
  
He published a paper on this in Science a few years ago.  
http://science.sciencemag.org/content/344/6184/598  

[removed]
[removed]
Keep in mind that answering questions with personal anecdotes and/or layperson speculation is not allowed. We require relevant expertise and/or scientific sources for top-level answers. Simply owning a dog does not qualify you as an expert.

Cheers.
Dogs are relatively intelligent, with complex personalities, so the answer isn't completely "yes" or "no.

The simple answer is no, in the general case dogs can recognize other dogs, but don't show preference for certain breeds.  This has been tested with simple visual identification:  Dogs recognize the dog species among several other species on a computer screen  https://www.sciencedaily.com/releases/2013/02/130214103703.htm

The complicated answer is closer to "yes", because the size and behavior of the other dogs will be relevant to their social dynamic.  Also, dogs may respond to certain smells or visual cues that correspond to various breeds depending on that dog's personal experience; Paedomorphosis affects agonistic visual signals of domestic dogs.  (In fact there is a study called "Paedomorphosis affects agonistic visual signals of domestic dogs. 

https://www.sciencedirect.com/science/article/pii/S000334729690370X)


In addition to that, the instinctive methods of communication may be slightly different between breeds


So while dogs don't necessarily prefer one breed over others, the morphology and psychology inherent to different breeds will affect how they treat different dogs.  In addition, they can make positive or negative associations that correlate to those breeds.
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
Even though the "red" in the altered image is literally just gray, it is still *more* red than green is on the hue scale. If we break the colors into the RGB components, the green will be something like R:0 G:200 B:50, and the grey will be R:100 G:100 G:100, that is, much more red component and much less green component.

The eye/brain's color processing is super complex, but the most simple version is that we take a lot of cues from the *differences* between shades, and not their absolute values.
Your brain will compensate away the green tint, similar to automatic color balance in a camera, and the gray will then look red in comparison. Knowing anything about strawberries is not necessary.  The brain guesses that the background is meant to be white, which helps this compensation happen.
I actually messed around with that picture today. Here's an imgur link to my experiments: http://imgur.com/a/y9TGC

I made it an indexed image with 3 colors: black, white, and blue.  Zoomed out, the berries look red (so it wasn't the varied shades of gray causing it).

The more you zoom in, the effect goes away.  I was staring at it up close, then would zoom out slowly and there was like a magic % zoom where the colors would just kind of pop in - really weird. 

I have a cousin-in-law that has a degree in art/painting/color theory and said it had to do with the color white, since it's made of all colors.  I tried replacing the white with a ton of different colors, and the illusion immediately goes away.

I then did some color corrections (curves) and re-indexed it with 5 colors: red, green, blue, black, white - and was able to recreate what the original image probably was.  
This has always been my favorite illusion like this.  The center cross pieces of each X are the exact same color!

[Image](http://www.illusionspoint.com/wp-content/uploads/2010/09/Color-Adapting-optical-illusion-4.jpg)

If you cover up everything but the center pieces, you can see that they're identical.
Yes. A person who has never seen a strawberry before, will still see them red.

You can crop a small part of the image (where you see red) with 'paint' for example, and look at it alone, so that it is no longer obvious that you are looking at a strawberry, and you still see that small area red.

However, if you copy that reddish color (use the color picker tool) and paint some lines in other place, over white background, you won't see it red any longer. Now it will be clearly grey.
It becomes reddish when you use that color next to the other greenish color.

[example](http://i.imgur.com/matFq6b.png)
[removed]
I have had a look at the rgb values for some of the pixels, and it seems like the strawberries have a higher red value than the surrounding background, and even though the green and blue values are higher, they are consistent. this shows that the strawberries aren't explicitly red, but in the context of the photo, they are
See if this helps clear things up: http://imgur.com/a/asxki

TIDL (Too impatient, didn't look): It has nothing to do with the strawberries, it only has to do with the two colors being next to each other.  Your brain perceives color as what is there modified by what is nearby.  Local changes in color affect your brain's perception.
This is called color constancy.Color is calculated as ratios of wavelengths across spatial locations on the retina. Many traditional theories of color vision such as trichromaticity theory and opponent-process theory fail to account for it. But the Retinex theory is specifically designed to explain this.  
https://en.wikipedia.org/wiki/Color_constancy   
https://www.scientificamerican.com/article/the-retinex-theory-of-color-vision/

Be clear that the image is *not* monochrome.  A green tinted monochrome picture could not achieve the same effect, otherwise black and white TVs could have reproduced colour just by having a green filter in front.
The image is green/blue and grey.  That's significant because the grey contains reds, greens and blues within it - whites and greys appear neutral because they contain the other colours in equal degrees.  The red is not coming from no-where - it is actually there.  The green/blue tint of parts of the picture that should be neutral leads your brain to automatically compensate by removing the greens/blues from the whole of the picture - thereby bringing out the reds *that were already there* in the grey parts. 
I think the brain just removes the tint from the image. You can do the same in Photoshop using levels, [this is the result](https://i.gyazo.com/9f38361cbd0425eabc2247cbef3f7172.png).
A feature of the color spectrum is that most colors are made up of combinations of other colors and when there is an imbalance of a given colors representativeness, it can cause another color within to stand out more prominently which is more noticeable when you place that color against a similar color that is more pure.

So, showing a hue of green, that is made up of more red, in isolation, it will look green.

Show the same color next to a more pure green, the red will stand out more.

This is not your brain tricking you or some artifact of psychology. It's just how the color spectrum works given the lighting conditions.
There's a few things going on here,

1. The picture is not "all green". It has red in it. It is green tinted and desaturated, meaning individually, the colors look pretty not-red. But there is red in the components of the color.

2. So if you look at the RGB values, the berry color is around (173, 182, 181), and the table is (158, 225, 208). You can see, there is significantly more "red" in the berries both absolutely (173 to 158) and relatively (the G and B components go from ~180 to 208-225). 

So the only really remarkable thing is that when you look at the colors individually you see green and grey. It just turns out that putting those two next to each other highlights the relatively high amount of red in the "grey" color.
EDIT: TL;DR view these images I made first: http://imgur.com/a/42m1K

THERE ARE RED PIXELS IN THE IMAGE! Each physical "pixel" on your display has 3 channels - red, green, and blue. Something on the screen appears white when the brightness is 100% for all 3 channels. We see black when they are all off, and we see gray when the brightness of each channel is equivalent but less than 100% (for example R:70% G:70% B:70%). We perceive different colors on a phone, computer, TV, etc... when the ratio between the RGB channels changes, and unless the "pure" color is being displayed, all 3 RGB channels will all be active. In other words, intensity differences between these 3 channels within a pixel create the perception of different colors. Although something may appear blue, it will actually contain varying degrees of red and green as well. This means that RED LIGHT IS PHYSICALLY EMANATING from your screen in the strawberry image!!! You can verify this yourself by using a strong magnifying glass (a pocket microscope would be best)!

This will help to visualize what I am talking about: http://cdn.bigshotcamera.com/images/learn/LCD/color_pixel.jpg
If this image were actual size, you would see (from left to right) a red, green, and blue rectangle on the top row and a white, followed by black rectangle on the bottom row: https://upload.wikimedia.org/wikipedia/commons/a/a6/LCD_pixels.jpg

But zooming in to the strawberries or sampling the color in photoshop looks grey - why? Well when you zoom an image it gets "pixelated" and you begin to see defined squares. Zoom in enough and you can see individual pixels right? Not exactly. These are *rendered* pixels, meaning that if you were able to somehow physically stretch your screen to the same size, you would see that same gray color. In reality however, you would see the three RGB bars - the gray color displayed is the rendering/ sum/ blend of the 3 channels in that pixel and the color that should theoretically exist. 

Recall earlier I said that gray is perceived on a screen when the RGB values are of equal but less than 100% brightness. This is a bit oversimplified because the values do not need to be exactly equal, gray starts to be seen as white around 90% brightness, and there are other factors as well. However, it is IMPORTANT to note that things we see as gray will have approximately the same intensity for each channel. This means that the viral strawberry image DOES SHOW RED LIGHT! If you still don't believe me, I imported the linked image into photoshop and made some edits. I also used my phone's camera focused through the pocket microscope to demonstrate what was happening on the actual laptop screen: http://imgur.com/a/42m1K
If you open the image (RGB) in photoshop and turn off the Green and Blue channels, you'll see that all the detail of the strawberry's is on the red channel, there's just more of a green tint over it than usual. The red is there and you're seeing it, but it also doesn't look like it normally would because there's a green tint. if you turn off the red channel and leave just the Green and Blue, it looks obviously Green, and looses all the detail.
Our brain can probably account for looking through transparent colored objects. 

When you throw a green filter on an entire picture, our brain probably realizes it's looking through a "green window" and can tell which objects are "more red" than the other objects through that window. 

The same thing happens at night when you're looking at things under yellow lighting (electric light). Everything is more "yellow" than daylight but you get used to it and see everything as it's normal colors. 


Wanted to chime in just for the interesting factor, I'm colorblind and I didn't even realize the effect until reading the comments and then I had to go back and zoom in to convince myself the strawberries were actually gray in the photo. I think this is a neat photo because you're experiencing a similar reaction (I believe) to what I do all day, where my brain will see something that's rather dull or gray and then adjust the color in my mind so based on context so I still look at it and go "hey thats red." To me it doesn't feel like I'm not actually seeing the red, but if someone who isn't colorblind could look through my eyes they might think things are pretty gray. 
[deleted]
Basically your brain assumes: "the plate is more white than that". 
The Picture shows a cyan plate. 
To compensate for this discreptency your brian "adds" the missing red (red+cyan = white) to the whole scene. 

https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/AdditiveColor.svg/400px-AdditiveColor.svg.png

cyan plate + red = white plate.
grey strawberries + red = red strawberries.


I think this has something to do with colour opposites as well. The strawberries has green as its opposite and a green filter somehow still evokes the reds. I tested a blue filter on its opposite of [orange/s](http://i.imgur.com/B4zk7Xw.jpg), and it had the same effect of preserving the orange colour in spite of there being no orange in the picture, only greens and blues. 
I feel like while the effect is real, this picture is somewhat inaccurate. If you zoom in real close to that region they've highlighted, there are clearly some red tinted pixels. Quite a few actually. It looks like they took a color average of all the pixels in the circle, some blue/green and some red, to justify saying its gray. I think they pushed a little more red saturation out to accentuate the effect, but that means they actually are slightly red, and their graphic is somewhat misleading. Either that or my phone is somehow to blame.

Edit: To clarify, if you focused on a red area with *no seeds*, you'd find that the strawberries are, in fact slightly red.
Well you see, this is a bit of an unfair picture because brown is essentially a shade of red.  But it is a correct assumption in that if your mind imagines a different color, with the help of contrast, it will be a different color to you.
If you open up this picture in whatever painting software you have, and use the "grab color" you can see that there are large bits of strawberry that have much more red and look less grey than the sample color they grabbed. I think this picture is deceptive since they grabbed the greyist bit of strawberry rather than a part that actually does have a red tint in it.
There are tribes that can actually see many more shades of green than we can, but have trouble with other colors.  However, it's not just an issue of having seen a strawberry, because there is inter-generational memory as well.  

Some people speculate that ancient Greeks actually saw the ocean as red, which is the color they refer to it as.  
Your description of the picture isn't correct. "All-green" would be shades of green only... The samples shown on the right demonstrate that there is gray in the picture. Gray isn't a shade of green. It's white, with a lower luminosity. White, as represented on a display, is pretty much equal luminosity red, green, and blue. 

So you have the parts that look green, where green is higher luminosity than red and blue. Then you have parts that look gray, where RGB are about the same.

This pic also has parts that look sort of teal (the plate, and the stuff between the strawberries). Those parts are about equal green and blue, with less red.

You see red in the strawberries in this pic because the strawberries are more red than anything else in this pic.
[removed]
Ex stage tech here, so light and colour are among my favourite toys.  The human brain corrects for colour of real things so as to see a "warm white" lighting condition (really it's close to white in the yellow spectrum).  

We're adapted to orbiting a yellow sun, and that yellow light moves closer to white when it transits an atmosphere of "blue" nitrogen.  Being diurnal creatures, our brains are adapted to see this as the default and to adjust what we see accordingly.  

We "defilter" colour by assuming that the image has had a single wavelength of light amplified while all others are reduced and it turns that wavelength down and the rest up.  Picture a GEQ where only one slider can be above the midline at once while the rest are an equal distance below, then picture correcting this all back to the midline.  

Like all abilities, this one varies between individuals.  Those with very good colour correction might become graphic artists, electrical engineers or stage techs.  Those with very poor colour correction see a white and gold dress.  
sorry, those are red with a green filter over top.  they are still red. they are not grey like the colored circle to the right appear. for example. if you cover every part of the picture except a part of a strawberry, you will see red still. you will see red, because the strawberries are red in the picture, with a green filter over the red. like a green hue on top of red.  the circles to the right are a trick. the grey one is anyways. *easiest place to demonstrate this is the edges of the cut strawberries. 
Even if you'd never seen a strawberry in your life, you would still correctly interpret them as red. Your visual cortex's ability to perceive relative colors is the whole basis of [this illusion](http://www.slate.com/content/dam/slate/blogs/bad_astronomy/2013/12/06/greysquares_illusion.jpg.CROP.original-original.jpg).
This picture just has a green gradient on top of it. Any web developer can easily do this with CSS, that's why any picture with text over it often has a black layer on top of the picture which still allows you to see the picture, but also places an emphasis on the text.
I put together a little demonstration on this image when I saw it on Facebook: http://imgur.com/a/VTW3N 

The trick to understanding what's going on is the following: the image still contains red light and gradations of it as well. Any neutral tones such as grey or white contain an equal mixture of red, green, and blue light. What we experience as cyan happens when there is less red light coming from a point than there is green and blue. However, our eyes also tend to balance color towards neutrality because of the evolutionary advantage of being able to differentiate color in a multitude of lighting conditions. Combining these concepts, we can understand how our eyes will take the strawberry image, boost the red and suppress the green and blue in order to attempt to "white balance," which makes the grey look more red than it actually is.

It technically has little to do with the fact that we know that strawberries should look a certain way. We could do the same thing with something that we don't know the original color, though it would definitely work best with  red, green, blue, cyan, magenta, or yellow
A way I like to look at it is that the tinting is narrowing the color range, from all the colors to a range that is closer to the tint color (in this case green). The strawberries appear to use as red, even though they are technically not, because within the changed color range it is *closer* to red. 

Imagine the color wheel, where at first have the full color range, so 360°, and the stronger the tint color is, the narrower the angle, with the tint color at the middle, until you only have one point, that is, only the tint color is left.

I talked about it in one of my videos about color science: https://youtu.be/0sUKEDXYCc4?t=10m28s . This video is a bit more simplistic though, since I primarily made it for artists, and what is relevant for drawing and painting.
It's not psychological. You could make it of red clothes-pegs andthey'd still look red.

Your brain balances against the hue of the image, which is green, and the brown-green strawberries have the green hue of the whole image plus red component, so your brain just reports the red component as the interesting bit.

The green background tricks your brain into thinking a green filter has been placed over the objects in the picture.

Thus your brain removes the green from whatever color they actually are, and identify them as the color they would be if the filter was removed.

There is a really interesting TED talk on this subject by [Beau Lotto](https://www.youtube.com/watch?v=mf5otGNbkuc&t=926s), I linked only the relevant portion, but the entire talk is pretty good.

E: the important part from the video is that the appearance of the edges of the filter on the background around the center squares makes your brain identify the area in the filter, and this triggers it to assume the purple squares are the same, and there is just a yellow filter over the center ones.
I am not that impressed with this illusion but maybe that has to do with me working with RGB color fairly regularly. It is clear this illusion factors in how the human brain works so I just want to get that out of the way. What I wanted to say is along the lines of what others have already suggested. There are no PURE red pixels in the image but that doesn't mean much as there are no pure red pixels in any image that realistically depicts everyday life. Pixels in that image still have red information in it. It just happens to never have more red info than green or blue info so there is an imbalance of shifting towards cyan. The thing that makes the strawberries look red is the fact that the pixels that make them up are relatively more red than the surrounding pixels. I knew this is how we perceived color but just didn't know there was a term "color constancy" for it.
The picture is not 'all green'.  Use a sampler and pick any of the 'red appearing' pixels as a color source.  
  
Examine the resultant RGB values.  
  
There is SUBSTANTIAL red involved in all of the red-appearing elements, they have just been masked by increased green and blue values.  
  
So, the image is not 'all-green', it is just green-tinted.
one comment,  holy crap,  looking at the kingdavid73 images
( http://imgur.com/a/y9TGC ) 

so I see red on that first image.  I see red on that second image.  I SAW RED ON THAT THIRD IMAGE.  what the hell man.  those aren't even strawberries anymore,  it is just a few black rectangles.  wow. 
Zooming in on the strawberry and using my hand to constrict a pinhole, it doesn't look like the gray color they chose to show is representative of most of the pixels. It's certainly in there...but there are definitely pixels that look red shifted from the teal. 
What makes me see red is all the people on Reddit who MUST BE RIGHT AT ALL COSTS, and wouldn't admit they're wrong if the sky fell in.  The red, green and blue _channels_ of a pixel make up the pixel's color.  Just because the red channel has a nonzero value, or even if it has the highest of the three values, does not make it red.
A while back I looked at this image in Photoshop so that I could sample colors and see what the spectrum of color was in the entire image.  What I discovered is that even though the small sample they took of the strawberry looks grey, it's actually in the red spectrum of color.  So, when the gradient changes on the strawberries (the picture goes from light to dark, or vice versa), the grey is still in the red color spectrum.  Even though you can sample one small area and see that it doesn't look red, it's actually many variants of red (think of it as a slightly red tinted grey).

Apply that same process to the rest of the image and you'll see that while the entire image appears to be green, it's actually got a lot more color than you realize.
Not an answer, but it would have been cool if you had omitted the word "strawberries" or any color mentions in the post title so those of us reading the post would be able to guess what it is without immediately knowing what is in the image.
Along with the top rated correct answer, gray tones will show up as the complimentary color of whatever it is surrounded

Red is complimentary to green.

Gray mixed with a color will show up as it's complimentary. For example if you want to paint a fiery orange sunset, you can plot in some gray between the clouds to make it look like a bit of the blue sky is peaking from behind.

example http://www.oilpaintingswholesalefromchina.com/oilpaintingshop/Hudson%20River/Albert%20Bierstadt/049.jpg


Wait a second, will this work on a printed piece of paper with the naked eye. 

Because we are all looking at it on the computer screen which is using red pixels to show us the white, blue, and grey.

I won't be surprised if this illusion works in real life too.

Also since we know strawberries are Red, Would the effect still work with a picture of a car?
[removed]
Strictly speaking, we CAN use most nuclear waste. Breeder reactors can be used to consume pretty much all usable fissle materials and produce a much lower volume of equivalent waste with different properties.

Breeder reactors can be used to manufacture weapons grade fissile material though, so there's political aspects, as well as economic ones-- uranium is fairly abundant.

Edit: spelling errors.
Radioactivity, by itself, is not that useful for generating power. What is useful for generating power is the induced splitting of _lots_ of atoms at the same time, not the slow trickle of energy release you get from radioactive decay alone. To put it another way: nuclear reactors don't work because their fuel is radioactive, they work because their fuel is splittable by neutrons. Those are not the same thing (all fuel splittable by neutrons is radioactive, but not all radioactive atoms are splittable by neutrons). 
[removed]
It is very usable, just not in our current nuclear reactors. Uranium fuel rods are pellets of uranium held together by a metal casing. Being inside a reactor causes the metal to become brittle, and the life of the fuel rod is determined by the life of that casing. 

In other reactor designs, like molten salt reactors, this casing is not used. The fuel stays in the reactor for much longer and much more of the potential energy is extracted. This results in lower volumes of waste that is much less radioactive for much less time than that coming from traditional reactors. 

Learn more about molten salt reactors [here](https://en.m.wikipedia.org/wiki/Molten_salt_reactor). They’re pretty awesome!
Nuclear fuel becomes “waste” when it no longer produces enough neutrons to sustain a nuclear chain reaction. The kinetic energy of fission fragments from neutron interactions (aka fission) drives the heat cycle that produces power. The gamma and beta radiation produced by decay of “waste” isotopes contribute very little.
American scientists developed a nuclear reprocessing system called PUREX (Plutonium-Uranium Extraction) to resume spent nuclear fuel rods. But Ford suspended it and Carter ended it permanently because of the plutonium by-product and the Non-Proliferation Agreements. Now France reprocesses many nation's fuel for reuse using this 50 year old technology.
There are lots of things classed as “nuclear waste”.  Some of those things can be used as fuel, or for other purposes.

The devil is in the details.  Which materials?  Used for what?  At what cost? Etc.

Most reactors are metal reactors and using these materials is either technically infeasible or economically infeasible.

But Liquid Fluoride Thorium Reactors (LFTRs) can use many nuclear waste materials as fuel.  They are an interesting technology I expect to see make an impact in the coming years.

EDIT: spelling

A lot of good answers here. But also in the US, there are actually laws in place that hinder our ability to even push for nuclear fuel recycling technologies. It's mostly dueto the Ford/Carter fear of fissile by-products and the cold war as some have pointed out. 

My company, in France, is all about pushing for recycling their waste.

That said, you need the right products left to split for heat production to make energy worth the effort. It's not about that they give off radiation, so this will only help so much. 

TBH it is a conversation I would LOVE to see more of though as it is a real way we can promote the energy and make it a bit safer considering how much the world is relying on it while fearing it lately. 
In a nuclear reactor, energy is released via fission. Some nuclides, such as U-235, are fissile, meaning they can easily undergo fission. Other nuclides, like U-238, are fertile; this means they can be converted to a fissile material by absorbing neutrons.

Nuclear waste consists of many different nuclides. There is some uranium left in the waste, and it is possible to reprocess the spent fuel to retrieve it. This is pretty expensive, and the US doesn't currently do this.  Many of the other components of nuclear waste (Cs-137, Sr-90, ...) are not fissile or fertile, so they aren't useful for generating nuclear power even though they are still highly radioactive.
Commercial nuclear reactor operator here.
The fuel itself is still very usable.  There is still plenty of unused uranium in the fuel assemblies.  The problem is that some of the fission products are poisons to the reaction.  Meaning that some fission products, xenon and samarium mainly,  will absorb more neutrons than can be compensated for with boron dilution or control rods. Boron is a chemical we put in the reactor coolant system (water) that we can control the concentration of and thereby control the power the reactor produces.  

The fuel could be disassembled and have the fission products removed.  The remaining good fuel could then be used to create new fuel pellets and new fuel assemblies.  President Carter made sure that this would not occur by cancelling funding for fuel recycling centers back in the 1970's.
It is. We can recycle it and separate out some of the material such as plutonium and uranium. We use an electrochemical process where the spent fuel rods are placed in a molten salt bath and a voltage is applied. The voltage can be dialed in to pull out specific things such as plutonium, uranium, etc. Some of the stuff can be used to make new fuel, and some of the extra nasty stuff can be separated and put in long term storage, rather than all of the bulky waste in long term storage. South Korea and France do it. The US has regulations against it (concerned about proliferation).

Source: Did a year researching and working with this stuff in grad.
One of the things people are ignoring here is nuclear contamination from reprocessing creating a lot of low level radioactive waste. Sure, you can reprocess stuff. But the entire reprocessing plant, with all of its concrete, steel, and machinery will be low level radioactive waste at the end of its life cycle. I visited a facility the produced radioactive medicinal chemicals for a variety of uses and they were saying that in 30 years the majority of the facility would be disassembled, sealed in concrete and buried. So if you reprocess 100s of tons of high level radioactive waste into usable fuel you generate 1000s of tons of radioactive materials that still need to be disposed of. You can't get things clean without making something else dirty.
It technically is still usable to generate heat, but since the fuel rods are in solid form, the gaseous radioactive Xenon isotope that is produced gets stuck in the solid Uranium matrix and forms cavities that eventually cause the Uranium rod to ~~shatter apart~~ *be poisoned*. The way reactor designs are now, they can't accommodate a ~~shattered~~ *poisoned* rod, so they have to retire the rods early, before the Xenon has built up enough to cause any ~~breakage~~ *excess neutron absorption*. Unfortunately, that ends up retiring the fuel rods far before they have converted the majority of their theoretical internal energy into usable thermal energy.

If you had fuel in liquid form then the gaseous Xenon could just escape and be collected and contained until it's 5 day half-life is spent. ~~I suppose Uranium pellets in a liquid matrix could also avoid the Xenon problem, although the pellets would still break up into tiny shards over time so the vessel would need to be able to contain those tiny shards (not impossible, but not easy either).~~

edit: I got some details wrong about the shattering, see /u/uraniumkore answer below for corrections
This reminds me of how they just burn the waste gas at oil refineries. You see this huge yellow flame burning into the sky 24 hours a day, and it feels like “why are you wasting all that energy?  Couldn’t you put that to use?”  Obviously not ai guess or they would...
I heard Pablos Holman speak about how his company invented a power pant that runs off of nuclear waste, but that the main obstacle in making this technology mainstream was actually getting a licence to build a nuclear power plant. It was pretty interesting-- http://www.intellectualventureslab.com/about/relationships/terrapower
It is still usable. Unfortunately, to use it you need a modern reactor or a reprocessing plant. Since uranium is dirt cheap it's more cost effective to just buy new uranium and burn it in the same old reactors we had during the cold war.  
The radioactivity is the side effect. What powers our nuclear power plants is splitting atoms and then letting that effect do a chainreaction. This process gives us heat, and we use the heat to produce steam and drive turbines. 

The fact of the matter is that all radiation is release of energy and it absolutely can be harnessed; however, in most cases cost outsrip utility. Think of using tides to generate electricity; the entirety of the vast oceans rises and falls all the time and using the tide to float a big weight up and let it drop during low tide to produce electricity sounds simple. In fact it is simple, but a system that is rugged enough to last on the coast/in the ocean and massive enough to produce meaningful amounts of electricity is not cost effective. Similarly trying to collect small amounts of gamma, beta, and alpha radiation just isn't going to be worth the effort with current tech.

This isn't a fair comparison, tidal generation is limited by the physics of lifting something less dense than water the the height delta of the tide and you can quickly figure out just how much energy you can get and how much the contraption is going to weight and see that wind/solar is the better option. If some creative approach comes up to convert beta and alpha particles directly to electricity on a large scale with suitable efficiency, then we may see waste barrels being rolled out to make power again.

Waste can be reprocessed to refine out the non-waste components for further use in standard reactor designs. For anti-proliferation treaty reasons, the US doesn't do this, but France does and they reprocess fuel for most of the world (this is actually a service offered to Iran and North Korea as incentive to not do it themselves to reduce proliferation).
It is usable! We have lots of technology and research dedicated to reprocessing and reusing nuclear waste. Unfortunately in reprocessing, the isotope of plutonium used to make bombs is produced, which is why several countries with nuclear power have laws to not reprocess waste so they can rest assured their neighbors aren't trying to sneakily make bombs. So because of politics and the historical evidence that none of us play nice with nukes, we just waste all of the recyclable material from a nuclear plant and bury it in some casket for an archealogist intern to discover 3,000 years from now.
Usually because it doesn’t generate enough energy to be usable. Nuclear reactors usually work by using radioactive material to heat water into steam so it Can turn turbines in generathe electricity. When the radioactive decay decreases due to the half life it can no longer do this and as a result becomes radioactive waste.

Energy could be extracted using thermocouples, which generate electricity due to heat differential, but not much.
because we havent figured out to harness that kind of energy as energy like we do with stray electrons from the processes we use to generate electricity. 

most people dont realize, nuclear power is still just basically steam power - we use heat from the nuclear reaction to boil water, producing steam which spins turbines consisting of central magnets being spun inside of a container made out of basically copper wire, which generates stray electrons creating electricity, some other things happen to increase the power of that electricity, which is then distributed to a sub station, or other storage, and then distributed across the power grid. 

TL/DR: We still haven't figured out how to harness actual energy beyond just simplistic electron transfer. Its also why Coal and oil are still the backbone of electricity world wide. 

Edit: Just to clarify some things
Nuclear reactors operate at criticality.  This means the reaction is producing a constant amount of energy and will not accelerate or decelerate to a different energy level without a change to the environment. Criticality is impacted by several elements including the geometry of the reactive material, and the reactive material itself. The material produces energy through fission and as the material fissles, it decays into isotopes which are no longer critical to the system. The isotopes are very likely still radioactive but they are no longer useful and must be removed.
I recently read that in the UK or somewhere they've been working on making batteries with nuclear waste.

They essentially encase some amount of waste in diamond (may be silicon diamond), and can draw a charge from it while blocking the radioactivity. The battery can last hundreds of years potentially, and is self charging due to the continued decay.
The vast majority of the the used fuel can be reused, and other countries, such as France, do; however, Jimmy Carter killed off the commercial reprocessing facilities in the US in the 70s.  He did it for two reasons: 

1. There was fear that the fuel could be stolen and repurposed for weapons.  

2. He thought it would reduce the likelihood that other countries would build reprocessing facilities, which would increase the availability uranium for foreign weapons.

In short, his policy failed, and we have tons of unnecessary nuclear waste to show for it.

Source: James Mahaffey's book Atomic Accidents (awesome book!)
As I understand it the vast majority of this waste is not hideously glowing rods, but stuff like gloves and one-use overalls used by staff in nuclear plants. Most of it is about as radio-active as your average banana (or less) but the protocols simply demand it is all treated as nuclear waste.
It's radioactive enough to kill you but not enough to generate enough heat to power steam turbines.

It could be reprocessed but that same technology is useful for enrichment into nuclear weapons grade material so it's development has been blocked.
Well, there is a thing called "reprocessing" which is only rarely done because of fears of "nuclear weapons proliferation" and opposition to "nuclear power" at all.  

Strictly **most of the uranium or plutonium remaining in the "waste" from nuclear reactors is NOT used and is left in the waste**.  These can be extracted by "reprocessing".  Additionally, many fission fragments have economic value as radioisotopes and could be extracted with reprocessing as well.  The volume of nuclear waste that actually needed to be "disposed" could be radically reduced.

Only the fissile uranium and plutonium can be "used" to generate energy but it needs to be fairly pure.  Hence if left in waste, it isn't pure enough.

Opposition to reprocessing is a major **political** point of the anti-nuclear movement - anti-nuclear proponents don't want to reduce the volume of nuclear waste because it would make using nuclear power more attractive. 

It is also a major **political** point of anti-proliferation - though that has not been nearly as effective as a deterrent (Pakistan got it fissile material from China and China was intimately involved in the Pakistani nuclear weapons program). 

As it is, there are two nations to do reprocess nuclear waste: France and Japan.  So currently reprocessing goes on despite anti-nuclear/anti-proliferation opposition and the logistic supply chains are in many ways far more dangerous than if major nuclear nations simply had their own reprocessing and breeder programs.

Much of the opposition to nuclear power is emotional rather than objectively rational.  An additional "real" problem is that reactor designs are stuck in the 1950s and have not innovated like many other things like, say, transistors.  Safer reactors could easily be designed (and have been designed) but emotional opposition prevents new designs from being adopted **in the United States and Western countries**.

This is why most nuclear reactor and ecosystems are primarily being innovated in both India and China rather than in the West.  Perhaps once these nations have created new reactors and reprocessing systems they will *allow* Western countries to buy *their technology*.
There's a pretty good documentary on Amazon Prime called Uranium, it's a PBS production. Really helped me understand it better. It's because U235 is the only usable element due to its unique number of protons and neutrons which causes it to split more easily when disturbed. As it decays, it becomes different elements, eventually becoming lead. That takes more years than any of us will live to occur.
It could be with processing and using different Reactors. Current reactors mostly are PWR(Pressurised water reactor). they aren't very good at getting all the energy out, in fact like only 5% the est remains behind as waste.

With different reactor type like integral fast reactor the waste can on-site be processed with new material and used again. In this cycle you ill use up to 90% of the fuel (eg 5G% vs 90%). It's a huge difference. Such a test reactor was actually running for 2 decades in US but to scale and commercialize serious investments would be need and sadly they are mostly going into inefficient renewables.

Note that the much better use of material leaves behind a lot less waste. That waste is also radioactive for much, much shorter time, like 300 years. But of course it's much more radioactive in that time (much shorter half-life).

Still IFR could solve a lot of clean energy problems, the reactor could burn the waste from current reactors (after processing) and the reactor could also be made to work with thorium which is much more abundant than uranium. This would give us power for the next 100K years easily.
It's tougher to manage the heat production in older fuel rods. The process of burning the fuel produces waste products that poison the fuel. The effect is to cause the heat output to oscillate between not much heat to too much at a given power setting.

You can however take old fuel and run it through a purification process to filter out the waste elements and you'll have good as new fuel rods.  

Problem is the anti-nuke crowd have put up so many obstacles it's not worth doing. So the waste just goes to waste.
there are several types of "nuclear waste", but you've meant "depleted nuclear fuel" probably.

It it was a uranium-235 thermal neutrons reactor, it consists of

1) unused "fuel" (e.g. uranium-235) which can be reused for new fuel,

2) leftover uranium-238, which can be reused for production of Pu-239,

3) plutonium-239 which can be used for new fuel or glorious nuclear weapons,

4) other products of U-235 fission, some of which are higly radioactive, making the depleted fuel higly radioactive and hard to handle, most of them unsuitable for forced(induced) fission, and thus mostly useless.

5) products of non-fuel materials catching neutrons, most of which are also radioactive and useless.

So yea, #4 and #5 are mostly useless - they are radioactive enough to be dangerous (and for many-many years), but are not really heating much (well, after several years anyway).

And if there is a useful element in there, it is hard to separate it from the other ones, because, again, the compound is higly radioactive.
Ok lets get real.

they are usable, and you can actually do some safe stuff with them but they are not economically viable.

A lot of scientific methods exist beyond what you see in stores, industries.

most are economically not viable.

back to your first question: it is cheaper to store Nuclear waste than try to use it for something else.

Find a cheaper method than storing it underground and you may get rich...
Most of it is usable, if we would start building Thorium reactors, which use high level nuclear waste to drive the reactions that provide power.  During this process, the high level nuclear waste is destroyed, and different types of waste are generated with much shorter half lives.
Simply put, most reactors today use so called slow, or thermic neutrons to split the nuclei, but those can't split the nuclei of the waste. If you use fast moving neutrons, they also split the waste. The problem is, that a reactor with fast moving neutrons, is very different to the "standard" reactors, that are currently used. They use water for cooling, but water in the reactor will slow down the neutrons. Tested designs with fast neutrons used Sodium for cooling, but they had problems with attrition because of that, and were generally very expensive. The Experimental Breeder Reactor II was one of those.

To use new Uranium is simply cheaper, so it is not commonly used.
These fast-neutron reactors would be good to get rid of the waste, though, if it can be made affordable.

Edit: spelling
https://youtu.be/-Nc0wCrkk00

This is a neat old documentary about how an atomic bomb works but it goes into fission and fusion of atoms as well. It also mentions how decay and radioactivity occurs. Worth a watch and it will give you a better understanding of the subject because it is broken down in a very understandable way. Hope this helps.
It is usable in several ways.  To understand why it's not in the plants you have to understand what the plants we have today use.  Most commercial reactors use uranium enriched to about 5% u235.  Once the amount of u235 falls below some level the reactor would no longer be critical (capable of creating a self sustaining reaction).  We there fore take those spent fuel rods and place them in a storage pool and put in new ones.  As for the spent fuel what can we do with it?  Well three major ways it can be used:
 
 
1.) In a radioisotope generator.  Here we just want the heat it produces.  Not super useful and not really the most practical way of building one, but we could if we wanted.
 
 
2.) In a breeder reactor.  Here we create the fuel we want.  To get into this you kinda need to understand fission a bit.  So fission broadly speaking happens when a neutron hits a heavy atom and the atom splits.  All isotopes have a cross section for which they can absorb neutrons and how likely they are to do so.  U235 fission happens when a thermalized neutron (a slow one caused by a fast neutron hitting something that moderates it's speed like water or graphite) hits a u235 atom which then splits.  A slow breeder uses slow neutrons to turn thorium into u233 which then is the critical reaction.  Otoh a fast breeder uses u238 (which is the most common uranium and most of what will be left in spent fuel) to create plutonium, which then runs the reaction.  This let's us burn up pretty much all of the fuel with no really long lasting waste, but the concern is it produces plutonium which can be used for bombs.  Contrast this with a uranium atomic bomb where you need highly enriched u235 and it's much easier to get and much hard we to check people aren't doing this.
 

 
3.) We can reprocess the fuel to get what's called mixed oxide fuel (Mox) which we can then burn in reactors, but again we start talking about dealing with plutonium and proliferation fears.
The decay itself isn't how you extract energy, however strong radioactivity tends to correlate with the presence of unstable isotopes that are fissile.

For a large fraction of the waste considered highly radioactive: politics is the reason. Basically it's a fear that the same tech that will "enrich" the waste by concentrating fissile materials could hypothetically also be used to concentrate material to make a weapon.

As far as true "spent" waste, half-life is inversely proportional to how dangerous it is. So when you see "RADIOACTIVE FOR 10,000 YEARS" in scare quotes what that really means is the material decays slowly, which means it's emitting radioactive particles very slowly, which means the danger is very low. (As opposed so something highly radioactive with a half life of months or a few years spraying out a lot of particles very quickly and going inert similarly fast.
Side note but reading all these comments brings up what a shame it is we haven't gone even further infrastructure wise (while waiting on fusion) because of politics, military-industrial-complex (why we didn't go thorium) and just general negative & "scary" perceptions about nuclear energy. 
I wrote a paper a few years about about how the nuclear waste on the US could actually still be used further. If I remember correctly, we only extract 5%-ish  of the energy and further use would be efficient. However, using the remaining energy would turn the waste into a refined nuclear product that could be used in nuclear weaponry. And people don't like the idea of having that hanging around.
The waste from Uranium and Plutonium reactors CAN be used as fuel by Thorium reactors, which ultimately leave orders of magnitude less waste, and with shorter half-lives.

I don't understand why they're not being more actively pursued.
It doesn't get more unstable, on the opposite it's trying to come back to stability.
Radioactive material are unstable and can be made even more unstable for civil (and even more for military) use. Then it's put into uranium pills, in zirconium tube and a bunch of those makes a reactor.
The matter getting out of the radioactive material heats water that is used to produce energy.
But with time, it's trying to get back to some stability, gets less unstable, and generate less heat thus becoming useless or at least not effective enough in the process of heat generation.

But here's the trick, the more unstable means further from stable state and more radioactivity. Which is something going out so reducing the instability. And the closer you get from the stability, the less radioactive is the material,until it's not enough to be used but still a concern for health. But less radioactivity means less going out, so it decreases slower and slower for a very long useless life.

TL;DR : So it decreases pretty quick in the beginning/more potent phase, and then you'll have a very long time where it's not potent/heating enough to be used but still a concern for health.
Well it kind of depends how you class the waste, and if your pedantic if it can be used is it really waste?

For example a lot of space probes use radio active materials to generate heat which is used to generate electricity that runs the probe. It doesn't generate that much but that's mostly due to the material having to be rationed due to limited supply. But the material is still something that has to be seal away and stored, but because it has a use its not really waste.

When you are looking at if some thing is usable you are looking at the number of decays per second which is inverse to half life. The more decays per second the more dangerous it is but it also spits out more raw energy if it can be harnessed. But also note it also reduces the operational life span which is good and bad for different uses.  

If you wanted to just gather all radioactive waste shoved it in a seal pool and just collect the heat produced its possible you could design a system to do that but it would just be really expensive and hard to make/maintain. Its just easier to just contain it. 
It is however it's not that cost-effective right now which is the main issue. Large advances have been made in this field though and now we can create "batteries" that work from this. Often very useful for things like long range satellites sent into deep space.
Imagine that the fuel in your car produced thick, incombustible oil. Sure you could process it into something usable but that might take more energy than you get out, and in the real example it's still dangerous to be around.

Tl;dr not all radioactive material is created the same 
Think about it this way, Uranium is made of the Radioactive stuff and the Not Radioactive stuff. When the Radioactive stuff decays into Not Radioactive stuff it releases energy and when the energy hits another bit of Radioactive stuff it decays into not Radioactive stuff releasing more energy. but when it hits a not radioactive stuff it just gets absorbed. It usually happens very slowly but we can speed it up by hitting it with energy.

From there you can see the problem enriched uranium has a lot of Radioactive stuff so we can make it suddenly explode with energy. Of course the energy doesn't hit all the radioactive stuff, so there is still radioactive stuff decaying slowly. But what if you try giving it more energy to use the remaining radioactive stuff. but after giving it energy the first time it has turned mostly not radioactive stuff so the energy will just get absorbed by the not radioactive stuff. Releasing a negligible amount of energy. However remember the Radioactive stuff that is in there does decay slowly and still emits harmful radiation when it gets into your body. 
Short answer is that it is fairly easily used but the process of enriching the waste into fuel to be reused is so similar to the process of making nukes that we don't do it in the US. They do enrich nuclear waste to be reused as fuel in some Nations. France is one I remember reading about when I was learning about this stuff in University a few years ago.

This is also where a lot of the conflict about Iran being a nuclear power comes from. Iirc They enrich their spent nuclear waste to be reused in reactors but because the same facilities that do that could be so easily re-purposed to make nuclear bombs the American and neighboring governments do not like that Iran has that ability. Even if all they really want is nuclear energy which is arguably much better for the climate than coal or oil fueled power plants.
I'm glad to see love for MSRs. Really excited about TWRs (Isn't there a Bill Gates funded initiative for a good TWR design?). I scrolled all the way down hoping to find someone reference the LIFE engine project at Lawrence Livermore. I think after reprocessing the LIFE engine would likely be the best option in the next twenty years.
It only produces a little bit of the heat that it is used for, so it can be used for things that require a small amount of heat. Power generation requires a large amount of heat transfer to fully boil water, nuclear waste does not generate enough heat to spin a turbine that produces electricity.
You can reprocess the waste, but the various fission products make it expensive and difficult, newer designs that use molten salts as fuels can do so much more easily, you convert the "waste" (mostly unburnt fuel) into a salt form and add it to the reactor fuel mix.
Could nuclear waste in theory be used over and over again through various methods, until the waste is all but reduced or all its radioactive properties?

I’m a total laymen when it comes to this so please excuse my ignorance in the following example.

Say a modern nuclear power plant produces nuclear waste equal to 100% potency.

Now we develop some method for using that waste as a source for...Something. Idk, maybe another power source? Something else that’s usable in a positive manner?

Now the lineal waste is reduced to say, 50%? 

Continuing on we develop yet another method for extracting something useful from the new half potent nuclear waste, resulting in lineal waste equal to 30% of the original potency.

I mean, is any of this possible, in theory at least?

I apologize ahead of time to those that actually know what they’re talking about, as I’m sure the above is very cringe worthy. 
It _is_ usable.  We'd just have to clean out the fission poisons - the subset of fission products that have high neutron absorption cross-sections.  They call that "reprocessing".  France does a reasonably good job of it.  The US does not because the right don't care and the left are paranoid about anything nuclear.
As my lecturer once put it, the difference between a poison and a cure it often the dose. Something that mildly reactive so to speak, just isn't reactive enough to be useful for industry, but still potentially lethal to human life
One thing no one mentioned. If the waste has high energy and is dangerous to people and nature then its half life is very short, months, maybe years, not thousands of them. If half life is long, then the energy levels are low and there is no real damage.
Because fissionable material inside fuel rods starts to split and crack, which can damage the casing of the fuel rods and cause harmful leaking of radioactive materials into spaces where people work. Only 1-2% of the nuclear fuel is burned before the rods are changed for saftey reasons. We need LFTR bad.
It IS still useful, but as u/restricteddata said, it is not useful in the state the radioactive waste is in.
We CAN recycle it into useable fuel, but there are a lot of laws that restrict the whole process of transporting and recycling the radioactive material.

The reason those politics are there have to do with: 
1) public fear of radioactive meltdown (like Chernobyl or what *unlikely* could've happened at 3-mile Island).  There IS technology that allows for much less risky nuclear power plants, but there are more politics that prevent quick implementation of these because...
2) Lots of lobbying by coal/oil magnates to keep cleaner and cheaper power sources down.
What's interesting is that most of the waste (~95%) can be recycled into new fuel rods. In the US we don't because it's illegal for some reason. It can't be used as is because they are ceramic and the process of transmutation causes it to crack and release gases. Other countries like France will actually recycle them and they end up with much less waste to manage. Logic in the industry ended decades ago. It's kind of a shame considering it's 100% zero emissions and millions of times more energy dense than fossil fuel. 
There are many different types of “nuclear waste”. Some is spent fuel that is no longer hot enough to be used for energy, but too dangerous to leave around. Some can be reused, especially if it is reprocessed or used in a different type of reactor, but that is not usually economical or politically viable. The remainder — possibly the majority — of what we call “radioactive waste” is equipment or materials that has radiological particles on it, making it too dangerous to use or recycle. 

The short answer is: some could be, but economics or politics get in the way; the rest is just dangerous and useless.
Actually you can, it just depends on the processes and reactions done. Most reactors were made to make weapons grade materials for nuclear weapons back in the cold war.

There are reactors designed to use certain waste and recycle old material.
We can and we should have transitioned from coal to nuclear to renewables, but we skipped nuclear due to cold war, and fear mongering so we were stuck with dirty poisonous coal for many more decades than we should have have been. Many countries tried to go nuclear and deal with waste and using it for more than weapons grade waste. 
I actually did a class on nuclear power with a final project that was an original design. I did mine on a radioisotope thermal generator for a gokart. It did not end well, weighing tons for such a small output. 

The best option for using waste as a power source would probably be for small devices where solar is unavailable.
You could concentrate it and have a potent heat source. The problem is, in order to get some useful energy (steam and then electricity for example) it has to operate at a high temperature despite something (water for example) constantly coming in to take the heat away. This alone is no problem but in case of malfunction (no new water comes in to take the heat) your heat source can't be turned off. It will heat only itself with all it's power. If it was capable of creating enough heat to power your generator it is easily capable of explosively evaporating itself in such a situation. 
So, immunotherapy has long been seen as a holy grail for cancer treatment. The immune system is naturally programmed to attack cells that have gone a bit weird (to use the scientific term). The problem tends to be that the cancer cells can also alter themselves so that they are disguised from the immune system, or in fact inhibit any immune cells that come into contact with them. This stops the immune system from seeing them as dangerous, allowing the cancer to grow. So the balance of the immune response is in favour of leaving the cancer alone.

What this treatment does is inject the tumour with molecules that tell the immune cells in the vicinity of the tumour to wake up and start doing their job, overcoming the inhibition that the cancer cells have put in place. This means that the balance of the immune response is now to attack the tumour, which seems to work very well.

The really cool thing is that now that the immune system is trained to see the tumour as bad, and will attack similar cells in different sites. This is why it behaves in some fashion like a vaccine.

It's perfectly viable, and very exciting. As always, there is always the question of how well it translates into human biology but it is still very promising. I think one problem is going to be how specific the immune response is. In the paper, they see the immune cells are trained to attack cells with protein markers unique to the tumour cells, which is a good sign. One concern might be that if you accidentally trigger the immune response to normal cell markers, it could cause your immune system to attack healthy cells which would obviously be a very bad consequence. Another would be how readily a tumour can evolve to overcome the immune system attack. If the immune system only ends up going for certain markers, it could miss tumour cells that don't have the same ones. These could then continue to grow and cause the cancer to return.

ETA: thank you kind, golden stranger!

...strangers!
I work in this field. 

This type of cancer vaccine is kinda old news. What's really cutting edge are the personalized cancer vaccines based on mRNA that are in clinical trails in humans **already**. Basically, these vaccines are tailored to an individual's cancer. You see, your cancer and my cancer may have the same name, lymphoma for example, but they may be very different in terms of their susceptibility to drugs and especially to vaccines. Using personal genomics to tailor mRNA to program your own cells to make a vaccine against your very unique form of cancer is the real key to all of this. Adding on top of this is that mRNA is just a superior way to produce vaccines. As it turns out, when your body makes the vaccine inside its own cells, it stimulates your immune system more effectively than a vaccine made in a factory and injected (this has to do with MHC1 and MHC2 activation). 

These clinical trials are looking promising. If they work, this can be combined with checkpoint inhibitors and other mRNA therapies (that also boost immune response against cancer) and it's bye-bye cancer. 

Let's just say that it's the most promising thing I've seen in cancer therapy ever. 
There is a treatment for stage 4 brain cancer that does this indirectly. Duke has modified the polio virus to attack cancer cells, and they inject it into the center of a brain tumor. While the polio virus is attacking the cells, the immune system starts going in to clean up the mess.  In turn, the immune system learns to recognize the cancer cells.  The whole process seems to last for months after just the one single injection.

You can google PVS-RIPO for further information.  The first few patients were treated in 2012-2013 and some are still alive without any further treatment. Those patients had recurrent Glioblastoma Multiforme, which is perhaps the deadliest cancer situation that one can imagine.

While this would technically be considered virotherapy, the immune system plays a huge role in the outcome.

They are now in phase 2 trials, and still only for brain tumors, but I believe the assumption is that it could be widely used among many different solid tumors in the future.
It's actually a really cool concept that frankesteins together mechanisms of action that we already use.

1. Making cancer cells visible to the immune system: PD-1 and PD-L1 inhibitors

2. Siccing T-cells on cancer: CAR-T cells, Allogenous stem cell transplantation, BiTE immunotherapy

They then do this only locally, circumventing the problems that would arise if we did it systemically, i.e. death from immune system overdrive.

I'm interested in seeing how well it works in humans and reading about it made me slightly nervous about my job to be honest
So I worked on a project and helped publish a paper that also showed this exact effect (even showing it attacked tumors in other locations) several years ago (PMID: 25179345).

One of the biggest issues with directly translating this to therapy is that directly injecting a vaccine into a tumor cell is not necessarily a wise choice and not commonly done (I think, my clinical exposure to oncology isn't large). The main fear is something called "tumor seeding", which can also happen when they aspirate or biopsy, or do something otherwise invasive with a tumor that could cause cancer cells to dislodge and make it to the blood or lymphatic systems. If the tumor is well-demarcated (ie. enclosed), you don't want to open up a channel to your blood supply for cancer cells to spread throughout your body (hematogenous spread). Clinically the chance of this is probably very low and this is a consideration when physicians consider how to biopsy a tumor, but if you start making it standard procedure, who knows what could happen. 

Even if the cancer is metastatic, it is possible that it's in a compartment that is very well isolated from the rest of your body. For example when testicular cancer is suspected, a biopsy is almost never performed and the suspected testicle is straight up removed because it's in a very well-enclosed portion of the body (referring not to the skin scrotum, but another membrane separating it from the inside of your body). 
Paper is available in its entirety here: 
[**Eradication of spontaneous malignancy by local immunotherapy**
 Idit Sagiv-Barfi, Debra K. Czerwinski, Shoshana Levy, Israt S. Alam, Aaron T. Mayer, Sanjiv S. Gambhir, Ronald Levy - *Science Translational Medicine*](http://stm.sciencemag.org/content/10/426/eaan4488)

Having read a bit of the responses, I have to ask: Given the cancer cells ability to "blend in", would this treatment possibly cause the body to attack it's healthy cells? If so, how would it be remedied and how often would this occur?
As scientists we're equally sceptical of new wondrous drugs, but I can tell you that immunotherapy is the most exciting development in cancer therapy in decades. Maybe the one thing to keep in mind is that it tends to work much better in certain cancers (e. g. melanoma) than others. Source: PhD in Molecular Biology working in cancer research
As a general rule, one should be very cautious about translating results from mice into humans.

A great example is avastin/bevacizumab (an anti-angiogenesis antibody).  When early studies of endostatin (an anti-angiogenesis molecule) performed very well, Time magazine printed a cover along the lines of "A Cure for Cancer".  

In retrospect, people in the field admit that it should have read "A Cure for Cancer in Mice."

There are many issues with mouse models, not least of which is that cell lines are often the source of the "tumours" (such that they differ dramatically from tumours that arise spontaneously in humans.

This study accounted for that somewhat.  They did some of their experiments with cancer cell lines injected into mice, and some with mice that spontaneously develop tumours.  Even the latter have to be taken with caution, though, as these are still derived from modified cells that express a specific tumour-inducing viral protein.  (All the cells in these mice have the gene for this protein and they develop breast tumours.)

Finally, immunotherapy has been an amazing step forward and, along with targeted therapies like monoclonal antibodies (which are used in immunotherapy) and small molecule kinase inhibitors, represents the biggest advance in the past 20 years.  

So the strategy is promising and it is aligned with what has been working over the past few years.  Too early to say how it will work in humans.

Here's an [article](http://fortune.com/2004/03/22/cancer-medicines-drugs-health/) on some previous stumbles.
Immune therapy isn't new. It works wonders in theory and for limited times has near-miraculous results in practice. Similar clinical trials have already been done, achieving similar results for a limited period of time. Problem is the mouse model in the paper is very simplistic -- a flaw for most (all?) tumor mouse models. Real stage IV nasty cancers that we have a hard time curing usually have genetic instability and *extreme population variance* that is somewhere between extremely difficult and impossible to simulate in a laboratory. Real advanced cancerous tumor cells tend to adapt to cancer "vaccines" and come roaring back in a few months, almost without fail. Because of that population variance. And the next time they are completely unresponsive to the vaccine. The population variance tends to overlap extensively with the variance of the normal body cells, which is why curing cancer *in general* seems to be nearly or actually impossible.

Calling them "vaccines" I think is a bit of marketing. They are not vaccines, since they are not meant to prevent cancer, unlike the HPV vaccine for example. It's marketing to get people to trust it a bit more, I think. I would call them immunotherapy, just because that's really what it is. But, in my cynical opinion, the reason why immunotherapies are getting hyped so much is because there's TONS of money in it. It's not creative, risky work so much anymore. Corporations or investigators just follow a known and predictable process to generate these antibodies. Build an custom antibody, patent it, and sell it. And it's all patentable, every new target, every new cancer is a unique invention. And treatments can cost hundreds of thousands of dollars for these cancer vaccines. That, in practice, yes, are almost certainly doomed to ultimately fail (won't cure the cancer). However, they are also very likely to reliably prolong survival, especially in cancers where there was nothing else you could do a few years ago to even budget survival a little bit. So, want to make a few billion dollars? Find a cancer for which no current practical therapies exist -- a nasty, lethal cancer that kills people all the time with few options -- make an immunotherapy to it. Show any kind of efficacy, and bam: Instant money. Is that the way we should be running medical research and making healthcare decisions? It doesn't matter, that's just how it's done. But, like I said, I'm probably cynical. 

Some people believe in this a lot and think it's the future. They know the problems but think if you just tweak it, it will work like gangbusters. That kind of tweaking is not what this article did, which would be a lot to ask. That's why it's buried in the middle of a pretty prestigious journal instead of the lead story for the most prestigious journals. Maybe the tweaks are possible someday and the therapy will work super consistent. Who knows, that's science. My personal and professional take is that the problems with immunotherapy are *probably* mostly intractable, that they've essentially already made almost all of the progress they are going to make, and that further advancements might help a few edge cases but will fall far short of a general cure for cancer.
Can I ask a follow up question? How long will it be until we get first results from the clinical trials?

This seems so promising, that I'd love to stay up to date on current progress, can anyone tell me where I should check?
TL;DR. Immunotherapy uses monoclonal antibodies which serves to block inhibitory pathways tumour cells might utilize to halt the immune response. This works best on tumours with high mutation rates, "hot" tumours, essentially making them appear as foreign to our immune system and illicit a response. Other tumours have low mutation rates, "cold" tumours, which appear almost identical to our own cells and therefore wouldn't traditionally trigger the immune response. Using antibodies inhibits the inhibitory pathway, effectively activating the immune response. This therapy, although promising, is very toxic and expensive. Altogether, this is a very promising avenue in cancer treatments moving forward


Cancer vaccines, a.k.a immunotherapy, is a remarkably vast and exciting field of oncology. In general, immunotherapy serves to educate our immune cells, namely our B and T adaptive lymphocytes, to recognize and kill tumour cells. Tumour cells are smart and will develop mutations to prevent them from being recognized and consequently killed by our immune cells. Tumour cells have learned to up-regulate specific proteins, called checkpoint inhibitors. Checkpoint inhibitors are normally expressed on immune regulatory cells, which when bound to ligands on activated CD4 and CD8 cells, halt their activation and "shut" them down. Tumour cells expressing these checkpoint inhibitors, namely PD-1, can effectively inhibit the attack from CD8 cells that may recognize the tumour as foreign. Tumour cells also can secrete various cytokines in the microenvironment, such as TGF-B and IL-10, which can also inhibit the activation of CD4 and CD8 cells, as well as boosting the activity of regulatory T cells which can exert similar effects.

Immunotherapy works by using humanized monoclonal antibodies that when bound to checkpoint inhibitor proteins prevent their binding to its complementary ligand. This means you are inhibiting the inhibitory pathway, which ultimately leads to an activation of CD4 and CD8 T cells that recognize the tumour as foreign. Now, that is the major caveat of immunotherapy. The CD4 and CD8 T cells MUST recognize the cancer as foreign. "Hot" tumours are tumours with high mutation rates and therefore generate neo-antigens quite frequently. A model "hot" tumour is melanoma, whose high mutation rates are as a result of constant exposure to harmful UV radiation from the sun directly. Tumours that generate new antigens frequently increase the likelihood our CD4 and CD8 cells will have receptors that recognize this as foreign. "Cold" tumours on the other hand have low somatic mutation rates and do not generate neo-antigens as frequent as their "hot" counterparts. Therefore, our CD4 and CD8 cells are much less likely to recognize the tumour cells as foreign and the tumour persists. Keep in mind, tumours cells are just really messed up versions of our own cells. We have mechanisms to prevent our immune cells from attacking our own cells (autoimmunity). Cancer cells with low mutation rates look identical to our own cells, which would not typically illicit an immune response.

Another avenue recently taken is the use of oncolytic viruses as part coupled with monoclonal antibody therapy. Oncolytic viruses prefer to infect tumour cells as tumour cells represent the perfect host; they do not undergo programmed cell death, they do not alert the immune system and most importantly, are always dividing and replicating their DNA which the virus needs to survive. Oncolytic viruses, even with broad tropisms, will preferably infect cancer cells over normal healthy cells. When the virus completes a round of replication, it will lyse the cancer cells, releasing internal antigens that can stimulate the adaptive immune response, as well as danger molecules such as ATP. Danger molecules like ATP are powerful inducers of the immune response and can very quickly establish local inflammation in the tumour microenvironment, which is necessary to allow trafficking of the appropriate immune cells. Danger molecules essentially are the ticket for the immune cells to get into the tumour site. Monoclonal antibodies can remove any checkpoint inhibition the activated lymphocytes may encounter to kill the tumour cells. This has been shown to work extremely well, and even better, offers excellent chances for long term remission (memory effect of the immune system).

Unfortunately, monoclonal antibody therapies act non-specifically and therefore can be incredibly toxic. Almost anyone that is enlisted on monoclonal antibody regimens for immunotherapy experience some adverse side effects. Unfortunately many must pull off treatment altogether. Also, there is a major cost issue especially in countries with private health care. Ipilimumab, a monoclonal antibody that blocks the CTLA-4/B-7 interaction, costs around $700 000 annually. Unfortunately, most people just can not afford this treatment even though it works.

In my opinion, this is the poster child of current cancer treatments. It is one of the only treatment regimens that attempts to "personalize" the treatment as much as possible and its ability to offer long term remission is extremely promising.
Cancer vaccines are totally a real thing. Well, they're actually a couple different real things.

The first is vaccines like Gardasil (human papillomavirus), where the vaccine is **actually against a virus that causes cancer**. These are pretty normal vaccines. 

However, there is also a concept of cancer vaccines against "neo-antigens." As cancer cells accumulate mutations, they begin to display mutant proteins on their cell surface. If we can identify these proteins, and create a platform to command cells to create antibody responses against them, we can program the immune system to seek out cancer and destroy it. This idea has worked in a variety of different animal models of cancer, from a variety of different groups, in a variety of different ways, which convinces me that the underlying technology is valid, and it's a question of when, not if, we see clinical advancement. 

Chimeric antigen receptor T cell therapy (CAR-T cell therapy) is a closely related technology where the T-cell programming is done outside the body rather than with a vaccine. It's very promising and even has an FDA approval at this point (Gilead/Kite's ~~Keytruda~~ Yescarta). We're talking cancer *cures* for cancers that were previously universally near fatal. Side effects and especially runaway T-cell proliferation can still be a problem. 

Watch this space. Scientists are seriously starting to crack this nut wide open, between new immunotherapy regimes (targeting PD-L1 and/or CTLA-4), programmed CAR-T cell therapy, and cancer vaccines. We're starting to understand how traditional cytotoxic chemotherapy drugs may work in conjuction with immuno-therapies, and we're starting to understand the interplay between these systems previously thought to be disconnected. We really are on the cusp of a real revolution, and I'm usually an intractable cynic about most cancer therapies. 

Edit: got a brand name wrong. It's Yescarta. Keytruda is a monoclonal antibody immunotherapy targeting PD-L1. Five years out, it's 8x more patients are alive and cancer free (40%) than the previous treatment (5%). 
There are many very promising developments, but one must remember that cancer isn't a single disease, it's a whole group of related diseases, which are very different. So, it might work on some types, but not others. Also, from doing it in a lab until its a useable tool in clinical use might be decades.

As for the vaccine effect, don't forget that cancer is based on random mutations, so there is no guarantee that "defense knowledge" from one immune system is transferable to another patient, with a slightly different "enemy".

Don't expect a silver bullet for all cancers. I think a more reasonable expectation is that it (or something like it) will become another tool in the existing toolbox for fighting cancer, and used in combination with other tools, much like how we today mix surgery, different kinds of chemo and radiation therapy in different ways, depending on the exact nature of each case.

As much as I would like to see a silver bullet, I don't think it's realistic to expect. On the bright side, though, we are getting much better at treating, and the advances are done at an amazing pace.
Veterinarians have been using cancer vaccines for a while. The melanoma vaccine has about a 50% success rate which is pretty good considering how melanoma is so difficult to treat. There is also a fibrosarcoma and B cell lymphoma vaccine. As far as I know those vaccines are less effective. Fibrosarcoma in cats is extremely aggressive while lymphoma is more manageable with chemo. The vaccines are all made by Merial. UPenn Vet and The Children’s hospital in Philadelphia have had some instances of curing children’s leukemia with immunotherapy but those cases were experimental. 
To make a broad analogy:

Picture your car for a moment. Immunooncology broadly speaking unlocks parts of the immune system that keep the body from attacking itself. In our analogy immunooncology released the parking brake and the car is in neutral. Certain combination treatments are the equivalent of giving a push.

What happens next depends on how your car is parked. "Curing" Murine models of cancer are the relative equivalent to parking on a steep hill and popping bottles of champagne when the car rolls downhill. It's dishonest at best to promote the results that way and a bit laughable, but academia does it periodically to stir public interest (and by proxy send public funding their way).

To continue the analogy most human cancers are the equivalent of parking on a level surface at the base of a hill. Unlocking the parking brake is important, but it's not usually going to send you uphill. Sometimes you get lucky and the metaphorical car wants to move, meaning you only need to give a small push to get the car unstuck and it climbs under it's own power (a so-called "hot" tumor) but more often than not you're talking about pushing a car uphill by hand, which isn't terribly effective.
Have we seen similar successful results in test on mice etc, that sadly haven't panned out successfully for humans in the end in the past? 
Just wondering if this is the first time we've seen such a positive success,  or if we should still be somewhat apprehensive? 
[removed]
[removed]
OK I've read the paper and it's very promising, but here are some caveats:

-nearly all of the tumors were treated at a very small size (a completely standard practice in the field). It's unclear how well this would work in a patient with heavy tumor burden.

-all of the injected tumors were at the surface and readily "injectable". It's not clear if this would translate well in a situation where all tumors were internal.

-all the recent successful immunotherapies don't work as well in tumors with a low mutation burden (which is most solid tumors), because fewer mutant proteins = fewer immune targets = lower immune stimulation. The models used here are fairly immune-stimulating, so may be a bit overestimating. That being said, the broad results are still unprecedented and exciting.
A lot of my published research was on Cancer therapeutics, so this topic is something I know quite a lot about. "Killing cancer cells" is not a hard process. We have been able to do this for a long time. The issue comes with therapeutics want to in some way kill the cancer cells, keep killing them, all while not hurting the person. The last part is the hard part, because we really haven't found a good way to do that yet. Most cancer treatments end up doing more harm to the person. I don't want to dampen any hopes on on this, but we hear about "the cancer vaccine" or that we can finally "kill cancer cells with a drug" and then we hear nothing about it in the future. That is because we can't find a way to safely administer these treatments since they always end up doing damage to the point of not being useful. Also, even if we do find something that works in the lab, it may very well not work as well with humans. Going from successful lab research to a treatment that can be used for the public can take an extremely long time, and for good reason. They want to make sure they aren't shotgunning out a therapeutic that will end up harming most that take it. With all of that said, I am very optimistic on where we are regarding cancer research. While we aren't quite there yet contrary to what these recent articles may say, we are still somewhat close. The problem is that we have been somewhat close for a while, but getting us over that last little edge is the hardest part. 
The common cold is actually a collection of over 200 different viruses that cause similar and typically minor symptoms. It's a pretty significant undertaking to try to develop vaccinations against all of them, and their eventual genetic divergences.

It's not that difficult to cherry-pick a specific virus out of the pile and develop a vaccine against that one, unless the virus mutates rapidly.

If you'd like to read more about the common cold, [here](https://www.nih.gov/news-events/nih-research-matters/understanding-common-cold-virus) is some further reading.

Edit:

I'm getting a lot of similar questions. Instead of answering them individually, I'll answer the more common ones here.

**Q: 200? I thought there were only 3 or 4 viruses that cause colds?** A: Rhinoviruses, Coronaviruses, Paramyxoviruses are the families of viruses that make up the vast majority of colds, about 70%-80%. It's key to understand that these are families of viruses, not individual viruses. [Around 160](https://en.wikipedia.org/wiki/Rhinovirus) of those 200 are Rhinoviruses.

**Q: Does influenza cause colds?** A: No, we call that *the flu*.

**Q: Can bacteria cause a cold?** A: No, not really. [Rarely](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC104573/), a bacterial infection will be called a cold from the symptoms produced.

**Q: Does this mean I can only catch 200 colds?** No. Not all immunizations last forever. See [this](https://www.ncbi.nlm.nih.gov/books/NBK27158/) paper on the subject if you'd like to know more. /u/PM_THAT_EMPATHY outlined some details that my generalization didn't cover in [this](https://www.reddit.com/r/askscience/comments/fq1rkq/if_the_common_cold_is_a_type_of_coronavirus_and/flqfiue) comment.

**Q: Does SARS-COV-2 mutate rapidly?** A: It mutates relatively slowly. See [this](https://www.reddit.com/r/askscience/comments/fq1rkq/if_the_common_cold_is_a_type_of_coronavirus_and/flqpxr3) comment by /u/cappnplanet for more information.

**Q: Will social distancing eliminate this or other viruses?** A: Social distancing is about slowing the spread so that the medical systems are not overwhelmed. It will not eliminate viruses, but it does seem to be [slowing other diseases](https://qz.com/1824020/social-distancing-slowing-not-only-covid-19-but-other-diseases-too/) as well.

/u/Bbrhuft pointed out an interesting caveat that may provide a challenge in developing a vaccination. Their [comment](https://www.reddit.com/r/askscience/comments/fq1rkq/if_the_common_cold_is_a_type_of_coronavirus_and/flu016h) is worth reviewing.
The "common cold" is not a single virus.  It's a term we use to describe a whole lot of different viruses, some of which are rhinoviruses, some are coronaviruses, and others too, all with varying degrees of danger to health and wellness.

Some of these viruses mutate frequently as well so we can't make one single vaccine that will work for every infectious virus.

The SARS-CoV-2 virus that causes COVID-19 is a SINGLE virus that has a relatively stable genome (doesn't mutate too much).  So we are all over this.  This virus was made for a vaccine.

edit: Thanks so much for the gold, kind strangers!
From a public health perspective, there are certain principles that are critical to justifying an immunisation program, which are roughly these:



- the germ needs to be common, easily spread;

- the germ needs to pose significant public threat;

- the vaccine needs to be sufficiently efficacious; 

- And overall: the benefits of the vaccine need to outweigh the costs, to the individual AND the community (monetary, side effects etc).



A justifiable vaccination program doesn't have to meet ALL of these criteria perfectly, but it does have to meet them to some degree. 



Rabies for example: relatively rare, but a huge threat to anyone who contracts it. Very efficacious vaccine if used correctly. The vaccine has side effects and may be costly, but it is of great benefit to the individual to avoid contracting the disease, and to the community to limit the incidence, and healthcare costs associated with managing the condition. 



Influenza: common, very easily spread. Causes significant mortality in certain vulnerable groups. The vaccine is typically not as efficacious as most others due to seasonal mutations and strains, but even if it does not prevent the flu it may reduce the severity. The vaccine is relatively cheap, and is well tolerated with few side effects.



Now, the common cold (which as others have said is not just one bug): very common, very easily spread. Mortality and morbidity? Most measured in productive time lost at a cost to individuals and society. Not a significant threat, rarely causes death even in vulnerable populations. Would a vaccine be efficacious? Well, there's so many bugs causing the cold and they change so often, it's very unlikely we could develop an effective vaccine. Risk vs Benefit? You might have noticed some people get a few cold-like symptoms after the flu vaccine - if the equivalent vaccine were to cause these or other side effects, the risk equals the benefit. Not to mention the cost associated with developing a vaccine each year to keep up with changing cold bugs.



COVID-19: Increasingly common. Very easily spread. Significant mortality in older people, and significant morbidity in other groups as well. This virus also has a huge cost to society - healthcare costs are astronomical, people are losing their jobs, the economy is going to be significantly affected. The efficacy, side effects and cost of any vaccine are yet to be determined. 



In summary, as much as everyone would like not to experience colds 3-4 times a year, a vaccine against the common cold is not justifiable. A vaccine against COVID-19 is justifiable.


EDIT: Thanks for the silver!
Something else worth mentioning is a vaccine isnt a cure. A vaccine isnt going to do a whole lot for someone who is already sick, but it can help prevent people from getting sick in the first place. We arent trying to "cure" COVID19, we are just trying to prevent its spread and to manage the symptoms in those who do get it.
[removed]
In addition to the common cold not being just one thing, and therefore complicated to address, the common cold generally isn’t deadly, or disfiguring. This is also why a cure for the herpes virus is not a top priority, even though everyone who carries herpes in any form would be very happy to be rid of it.

The economics of the problem are a huge factor, not the least consideration of which is how many products would disappear from the economy if certain things were eliminated. Vaccination research is prioritized, in terms of funding allocation, by their profit:loss ratio.

Cancer is very, very costly. So is heart disease. Not only is it expensive to treat, it almost randomly pulls workers out of businesses and, therefore, profitable production. People work throughout the common cold, and with herpes, and the cost of allowing it to survive is not only negligible, but profitable in all the remedies created. Homeopathic “remedies” are probably the most profitable in existence, since they literally do nothing and are literally just water. But it’s a profitable market, so it exists, and thrives. Placebos are a huge profit industry in countries with universal healthcare for this reason. 

The coronavirus has ground the economy to a virtual standstill. Placebos are no help against something actually deadly. Fixing it is an absolute top priority.
The common cold is *not* a type of coronavirus. There are 4 coronaviruses that cause the common cold, and they are among \~200 viral causes, including rhinovirus, herpes, and some strains of influenzavirus.

The reason we don't have a cure for the common cold is that trying to find a cover-all cure for a condition with such a laundry list of causes is rather like herding cats. It's easier to just manage the symptoms.

With COVID-19, however, we know the cause of each case, because it's only ever caused by SARS CoV-2 - which means we can just target that single virus.
To better state the question. There are 4 common coronaviruses which cause colds (about 20% of them overall). Knowing that colds are one of the major reasons for employee absenteeism and loss of productivity, why don't we have vaccines for those 4 coronaviruses? A vaccine which prevented 20% of colds would be a blockbuster product and would save billions of dollar every year.
Also important to note that while a vaccine makes a virus basically irrelevant to your body its technically not a cure. A cure is given to someone who is sick to heal them, a vaccine is preemptive it prevents you from getting sick in the first place.
A) a single common cold virus isn’t dangerous enough to justify the expense of producing a vaccine; B) it mutates too frequently to make a vaccine effective; C) even if a vaccine for one cold virus were produced and was effective, there are a couple of hundred other cold vaccines out there that you could catch.
That's because there's a different strand of the cold every year, so there isn't an "all purpose cold vaccine" out there. Covid-19 is a specific strand of coronavirus, so a vaccine could theoretically be invented because it would target that specific one.
Doc here. Know how “cancer” doesn’t actually mean anything useful because of how many different types there are? Some we can treat very well and others we can’t. But we call it all cancer in lay language. 

The common cold is the same. It’s just the constellation of symptomatic manifestation that many different and unique viruses have. Rhinovirus for example is one of the most common causes of what manifests to us as the common cold. Vaccination is impossible because of so much antigenic variability and very high rate of mutation, so if we can treat one, the treatment is obsolete immediately because of mutation. 

Coronavirus is one other manifestation of the common cold that doesn’t mutate much at all. So it can be targeted effectively. There just hasn’t been much need to prior to this strain mutating to jump into humans. Imagine a manifestation of a virus like Ebola but with the mutation rate of rhinovirus? Scary shit. 

So, covid19 is a coronavirus that mutated to become unique, it was unexpected entirely and spread around the world. Thus we call it a pandemic. There’s a big difference in words like epidemic and pandemic, so it’s important to use them correctly. But that’s a tangent. Treatment is for covid19 is a matter of time because it’s a relatively stable virus. The US strain is different than the European strain, but it’s not a meaningful difference for a targeted medicine perspective.
The common cold isn't a specific disease, it's a collection of loosely similar diseases characterized by a pattern of upper respiratory symptoms.

Thus, there are a lot of viruses that can cause the common cold, including rhinoviruses, adenoviruses, and yes, some coronaviruses.

Because of that, there isn't really a possibility to vaccinate against just one cold - you'd have to vaccinate against all of them.

By contrast, there is only one coronavirus known to cause COVID-19.

Further, many cold viruses mutate very quickly, meaning that even if you were vaccinated against (for instance) all rhinoviruses last year, there's a very high chance that a new strain of rhinovirus would evolve in that time that you're not immune to.

The flu virus mutates very quickly as well, which is why we need to vaccinate every year; and if COVID-19 becomes a common part of the microbiome, there are concerns that it'll need similar regular vaccination to maintain immunity.
There are many "common cold" viruses that cause the same symptoms in the body. The reason there is no vaccine is because there are just too many different cold viruses out there and they are constantly mutating to create new ones. Also they aren't very hazardous, I don't know the figures but the common cold has a very low death rate. These three aspects combined juts mean it's not worth trying. Probably would be too expensive also.
The common cold isn't a type of coronavirus. A cold refers pretty much any mild viral infection that affects the nose and throat. It can come from many different viruses  with different evolutionary origins, including coronaviruses.
It wasn't mentioned yet, but the most promising medications are not directly cures, but help prevent the body from overreacting to the virus.

  


Another thing that I didn't see mentioned yet is, that it's a quesrion of risk vs. gain. Many medications have risks associated to them that are simply not worth to be take for a simpme cold.
They don't, but once 60% of the population have survived it then a further 'outbreak' can't happen.

So the whole point is not to stop people getting it, but to spread out the outbreak over a longer period so the hospitals don't get over whelmed.

Once the epidemic is over, then a vaccine, if possible, may prevent deaths in future, but will never be in place to stop the current epidemic.
Misconceptions here. 

The flu vaccine is made from 4 different virus strains. Some strains are kind of the same each year, some others change. However, due to the high risk of mutation one year to another it's hard to predict what will be the epidemic strain. Guesses and data analysis from regions that are impacted before us help to determined the best selection of viral strains. Sometimes, the estimations and guesses are wrong, that's why the vaccine of a year doesn't work that much. 


As implied, it's not a cure. It is cause by the frequent mutations. You can get immunized to a certain strain for a certain period but it won't protect you for the next year. Moreover, your build immunity is not going to last that long. Hence the necessity to have multiple shots for certain diseases that are always the same strains.


About SARS - COV2, I didn't search why infectiologist have high hopes to find a vaccine. I think It's highly likely due to the unique strain responsible for the disease (cov19)
Bc a vaccine isn't a cure. It's a prevention. Also there is no such thing as the "common cold". It's actually an assortment of 200+ endemic rhino viruses, and influenzas, some of which mutate enough each year to become re-infectious among the general population. It's not like the vaccine for the flu season cures an illness anyways, it's just supposed to prevent you from getting sick with the real disease later if you encounter it
If covid-19 does mutate its likely to become less deadly. If it becomes more deadly then it will likely starve itself to death so the more likely to survive mutations tend to less deadly and more successful viruses. IE colds and flu.
I think there's also an element of it not really being worth it to put a big push into curing or preventing sicknesses that are generally harmless and self-limiting. Colds are annoying but generally not deadly or result in long-lasting ill effects. Preventing it isn't very sexy, and there's probably not a lot of funding in that line of research.
Once the fire is set, it is hard to extinguish it. But it is easier to recognise the torch that is used to light it up.

There is several ways to make a vaccine. Some is to make a simmilar virus that do nothing (not even reproduce), which can also be dead. In fact, dead is what they often do. Then the body see this intruder (a "plastic" replica torch) and make some defences against it. Since that one is inactive, it just get eliminated, but now the body know what it is and learned how to fight it. When you do get the real thing, the body already have some antibody for it, and know right away that this is an intruder and attack it, before it can do any serious damage. It's like having some firefighters already on site. Yes, thing may be scorched, but the building don't catch all on fire.

This is all good until the virus mutate. Now it is not a torch anymore, it's a zippo... Or a flame thrower. Or a "this is not a flame thrower". In some case, the mutation is small enought that the body will still be able to attack it. Sometime it may recognise it but can't attack it as the mechanism it know is wrong. Other time it will just say "this is not a torch, it's all fine" and the building get set on fire.

Also, there is some vaccine already made. It worked in labs. It worked on mices. I don't know at what stage it is, but it's going up the chain. Then there will be some clinical trials, which mean limited group, and eventually everyone.

Last time I read, atleast one was on human trial. They need to test the effect of it on the long term. Sometime, but rarelly, people get some bad side effects. Or finally get innefective. This is why they don't mass produce it right now. Also, they want to see how long it actually protect you for. It would be stupid to get a vaccine that work for only a week or two!
Covid-19 virus is pretty stable and doesn't mutate significantly. There is only 2 strains out there L & S.

Finding a vaccine is straight forward and will work against the viruse for several decades. 

But common cold viruses mutate every year into a new strain. So a vaccine that works this year won't work next year. 

Read this in a Bloomberg article while googling for Corona strains!
If this follows the genetic path other corona viruses like SARS and MERS, it will become less virulent as it passes through people. This means people at the beginning of a pandemic get sicker than people who get infected at the end. We are still in the beginning phase in our country. As for a vaccine, coronaviruses mutate pretty easily so making vaccine for them is difficult, it would mean we would need a different vaccine every year like we do for the flu vaccine. It's not impossible to make a vaccine for the virus with today's virus, but as the virus passes through people, it continues to mutate so eventually the immunity the vaccine would provide wouldn't be effective on the new strain with every passing year...
Not everyone in the medical community agrees with that proposition.

I for one, don't.

In my opinion, COVID-19 will be followed by a different strain in 2020 (COVID-20).

We may develop a vaccine for COVID-19 but it will not provide complete immunity to COVID-20, so some people will still get infected by COVID-20, leading to COVID-21 the following year.
Surprised that I haven't found a comment yet mentioning that viruses do not have cures.  You can't cure a virus.  You can do lots of things like vaccines, which is a weakened virus that prepares the immune system, and lots of types of medicine which can treat symptoms or help the body defend itself, but in the end the immune system is the only "cure."  Bacteria on the other hand can be killed, generally through antibiotics (which don't work in a virus but can treat secondary infections caused by bacteria).
There is no cure to coronavirus that cause the common cold not because it can’t be done, but because it isn’t worth a ~$10bn investment.

Also, what do we mean by “cure?”  If it shortens the duration of the illness by 12 hours, is that a cure?  That’s what we have with neuraminidase inhibitors like oseltamivir.  Baloxavir shortens flu by a whole day if given soon enough.  

If we do find one here, are we going to inject people who have a cold with remdesivir?  Unlikely.

And then you’ve just injected someone with a very expensive drug but it turns out that they have rhinovirus (not sure if remdesivir even works on that one).

So *could* we cure the common cold?  Yes.  It’s possible.  Is it worth our resources?  No.
Vaccine and cure aren’t the same thing.  

We can have a vaccine in 12-18 months that keeps most people from getting sick from the virus while we may never find a cure that helps you recover from the virus once you’re sick from it.
As far as I understood from a speed-lesson yesterday: we're not trying to find a vaccine as per *definition* what we're trying to do now is compromise the RNA in the virus and its ability to replicate, many therapies suggested are the same used to fight against HIV and Malaria. Please prove me wrong or right as I'm in an information overload and don't even know what to believe anymore.
I’m not an expert, but I work at a scientific instrument company. The way my coworkers explained it is that COVID 19 has an extremely simple structure, hence why it’s so effective in reproducing. The only way we can stop it is everyone was vaccinated thus preventing a mutation.
What it largely boils down to is this.  The common cold virus is a rapidly mutating one, as is the flu, albeit slightly less rapid. So tailoring vaccines for them, especially the cold, is a constant race against time as the virus changes in small-but-critical ways that make previous treatments ineffective and the immune system struggling to keep pace and re-learn each new strain despite having had millions of years of exposure.

COVID-19, on the other hand, from what we can tell, is a very slow-to-change virus, giving us much more time to work on a curative formula, and much more likely to remain unchanged for a long time, thus allowing the immune system to learn from it and be more likely to counteract it in the future.
Even if a drug company could develop a vaccine for all common colds, would it be worth it? All the R&D not to mention money, and how many people would bother getting it? Most people just see a cold as a minor inconvenience...
The common cold is genetically fluid. As harmless as it is, it changes by the week and so we are unable to practically develop a vaccine. Also, surviving minor infections strengthens the immune system against other infections.
The same reason there isn’t a vaccine for the herpes virus. It can make a person feel ugly, but it’s rarely lethal. The vaccines we have are mostly for illnesses with a high death toll. There are countless viruses in the world, so the scientists are going to get funding for the most dangerous ones first.
To make it fairly simple- 
Influenza is caused by a virus family called orthomyxovirus ( we’ll call it virus A) 
COVID-19 is caused by a virus family called coronavirus ( you’ll remember that) . 
So, virus A, among a few other viruses has a segmented RNA which is highly prone to mutation - via mechanisms called genetic drift and genetic shift. That’s why virus A’s genome changes almost every year and the flu shot has to be taken every year. (Scientists predict the strain prior to the flu season and make a vaccine ). 
COVID-19 has a fairly simple genome as compared to virus A. (RNA, SS linear, encapsulated) 
So just as we have vaccines for other viruses, like measles, mumps, rabies etc. we can have one for COVID-19 too.
The common cold is normally caused by a type of virus called rhino virus (which we have no cure for). We get em everyday theyre everywhere and normally all you need to do is rest and stay home. They are also something we have to live with cause even though our body knows what to do, they can still affect the actual body itself physically.  Covid19 and other corona viruses can also cause the common cold but are common normally in other mammals and birds. If the virus comes to us, theyll probably fudge our day up because our bodies aren’t exactly wired to stop them yet. So this is where vaccines come in. They allow your body to learn the genetic makeup of the virus (easy most of the time cause its normally just a random strand of rna). The hardest part is to make the virus safe (will it make the body learn or will it fudge it up) and effective (will this thing work for the majority?). In most cases we wont know until it gets out there but we have had success with other vaccines and since this virus is relatively mild in comparison to sars, its highly likely that a vaccine can help. There is no guarantee a vaccine will work...no matter what people say. A vaccine is like shooting in the dark but if it works its an amazing thing.
Other responses have gone into what makes an individual long-lived. This post will address the question as OP phrased it: what determines the lifespan *of a species*? And what's the deal with human longevity?

Many people have a concept of aging that could be described as the "wear-and-tear" model. Basically, the notion is that as you go through life, you accumulate nicks and dings from macroscopic scars and stretch marks to accumulating microscopic injury and DNA damage. Eventually it's just too much and you run down. *It turns out this concept is markedly untrue, at least at the species level*.

Drosophila melanogaster, the fruit fly, has been the workhorse of many experiments, including those studying longevity, life-history theory, and evolution. Many years ago, researchers working with flies did an evolution experiment. They selectively bred fruit flies for longevity, and in a remarkably short time they had flies with dramatically longer life-spans than is the norm. These flies weren't evolving novel genes in just a few generations, rather, they had recombined existing alleles in ways that lead to longevity. The important thing to note is what happened when these long-lived flies were re-introduced into a 'normal' population. Did these super-long-lived flies thrive? No, quite the contrary. Within a few short generations, any sign of longevity had disappeared. In competition with their 'normal' counterparts, they were immediately outcompeted. (I tried finding the original papers, here's a [modern replication](http://www.pnas.org/content/97/7/3309.full) of these sorts of studies, with more precision.)

What these researchers discovered was that while longevity is perfectly possible, and within the realm of already-existing genes in most populations, this potential isn't realized *because of trade-offs*. Increased longevity is exchanged for decreased fitness in other areas. Perhaps these long-lived flies in nature would be less likely to evade predators, have more difficulty finding food, or are less likely to find a mate. Everything has a cost. Note that this is different than the 'programmed' idea of death, that your genes have predetermined a time of death for you. In the life-history model of aging, it's not that DNA 'wants' an organism to die, far from it - it's just that other traits (that hurt longevity) are more directly advantageous to the propagation of genes, and so combinations of genes with these traits out-survive and out-reproduce the combinations that lead to longevity.

The immune system gets interesting. Unless something else kills it first, it's all but inevitable that an animal will die of cancer. The ticking clock of mutations accumulate, eventually leading cells to proliferate out of control, breaking internal apoptosis mechanisms, evading the immune system, and eventually killing the host organism by hogging the lion's share of nutrients and energy.

What exactly is a 'cancer resistance gene' or a 'cancer susceptibility gene'? Why would anything evolve a 'cancer susceptibility gene'? That doesn't make sense, and they don't. Rather, it's a gene that is perilously close to breaking something important, if it mutates. *Redundancy* is an important concept. If you have 3 copies of something important, you're more or less ok if 2 of them break. If you only have 1 copy, you will be in trouble quickly. Animals that tend to be long-lived, such as elephants and whales, tend to have many, multiply redundant copies of genes involved in important functions like apoptosis and immune regulation - the sorts of genes that when they break ~~tend to cause~~ are no longer able to stop cancer.

Humans don't seem to have nearly enough multiply-redundant copies of these sorts of genes to explain all of our longevity, while it might contribute somewhat. We don't look like elephants or whales in this respect. This sort of evolution towards longevity would reasonably be expected to take a relatively long time to evolve, as many duplication events would have to happen, and then spread through the population. 

It seems humans are probably like the flies that were selectively bred specifically for longevity. In humans, the [grandmother hypothesis](https://www.nature.com/articles/428128a)  conceives of human longevity as something that is directly advantageous to gene propagation ([wikipedia](https://en.wikipedia.org/wiki/Grandmother_hypothesis) introduces the concept well). It's a sort of grandiose kin selection, where elderly humans in prehistoric times were still able to contribute to the survival and reproductive fitness of their children, grandchildren, and extended relatives. Unlike other species that experience a rapid deterioration after their own ability to reproduce goes into decline (such as salmon after spawning), humans *could* contribute into their later years, and so longevity was selected for. Maybe we're not so unlike our experimental flies after all.
[removed]
Evolutionary biologist here, a lot of the posts here are accurately describing the mechanisms by which we are better protected from aging, but behind each of those is a genetic prerogative for longer life.

The first principle is that the fitness of an organism is defined as its ability to produce offspring which in turn survive to reproduce.

If humans live longer that a comparable species, it must be because of an increase in fitness. Some of the leading theories are longer term parental care or multi generational families.  Think about it this way: why should women ever go through menopause?  It's because having non reproductive women in the social group increases the fitness of her offspring. 
There's no hard-and-fast rule that I'm aware of, but several things factor into it (in no particular order).

1. Metabolism. As a general rule, the slower the organism's metabolism, the longer it will usually live.

2. Growth rate. The slower the growth, the longer the lifespan.

3. Age at maturity. Animals that reach maturity at a later age tend to have a longer lifespan.
[deleted]
From an evolutionary perspective, if an animal is likely to be eaten early on in its life, it doesn't make sense for that species to invest energy on lengthening its life. More likely, it would invest its energy in producing as many offspring as quickly as possible.

An animal that is not easily predated is more likely to invest energy in processes that lengthen lifespan, and focus on raising just a few offspring (quality vs. quantity).
[removed]
First, it is important to understand how hox  genes work. A hox gene is a gene that codes for several things at once. This means that certain traits are inherently linked as they are derived from the same line of genetic code. 

So for example: Labradors have webbed feet that make it easier to swim. The gene that codes for webbed feet however also codes for the trait for floppy ears. So as long as Labradors have webbed feet, they will have floppy ears despite there being no evolutionary advantage, it just comes with the territory.

The purpose of hox genes is that it sorta 'compresses' how much coding is required. Rather than code for every finger, there is one structural blueprint for 'digit' and regulatory genes that determine growth. It's these regulatory genes that are what make humans unique.

So if you recall your anthropology, Australopithecines were short, short lived, and had a more or less a chimp-sized brain. All these fearures are controlled by a regularory hox gene. When that hox gene mutated to exhibit increased growth it affected everything: We got taller, our brains got bigger, and our longevity was extended.

This is why we take an unusually long time to mature compared to other animals: our 'stretched' hox gene makes us have an extra long maturation phase which is why humans retain many childlike behavioral traits into adulthood and why we have a relatively early sexual maturation even though socially it is recognized that full brain development does not occur until later. 

So the reason humans live so long and have big brains is kinda by accident. We just needed to be tall and the rest just got dragged along with it.
I believe this could be dependent on the R or K strategy of that species.  This is the strategy a species takes to maximize its fitness and its contribution of genes to the next generation.

R strategists, like rats or insects, are fast reproducing, create many offspring that are less expensive, and live short lives because their reproductive life span is short but this is how they maximize their fitness.

K strategists, like humans, live longer and are more capable of a longer time-span for reproducing, (it's believed menopause exists in humans so that they can help family with THEIR offspring, maximizing that individual's overall fitness or passing on of their genes) so offspring is more "expensive" and typically live longer.

So maybe an extensive lifespan contributor is based on these strategies and that species' ability to maximize its fitness and overall genes present in the gene pool?
Disclaimer: not a geneticist.. but if my memory serves me correctly:

I believe there has been a connection found with telomere length, which is essentially extra DNA on the ends, which gets progressively shorter with each mitotic division.

There is a correlation between number of cell divisions, and telomere length. (And life-span of said creature).

Interestingly, lobster telomere don't get snipped off, and it's a little unclear if there is an associated hard limit to number of mitotic cell divisions for lobster. Some lobster have been found with surprising size and age..

And I here is a link for fun: https://www.drjoedispenza.com/blog/general/telomeres-what-does-a-lobster-know-that-you-dont-know/
[removed]
[removed]
"But for now, I think there are two things we can say: that the fundamental processes, almost certainly, are driving tissue health.  What actually drives lifespan?  We really don't know.  The truth is, we don't really understand, for example, why a mouse lives 3 years and a human lives over 100 years, if you're lucky, and what it is that evolution had to do to take these two organisms that are genetically pretty similar but have a 30-fold difference in lifespan"

— Dr. Judith Campisi, professor of biogerontology at the Buck Institute for Research on Aging, a co-editor in chief of the Aging Journal and an expert on the role of cellular senescence in the aging process and development of cancer.

[Source](https://youtu.be/adg3vUez3EU?t=1m52s) 
[removed]
To me, the interesting thing about this discussion is that longevity is directly related to the perception of time. Meaning, while we may perceive 65 years one way, a cat or mouse or ant may experience it's average life span in a similar way.  So, if you have a 16 year old dog, it may *feel* time the same way we would feel it if we had lived into our 90s.  I don't know if there is any research that has been conducted in this area, but I find how we perceive time to be completely subjective even within our own species and own life times.  So whose to say that an insect that only lives for 6 months doesn't *feel* like it's lived a very long, healthy life?  I know this is the other side of the coin, but I find it interesting and related nonetheless. 
short unscientific answer is that there seems to be correlation between heart rate and life span meaning there seems to be a rough "limit" of beats for the heart so that fast small animals with fast heart rate die faster then say turtles which their heart beat slower then even us.

as I said it's not scientific but that might give you a rough answer.
there are probably other things that take into account besides this simple answer because the answer seems stupidly simple...
The ability to consciously condition ourselves lets our bodies know when to we actually need to redline it. Same reason why long distance atheletes have resting heart rates in the low 30's & 40's. They may have increased heart rate during those 1-3 hours they exercise, but the other 20 odd hours they're body is basically idling. 
Smaller animals with a higher metabolism experience time at a faster rate. Therefore what seems like only 20 years to us is experienced by the cat as perhaps 80 years. Due to this difference in perception of time our experienced lifetime is actually the same. It is possible that this experienced rate is similar for all species and just that we perceive different lifetimes due to us functioning as an observer locked in our human rate of time. In order to assume that species have defined lifetimes you must assume that time passes at a constant rate for all observers. We know this isn't true from Einsteins theories on relativity. However, for all intensive purposes I would assume your question is answered simply by metabolic rate. More things happening (faster metabolic rate) causes more chance for wear and tear or failure. (Death)
While there are many ways in which organisms age and die from "old age" (determining their lifespans), ultimately what determines the body's various adaptive protections against these, and more accurately, at which age the protections are relaxed, have to do with evolution. 

Basically, evolution selects against genetic variants (which can include susceptibility to cancer, telomere persistence, organ and tissue maintenance) that decrease individuals with that variant's reproductive success. If this constraint is relaxed, say most individuals in a species don't live past 5 years due to predation and thus genetic variants that protect against aging and disease in older individuals rarely matter for survival since most individuals are dead before they would have an effect, selective pressure is relaxed on those variants and they may be lost randomly (genetic drift) or even be selected against if they confer an even slight disadvantage to reproduction.

We can see this by comparing the mainland and island populations of opossum in Georgia, the island population without as many external factors contributing to death such as cars and predation was correlated with slower aging. (http://www.spokesman.com/stories/1997/jun/24/an-age-old-question-can-we-conquer-the-aging/)

Basically, the specific causes for aging and death are many because external factors (accidents, predation, etc) make surviving past a certain age unlikely anyway and thus selective pressure to protect against these varied assaults is relaxed. 

What does it matter if a genetic variant protects against cancer if most individuals who carry the variant are already dead by the time they would have developed cancer anyway?
Most animals get roughly the same magnitude of heartbeats, depending on the source between 1 and 10 billion. Smaller animals hearts beat faster so they clock that number up quicker. Humans also have access to medicine and safety/shelter which reduce the chances of untimely death.     
The link includes a graph a little way down the page comparing us to a range of common mammals
https://www.google.com.au/amp/s/www.runnersworld.com/sweat-science/how-many-heart-beats-do-we-get%3famp
I think what is being slightly underrated in this thread along with all the other great answers is the fact that this "lifespan" we have is based off of modern technology and is also based on how we treat humans differently. If a human gets a disease, they will often try until the bitter end to continue living even if that means living with the disease for decades. Hose pets are often put down when their lives are no longer "humane." We definitely have a different standard for this which I feel like artificially lengthens human lives.
At a genetic level, which is probably the most applicable here, different species have different lengths of Telomeres. Telomeres are the tips of chromosomes and contain non-coding DNA, meaning it doesn't actually have any Gene's on it. Every time cells divide, these Telomeres are degraded slightly, and cannot grow back, limiting the amount of times cells can divide. When the Telomeres have completely degraded, the cells can no longer divide, halting the creation of new cells and aging the organism. Longer telomeres take more cell divisions to go away, meaning the organism will live longer, therefore, species with different lengths of Telomeres on their chromosomes live for different lengths of time.
How much accumulative damage your body can sustain. 

In practice this can be seen by measuring heart rate.

Which is relative to total animal mass (I believe roughly half the rate for every quadrupling in mass).

If you equalise for heart beats per life, every animal in nature gets roughly the same number of beats (I believe it is ~1.5 billion beats per lifetime)

Modern medicine and vastly more hygienic lifestyles with less potential stressors have helped to improve our life times way beyond the general range expected in untamed nature given our heart rate.

From 35-45 years to 80+

If you Google universal scaling laws, with respect to heart rate, you should have no trouble at all finding studies corroborating this. 
Super summarized version, the length of time a cell can survive replication. Humans will never be able to live forever purely because even if we could cure all disease, our cells could not survive more than around 140 years of regeneration. Other species have different length of time before the cell damage is too great to continue healing.
[Lake Baikal](http://www.irkutsk.org/baikal/animals.htm) (deepest freshwater lake) appears to have complex life forms at it's greatest depths.

"Baikal is also home to the world's most abyssal freshwater fish. These fish have managed to preserve eyesight even at the greatest depths, although they see only in black and white."

Also the golomyanka (oil fish) "can endure most pressure in the depths of the Baikal water. At night it rises to the water surface, and at daytime it swims down to great depths. Limnologists have had a chance to observe the golomynka's behaviour in the water depths. At a depth of 1,000-1,400 metres and more, the golomyanka moves freely both horizontally and vertically, whereas at such a depth even a cannon cannot shoot because of the enormous pressure."
There are organisms that live at the bottom of the world's deepest lakes such as [amphipods](http://onlinelibrary.wiley.com/doi/10.1111/j.1463-6409.2011.00490.x/full), [bacteria](https://www.researchgate.net/profile/Natalia_Belkova/publication/12725338_Diversity_of_bacteria_at_various_depths_in_the_southern_part_of_lake_Baikal_as_detected_by_16S_rRNA_sequencing/links/00b49528d5fcc08ddc000000.pdf), and fishes like *Trematocara* and Abyssocottids, but not very many for several reasons.

The biggest reason is that the abyssal zone of the ocean is truly enormous and has existed for billions of years. Evolution has been able to have its heyday down there, which has led to a diverse assemblage of fishes, crustaceans, cephalopods, and other invertebrates.

On top of this, there are only two freshwater lakes on earth that are over 1000 m deep, Baikal and Tanganyika. The latter is about ten million years old, the former perhaps 30 million years old. Compared to the age of the ocean, that is a tiny fraction of time. They have also been isolated from other water bodies due to topography, so there aren't many immigrants that can later evolve into new species.

In addition, Tanganyika has spent a significant proportion of its history as a much shallower lake than this (it was actually multiple lakes, which is [reflected in the diversity of cichlid fish there](http://kops.uni-konstanz.de/bitstream/handle/123456789/7087/Mitochondrial_Phylogeography_of_Rock_Dwelling_1996.pdf?sequence=1&isAllowed=y)). So the abyssal zone in Tanganyika is relatively young and unstable.

Baikal is older and larger and the abyssal zone was more stable (including never scoured by ice like most similar lakes), but it is still in a cold climate (less biodiversity to begin with) and isolated from other bodies of water. Because of the depth of ice coverage, the entire Baikal ecosystem has occasionally [collapsed](https://www.sciencedirect.com/science/article/pii/S0031018204001099), effectively re-starting the evolutionary race.

So the two lakes on earth that are deep enough to be completely devoid of light just haven't had the evolutionary chances of the ocean.
Information is minimal but you may find this interesting: https://www.washingtonpost.com/archive/politics/1985/08/04/submariners-cast-light-on-bottom-of-lake-superior/d6ca0026-9874-4795-9c87-9de7405b2e03/?utm_term=.fe7a7e14f434

Superior isn't nearly as deep as baikal but still deep enough to be largely lightless.
[Olm!](http://www.radioantena.si/img/Gallery/Photo/cl_3106220b-0171-43e0-8776-66c399783a56.jpg) They're aquatic salamanders endemic to Slovenia, Italy, Croatia and Bosnia. They're basically stuck in their larval cycle and due to living in dark caves they have no eyes. Not 1000 meters deep, but still pretty neat.
Though due to different reasons other than lack of light, in this case - lack of oxygen, I believe the creatures in meromictic lakes are also quite abnormal - and certainly worth mentioning amongst "deep sea life" or creatures not found commonly in nature.

I'd never even heard of such a thing until I was wandering around Québec and found Pink Lake. I found it fairly fascinating.

https://en.wikipedia.org/wiki/Meromictic_lake

https://katatrepsis.com/2011/08/10/meromictic-lakes/


Not entirely relevant but if you're interested, it is possible to scuba dive and see deep sea life sometimes. At Milford Sound in New Zealand there's a dark layer of fresh water sitting on top of the salt water beneath (due to the tannins from run off from forests) which artificially cuts off a lot of light to the clear salt water below. It means that a lot of deep sea creatures come up and live in shallower waters than they would normally, in the strike zone of recreational diving. 
Not to be too pedantic, but...  It's not light incoming light just up and dies when it reaches 1000 meters.  As with most substances, the ocean water will have a "half-distance" thickness that will block or scatter half the light.  If the half-distance is 100 meters, the light will have been halved 10 times (a factor of 1024) by the time it reaches 1000m.  At 2000m, light is down to 1/~1,000,000 surface brightness. Etc.

The transparency of lake water will vary throughout the year as silt, algae, etc. concentrations change.  Deep lakes certainly will have a near-black zone for at least part of the year. 
Here's an odd case for you. [Mastacembelus brichardi, the Blind Spiny Eel](http://research.amnh.org/vz/ichthyology/congo/mastacembelidLIVE/images/Mastacembelus_brichardi3.jpg)

Definitely a weird fish that looks like it belongs in an ocean trench, but it's freshwater. Weirder still, it's not from deep water either. It lives in clear, shallow river rapids in the lower Congo, where eyesight ought to be highly advantageous. Yet it and at least four other fish species in the rapids, including the [world's only blind cichlid](http://www.cichlids.ru/mcforum/uploads/monthly_2017_07/large.9EBD0C0A-1BBA-41D8-AC07-E5DE8B249B31_zps7cyfoohi.thumb.jpg.591bbd1acb6fbf4cfe2d796c13c791f4.jpg.adcdf3f0c51911ef73209638f9888111.jpg), have no eyesight at all and nobody knows why.
Not specifically deep but because Edwards Aquifer is an underground body of water it is home to several unique and/or endangered and highly specialized animals that have evolved to live without light including the Texas Blind Salamander. The Texas Blind Salamander is a sightless amphibian best recognized for having no eyes. http://www.edwardsaquifer.net/species.html
If there was such a thing, wouldn't it completely vary from lake to lake?  It's like, the ocean is just one ocean, there's stuff you only find in certain parts, but theoretically anything can get to anywhere.  Lakes are like... isolated little communities.  There could be some bizarre creature that's only in one lake and evolved separately over a million years.  That kind of freaks me out a little to think about.
[deleted]
Finally something I can help out with! Source: I'm a lecturer in the UK (roughly equiv. to assistant professor in the USA) specialising in life cycle assessment, particularly energy sources.

I wrote a paper that's open access, which you can find here: http://www.sciencedirect.com/science/article/pii/S0306261914008745

The TLDR is that solar is good in terms of climate change but generally less good in terms of other impacts. Overall it's typically not as good as wind or nuclear power. However, bear in mind two things:

1. Impacts are very dependent on location. E.g. solar installed here in the UK is worse than in Nevada or Spain because the impacts are all up-front but you get much more energy output in sunny countries, therefore better impacts per kWh.
2. Solar technology is moving fast. I actually have some updated figures that I'd love to share with you but they're not published yet, so they're not peer-reviewed. Reductions in impacts in the past few years have been considerable: about 50% reduction between 2005 and 2015. So for instance I now estimate a carbon footprint of about 45 g CO2-eq./kWh for a UK installation or 27 g in Spain. This contrasts with the figure of 89g you'll see in the paper I linked.
Here is another LCA (life cycle assessment) of different energy sources regarding electricity production, 2013: 

>http://www.uni-obuda.hu/users/grollerg/LCA/hazidolgozathoz/lca-electricity%20generation%20technologies.pdf

Tech | kg CO2-eq per MWh
---|---                       
Lignite |800–1300
Hard coal | 660–1050
Natural gas|  380–1000
Oil| 530–900
Biomass |8.5–130
Solar energy| 13–190
Wind| 3–41
Nuclear power| 3–35
Hydropower| 2–20


[This study](https://www.bnl.gov/pv/files/pdf/230_SolarEnergy_PV_LCA_2011.pdf) is outdated, and things have likely improved, but it's nice and comprehensive and was #2 in my google search.  Some take-aways:

Greenhouse gas emissions for solar panel production is about on par with nuclear power production - both of which are about 1/500th the emissions of burning coal.  (Mostly from the fossil fuels burned by the  mining and processing of materials, which could also be eliminated with electric mining vehicles)

Heavy metal polution was a win too, though "less" of one - solar released 1/50th of the cadmium that burning oil does, and less than a third that of coal, due to the requirements that coal power plants capture particulates.
take some time to look at this article about evaporative generators. 
they can be made out of sustainable materials, and will be much safer than any other source of energy.

https://www.newscientist.com/article/2148623-energy-from-evaporating-water-could-rival-wind-and-solar/
Please help:

I've have an ongoing debate at work comparing the energy it takes to produce a solar panel vs. output over its lifetime. A clear energy in - energy out analysis. 

His argument is that there is no clear data of the actual total energy being saved by producing solar energy in contrast to coal or natural gas... There are plenty of reports on green house effects, but not on total energy. 
In terms of the mining and manufacturing it is not too favorable in comparison to fossil fuels; however, once made and in place solar/wind/etc do not continue to pollute. Problems created from mines are much easier to re-mediate than spills from oil, or air problems with coal. [Do not confuse 'easier' with 'easy'].

Solar panels can work for their entire life span without polluting their local environment. The bulk of the pollution created is in the vicinity of the mine and the plant where they are created. With fossil fuel, pollution is created constantly over its life at the point(s) where it is being used--by your car, your water heater, your local power plant.
There was the perfect  documentary about this and I can not think of the name of it for the life of me, something about elements?  Anyways this is a snip-it that competes answers your question from it.  Don’t know how I found the video but am still unable to find the name of it.  I think it was Dallas based.  Whatever, here it is:  

https://youtu.be/sRZh42fWLbg
My understanding is that the pollution of a solar panel is almost entirely front loaded into it's production.  Pollutants are probably similar to what is seen in the silicon chip industry (silicon dust, which is a health risk, heavy metals, etc...).  The flip side of this is that because it's all focused on one end of the products life cycle it's probably much easier to handle (we have a much better understanding of how to clean up a factory than we do about cleaning up the atmosphere).  Unfortunately I do not know any solid numbers to compare :(


A better question might be "how long does a solar panel need to run in order to become 'green'?". That is, how long does it have to run before the amount of pollution created by it during it's lifetime is less than that produced by fossil fuel for the same amount of product (electricity).
Well, Silicon is the most abundant element per weight on earth after oxygen. To purify it from quartz it takes a certain amount of energy through electrolysis.  Quite a lot of electrical energy is needed since we need to reduce silicon from +4 to 0. That means 4 electrons per atom of silicon at some 2 V. In comparison to iron which needs 2 electrons at 1 Volt (just using arbitrary numbers here because I can't be bothered to look them up). As such, It takes roughly 4 times the amount of electricity per mol, a little less per kilo.  

This is roughly the same amount of energy gained by burning coal atom for atom. 1 mol coal = 1 mol silicon --> 1 kg coal = ~1/2 kg silicon per weight if we assume 100 stochiometric conversion. The engineering around electrolysis is trivial for this question. Depending on the source of electricity and silicate this step can approach carbon neutrality. 

However, the purification, doping and making wafers can be quite energy intensive, but could arguably be done at a fraction of the energy needed for electrolysis. But we're still depending on the source of electricity. 

Once reduced to elemental silicone, recycling silicon wafers costs a fraction of the energy needed for electrolysis. 

Can a silicon wafer deliver more electricity throughout it's lifetime than what is consumed during electrolysis and refinement, yes and by a large margin. 

So it all boils down to the source of electricity and quartz, and transport from mine to installment. This can vary a lot. In all, silicon can easily be net carbon neutral or better. 

Compared to petroleum, which is more or less energy for free considering you simply put a straw in the ground and "drink all the milkshake". On the other hand, petroleum is not even remotely carbon neutral for obvious reasons. The main difference is the upfront energy cost needed to reduce silicate to silicon, but once there, PV is energy for free forever after.
While Solar may not be as "green" as wind power, the two tend to complement each other, as some days aren't sunny, some aren't windy but it's seldom that neither is  true. The tide can also generate power, so anywhere along the shore you can do all three.
It's very difficult to know green all the inputs and outputs of a solar panel would be. One would hope they're at least a bit recyclable, or that recycling options will become available in the future. I've wondered what will happen to antiquated wind turbines in 20+ years, and worry that the landscape will be dotted with rusting "crazy turn of the century green energy structures" as technology improves. 
I think that one thing to keep in mind, that a lot of folks like to ignore, is the economics.  With lowering costs to acquire, the economics have improved to the point that many folks can consider going solar from strictly a return on investment perspective.  Since you come out positive financially, you have to expend less effort to earn money, theoretically, to pay for your energy consumption.  Realistically, that money goes elsewhere, but if you goal is to retire, you can achieve it more quickly now, and other similar circumstances.  This further reduces the "impact" of energy consumption.
top comment is about C02, let me comment on energy requirements of production. 

in 2006, the term needed to earn back the energy needed for production of solar panels varied wildly between sources. some said 4 years, mainly in older sources and of installations in moderate climates, others quoted 9 months (desert installation). The interesting part of this study was that the variance was strongly influenced by the mounting bracket. It turned out that allu brackets with concrete weights took up quite some energy to make, while building integrated systems didn't have that problem.

A great improvement in "return on energy" is to be expected from thin foil systems. apparently already on the market but not very wide spread atm. 

the premise of my master dissertation was " how much would electricity cost if it was produced by renewable energy sources, in available means, in a feasible energy mix (yeah, 100% hydro is cheating), if those renewable energy sources were produced by renewable energy sources.

Just outside Las Vegas, they have a solar power plant where they use mirrors to reflect light to a tower where the heat generated is used to boil water in order to spin turbines. Since this method does not use typical solar panels, is this more or less environmentally friendly and is this more or less efficient?
[removed]
[removed]
[removed]
Here's an offbeat question that my dad asked that's been stewing in my mind for a while. Has anyone researched the impact of absorbing solar energy, rather than that energy hitting the earth? Or has anyone researched the impact of wind power on weather and so on?
I work with aircraft telemetry. On a test aircraft, we telemeter only a small portion of the parameters we record, because there isn't enough bandwidth to send everything in real time. It's probably not feasible to live stream all the parameters a black box records, and its easier to just make them really hard to destroy and recover the data later.
In some cases, data is already streamed live. For example, some aircraft engines stream data to the engine manufacturer during flight, so the manufacturer sometimes knows about potential problems before the flight crew do.
A black box is two data recorders, one that's recording real-time information about plane and one that's recording voice. 

The information is useful after a crash, or after a near miss/emergency, but it's not particularly useful any other time. 

It's hard to estimate how many planes fly a day, but based on FAA information on [faa.gov](https://faa.gov), just the US FAA handles: 16,100,000 flights a year (including international flights that enter FAA areas). That's 44,000+ daily flights. There are 5000 planes in the sky at any time at peak travel just in the US alone. 

In 2019 there were 14 fatal crashes *globally.* 

The amount of real-time data streaming you'd need to track even just the domestic commercial flights, plus cargo flights would be staggering. Streaming telemetry and voice from the entirety of a flight's transit would require massive amounts of data, storage and processing. And it's only needed those 14 times a year. 

There are limited ways to transmit data from a plane, you've got terrestrial and satellite. Terrestrial wouldn't work, there are too many hops between towers. Satellite would be available, but someone would have to put the satellites up just to record flight data. If you've ever seen how crappy in-flight WiFi is, imagine how bad having to move the data from 16 million flights would be. 

You couldn't rely on that transmission either, because it's another system to go down, satellites lose communication etc. 

The flight data recorders and cockpit voice recoders are designed to survive 3400Gs and temperatures exceeding 1000º C (1830º F). 

The NTSB has proposed cockpit image recorders as well, because control panels are now electronic—when a plane crashed with an analog gauge it usually stayed on the last position at impact. LCD screens just break.   


(A good overview is here [https://en.wikipedia.org/wiki/Flight\_recorder](https://en.wikipedia.org/wiki/Flight_recorder))

In 2014 after the Malaysia flight vanished, there were pushes to make planes transmit their data or to eject from planes before crashes. 

House Rep David Price called for black boxes that would eject after Malaysia Flight 370 vanished. 

"But he said the 9/11 Commission recommended after the terrorist hijackings in 2001 that planes carry ejectable "black boxes" to make them easier to find. Navy planes have carried them for years, and Transportation Security Administration was given $3.5 million in 2008 to study and test the proposal."

Which is good except, it's not moving along very well. The same article from that quote points out that F/A 18 Navy jets have black boxes that eject on impact detection, or when the ejection seat is triggered, and they float at well. 

In many cases, you don't need a FDR and CVR to figure out what happened, though of course they're always helpful as they show you exactly how the crew and the plane reacted. In the 14 2019 incidents, one was an attempted hijacking . There was no crash, the hijacker was killed, so that's considered a flight-based fatality for some reason. Three were planes that overshot the runways. The reason for those crashes is almost always pilot error. 

There was one bird strike (cause of crash, birds), one was a collision between two planes (cause of crash, collision), one plane hit the runway twice, banked, and hit a building. Passengers who evacuated via the wing-exits slipped on ice on the wing. (cause of crash, ice). One had a plane flying through thunderstorms.

In a few of them the cause of the crash was determined via FDR or CVR, and several were crew error. 

So to answer your question, there haven't been a lot of researchers thrown at this because it's a problem that would cost an astronomical amount to implement and would only matter in those cases where the black boxes were not retrievable anyhow.
There are now satellites which receive ADS-B data over oceanic and other sparsely populated areas. Each aircraft transmits location and various flight parameters every few seconds. In the United States, the FAA made ADS-B transmitters a requirement for all aircraft in most U.S. airspace on January 1st, 2020. FlightAware has ADS-B satellite data, but currently charges a fee for access to it.
[removed]
This is something I might be able to contribute to. I was previously a director in the aerospace communications industry and one of my roles was to manage a team that sold connectivity to airlines and aircraft operators. I did this all over the world, including Asia and I regularly wrote articles, gave presentations and spoke on panels as it relates to connectivity and aircraft tracking. 

The short answer is that it’s down to technology and cost. Forgive me if some of this is repetitive, as I've not read through all of the comments.

There’s a technology called ACARS (Aircraft Communications Addressing and Reporting System) that's been around since the 1970s and it sends information via Very High Frequency (VHF) radio as a data transmission. Think of an old school text message for airplanes. Some messages are automated like engine reports and departure messages where the aircraft sends a signal that it has taken off and landed. There are also some manual messages that the pilots can send from the cockpit. VHF is limited by line-of-sight, and a typical VHF ground station can only cover a certain distance, much like a cell phone tower. This is generally around 200 nautical miles at 20,000 feet altitude. VHF stations aren't that big, about the size of a vending machine but they're expensive to deploy and it's a complicated process to obtain and maintain government permissions to legally operate in any country. So while there are VHF ground stations that transmit this data all over the world, it’s still not worldwide because there’s not always a business case for it. Africa is a perfect example where coverage is still sparse. Additionally, due to the range issue, flights over open water do not generally have VHF coverage once you're far enough away from a ground station. 

Another supplementary technology exists called High Frequency (HF) radio. The major benefit of HF is that it's much longer range than VHF. While the stations are generally much larger than their VHF counterparts, there are far fewer of them given their capabilities. HF is usually a cheaper option that satellite and many aircraft use HF when they fly across the pond. 

Both VHF and HF are used for voice and data transmissions every day in the commercial airlines and they are considered very standard means of direct and indirect communication with the ground. The issue is one firstly of cost. The major providers of VHF and HF connectivity for aircraft around the world generally charge based on volume. The more data sent and received, the more it costs. Although there are some specially crafted plans in place and the costs of data transmissions have gotten cheaper, much of the technology on-board aircraft was designed to minimize the amount of data that's sent or received from an aircraft. Secondly, they don't have anywhere near the bandwidth capability to support the high volume data transmissions that would be required for these types of transmissions. These mediums can and do, however, facilitate the transmission of simpler text based messages pertaining to engine conditions, position reports, and manually entered crew messages amongst many others. 

Now come satellite communications or SATCOM. SATCOM has been around for decades and is really nothing new. The largest users are the maritime and aviation industries. The major and likely obvious benefit of SATCOM is that satellites are orbiting the Earth and as such aren't limited by terrain. Thus, they can work well if an aircraft is operating far out to sea where terrestrial based ground stations cannot reach. Generally speaking, SATCOM has been wired into aircraft to act as the backup for your VHF and HF connections. Think of this like VHF and HF are your home based WiFi network, but once you leave the house, SATCOM takes over. The first few satellite systems launched were quite limited in their bandwidth and further, they were horrendously expensive to use. If I remember correctly, a call from an aircraft satellite phone (SAT phone) could reach as much as $20 per minute. It's far cheaper now, but still a significant cost over a fleet of aircraft. It costs a lot of money to build, launch and maintain a satellite network so naturally, it was going to be expensive. Airlines operate on thin margins and are notorious for keeping costs to a minimum, thus SATCOM much in the same way as VHF and HF are concerned, was to be used very sparingly and only for specific purposes, if the aircraft is equipped at all. The pilots and aircraft dispatchers that I worked with during my career are still very much in the habit of keeping messages and aircraft phone calls as short as possible due to the legacy of lengthy messages being more costly.

SATCOM is also capable of sending both data and voice transmissions and relatively recently, high throughput transmissions to provide internet connectivity. Today, SATCOM on an aircraft is becoming far more common and much cheaper. Fly a commercial airline today and you're quite likely to have WiFi service available to you, some free of charge and some with a cost to the consumer, the passenger. As the demand grows for connectivity, more companies are looking to increase network capacity and increase available bandwidth and so more advanced satellites are being launched to take advantage of this growing market. Two important points to note here are the bandwidth and cost of this service. I would argue today that the bandwidth is capable of supporting a constant stream of data transmissions from the aircraft relevant to the FDR and CVR, but this is only very recently that this is so. Further, the systems were not all designed to constantly transmit data on an open connection which would be a requirement to provide the "streaming" that you're referring to.  

The last point here is the cost. Satellite services are still very expensive to all aircraft operators. Although the costs are lowering, it's still very expensive to firstly equip the aircraft with the appropriate avionics (computer and instrumentation for your aircraft) and to pay for that data connection. A large aircraft fleet such as Emirates, can afford lower pricing based on volume. A business jet operator that operates only one aircraft, however, is not so lucky and they can pay prices that are nearly $8 (USD) per Megabyte (MB) if volume based or between $6 - $25 per minute for a streaming service. It was not unusual for a customer to see invoices in the thousands of dollars for a single month for a single aircraft for satellite data services alone. Even Emirates will likely pay cents per MB which spread across their entire fleet is no small sum of money. In summation, it's still a very expensive service. 

So, those are the general means of communication. I did see some comments related to ADS-B and while that's related and a super cool topic, particularly space-based ADS-B and the partnerships between Irdium, Aireon and FlightAware, ADS-B still does not generally provide the type and volume of data that that's stored within the Cockpit Voice Recorder (CVR) and the Flight Data Recorder (FDR). These two separate "boxes" record very specific parameters on-board the aircraft which aid in a determination of the causation of an accident. So while ADS-B information would certainly be helpful in determining the possible location of a lost aircraft and even assist in any possible accident investigation, the picture is not complete without the entire data set that's stored within these two instruments. 

Why is this now a question that comes up all of the time and where do we go from here? Well, although aircraft have been disappearing since the beginning of air travel, the technology today is so advanced that of course we couldn't lose an aircraft, right? Air France Flight 447 and Malaysian Airlines Flight 370 are two very big catalysts for a shift in paradigm and these two incidents have drawn attention to the limitations to aircraft tracking and communications. In my role in the aviation industry, I saw this as likely to change. As more and more data is automatically pulled from the aircraft, bandwidth increases and the cost decreases, I think it will be likely to see raw flight parameter data being transmitted to the ground and further, I expect that civil aviation authorities will be more inclined to mandate the use of a Quick Access Recorder (QAR) and or other technologies which require the date to be routinely stored and transmitted. 

Let me know if you'd like further info or any any sources. 

TL:DR: Sending data from an aircraft is still quite complex and very expensive.
1. Airplanes crash extremely rarely and more so crash so badly that the aircraft is completely destroyed even more rare.

2. The amount of data stored in flight data recorders is very high. There's hundreds of sensors all saving data at a pretty high rate. The fastest way to transport a hard drive full of data is still mailing the hard drive, rather than passing that over even high speed internet.

3. Aircraft still have huge lengths of time where they're completely out of voice communications when over the ocean, let alone streaming high bandwidth data.

4. There's thousands and thousands of aircraft in the air at any point in time. That's a lot of data to store if it's streamed.
It costs money. 

Airline profit margins are typically lower than many other industries (where 9% to 12% can be considered amazing years). When the industry is dividing cabins in creative ways to eek out more profit, they’re not interested in voluntarily (not being mandated by the FAA) spending money or adding weight. Especially for something that is a statistically rare occurrence.
Well... the short answer is: that's not really their job.  Essentially, planes are always kind of doing that anyway.  They are, in various ways, in regular communication with the rest of the system.  They've already got ways of communicating everything that they should need to communicate with everyone that they should need to communicate it with as they need to do so.

But unexpected things happen. And when they do happen, after the fact, once the emergency bit has been taken care of, the question becomes 'what exactly happened'  so that we can figure out if something that should have stopped this from happening failed to do its job, or if there was something we didn't even know to worry about that we should pay more attention to in the future.  That's the job of the black box, to let us figure out what happened after the fact.

If we did hook up the black box so that it was constantly communicating everything it knew in real time, that wouldn't actually be terribly useful.  Most flights go as expected.  Massive amounts of information would have to be communicated over great distances and 999 times out of 1000, actually even more often than that, that information wouldn't ever need to be glanced at, because the parts of it that needed to be know are already known by the people who need to know, the pilots.

So, what about that one in a thousand, or more accurately, one in a million situation, could that information be used to save lives?  Probably not.  Because it isn't enough to have the information, we have to know what it means.  The people who analyze the black box information are trained to do that.  They're also doing it with access to other information, like what exactly happened, so they're comparing what they know from the wreckage and eye witness accounts, etc, to what the black box is telling them.

In order for there to be any point to having a black box in constant communication, we'd need someone to be able to analyze the information as it's coming in.

The day may come when we have AIs who can take in all of that information, analyze it in real time, and spot problems before they become disasters, and when that day comes, hopefully we'll be in a position to set up black boxes in the way you described, but for right now, the amount of data the black box records is mostly useful in looking back to figure out what happened.
As I answered elsewhere, this technology exists and is being adopted:

https://www.wsj.com/articles/new-black-boxes-offer-ability-to-send-real-time-data-from-plane-crashes-11549535520

https://aerospace.honeywell.com/en/learn/about-us/press-release/2019/02/honeywell-and-curtiss-wright-to-reinvent-black-box-recorders
:Wave: - I worked in the industry on this issue, a few thoughts to offer.

The aviation industry is not one single well aligned entity - there are many players, companies, manufacturers, jurisdictions, laws new and old, regulations, and airlines all coexisting, but not always talking on the same wavelength. While it might seem easy to solve something, keep in mind the many many moving parts of aviation take time to discuss, agree, validate, implement, and correct any changes to the fleets. These are also necessarily expensive changes because of the scale and complexity of the efforts involved. Keep in mind, Aircraft have to fly all over the wold, often through airspace where countries may not even agree on what time it is. Even for something as simple as time, it gets complicated.

There have been manufacturers that have instrumented commercial aircraft typical of a normal passenger type that have generated in excess of 1.5 TB of data per hour of flight, this would be 'normal' data-points, useful for a flight incident, along with more frequently useful fleet maintenance and operations work. For example even if a part should still be good, this data can help predict a part failure and maintenance schedules could be adjusted to make sure parts are available, and ready for the work to be completed efficiently.

But as has been discussed in this thread, with the given context above, the real issue stems from bandwidth and coverage. Put simply, we don't have enough of either to get all the black box data off the plane - There are existing standards for smaller sets of data to be beacon'd out of the plane, which is typically refereed to as ADS-B. These data payloads contain things like aircraft status codes, location, aircraft identity, and a few more basic things. But this is not the entirety of the black box. There have been many standards, some more successful than others, and a great many minds have thought about this problem.

There are really two things the industry is trying to solve - one - the increased quality and frequency of ADS-B messages to avoid lost aircraft, but also less dramatically to solve much more common day to day problems - like trying to estimate when a heavy aircraft is actually going to arrive versus the scheduled arrival time at an Airport when coming over a long ocean flight for example. The airport likely won't know where the plane is exactly, until ATC sees that plane on radar or receives ADS-B data to confirm locally at that airport. This can be really useful to know a plane is 10 minutes late or early to help shuffle things around at gates for example.

This will be worked on with additional terrestrial receivers, as well as satellite receivers. [https://aireon.com/](https://aireon.com/) Is a prime example of the industry effort to collect more ADS-B data. This effort is costing billions of dollars, and each plane needs often millions of dollars of retrofit to support this. It'll take time for this to roll out - with typically more priority for international flights that need that extra coverage.

Second - the Black Box - this contains much more data about the plane than ADS-B would communicate, and it has a lot of value outside of incident reports. - There are efforts to add data cables to the gate umbilical that is used when an aircraft is at a gate so diagnostic data can be pulled off the aircraft and used for all sorts of beneficial scenarios, but this is typically something like a 10GBe Fiber Optical connection that tries to extract the data as quickly as possible while the plane is at the gate, and there could be a few TB of data to extract. This isn't easy, and not always available at many airports.

So to put it lightly - it's complex. Aireon in part was supported by ICAO so we could get some additional data on airplane locations, but keep in mind, many of these satellite constellations are less expensive low orbit units, and not geostationary ones, which means that the time the aircraft can be in range of line of sight to a microsatelite is limited. Still useful, but limited, and many aircraft may only see one of these satellites reliably to communicate once and hour, or perhaps a little more than that - but we are a long ways away from having a once a minute datapoint from all aircraft worldwide. And even farther from the TBs of data coming from each plane in flight, of which we have typically 6000-10000 flights in the air throughout the day.

That said, there is a lot of data out there, and if you'd like to look at it, check out this free ADS-B project and dataset:  [https://opensky-network.org/](https://opensky-network.org/) (And specifically look here:  [https://opensky-network.org/network/explorer](https://opensky-network.org/network/explorer)  )
Qantas have extensive real time monitoring data streams and have a huge amount of machine learning and other things running over the top of the data consistently looking for defects etc. they also download all the data after each flight and ingest it into a data lake which is enormous. 

Source : worked for qantas.
In computer engineering, there is actually not physically possible to do anything in "real-time", as you call it. There will always be delays due to hardware limitations, the algorithms processing and handling the data and of course bandwidth (aka signal processing). In other words, several layers of technology are communicating and dependent on the layer "below". 
Also, as someone here pointed out, there is a huge amount of data collected (telemetry and parameters, voice recordings etc.) , and it would be impossible to stream it all in "real-time". So instead, they prioritize important information the air traffic control needs in order to coordinate the air space. They send this data on low frequencies which results in a low bandwidth, but has a huge transmittion range. Otherwise we would need radar stations everywhere... And even this minimum information is delayed, so I believe the algorithms at the air traffic control are using interpolation and extrapolation to calculate and predict future data based on earlier data, and then corrects itself if needed when the real data comes in from the plane. But I haven't personally looked at these protocols, so I can't say for sure, but it's a fairly common concept in, for instance multiplayer games over the internet.

So yeah, that's why we can't just stream to a remote area. It would require a fairly complex system in order to work. And money is a huge factor when it comes to this. Some questions  have to be asked and answered. So one would have to do a cost-benefit analysis (CBA). First of all, is the investment worth it? What's the benefits of such a system? Does the current black box system work? And so on...
Obviously, we don't find it worth our time and money to improve it, because the current system works. As long as we find the box...
The biggest issue is the amount of data. One rolls Royce engine can generate terabytes if data. The F22 has a wire harness made out of optical fibers because copper simply didn’t have enough bandwidth to carry the amount of data that plane generates. Even if there were a bunch of multi band antennas at x-band (7-11ghz) our max throughput on one channel is 1-2gbps depending on data protection which needs to be high. The plane frequently generates much more than this. The thing that’ll get us there is laser data transmission. 900nm doesn’t absorb super well in atmosphere so you could use it for long range data transmission at insane rates. (Frequency is about 300ghz) and that could do it but the infrastructure will be highly complex and the range would still be lower than VHF and KA band which is what most planes use today
It would cost a lost of money.

Why would airlines (an industry always struggling to keep profit margins up) spend money on something that is not government mandated, and has no potential to increase revenue.

&#x200B;

Secondly to whom would you send the data?  I imagine you ask this question in relation to the Iranian government not giving up the black box data, but would this help?  Does the data go to the manufacturer?  The government of the manufacturer?  The airline, their government?  The country of origin, of destination?  Would require a good deal of international cooperation to get this fully fleshed out.
A lot do.  On our aircraft (fairly large advanced helicopters), all system and navigation information is transmitted every 2 minutes.  The information is protected, and not even management can access it, but certain individuals can access it under specific circumstances.
Because of the cost. I'm currently working on a few digital products for one of the bigger aviation companies and I couldn't believe how much would they rather keep the money than improving the experience for the crew and passengers. The stories they've told me in user interviews were downright scary and makes you wonder how there aren't more accidents.
One of the possibilities when considering designing something like a real-time streaming solution would be that doing so would require additional hardware that would likely need to mounted externally (i.e.: antennae for transmitting or receiving signals).  As soon as you damage _that_ subsystem, your streaming solution stops working.

As such, you can't rely on a streaming solution to always work, in which case you'd still need a blackbox recorder as a backup anyway.
Something like this probably will happen eventually once these internet beaming constellations like Starlink are online and affordable. Right now, the bandwidth and equipment is much too expensive for the very few times it might have been useful. 100,000 flights happen every day globally and only a handful of accidents over the last few decades where this capability would have made a difference
I’ve work extremely close with the black box manufacturers, FAA, and air frame manufacturers.  And the best and most true answer is Money, It is extremely expensive to develop and maintain such a system. So the easiest solution, keep status quo until the laws force you too.
I work in data science. 

Real Time is already a pain for companies as they need quite an infrastructure, internet connection and the software to actually handle it. 

I doubt planes could do such thing as they need to stream a lot of data, have some zones where they can't contact any tower and be detected by one. 

Trains on the other hand are much more likely to have a live stream system (see Trenitalia and SAP S/4 Hana).
The reason is economics, not science.

Cost v. benefit. There would be a high cost to implement such a system wide change. For very little benefit. The current system works very well for its purpose save a few outlying but highly publicized cases. Since the demand is so low, the price the market will dictate does not exceed the cost.
Seems like a no-brainer, but that's a LOT of data... per plane... per second of flight.  It all has to go somewhere and be stored (for how long?).

ADS-B broadcasts are one thing, but a full stream of every black box would require a lot of back haul infrastructure.
High cost, little to no benefit. You'll always have to have a blackbox anyway, because your hypothetical streaming infrastructure (or the link to the same) can go down. So it'll only really help in cases where the blackbox isn't recovered, and that's exceedingly rare.

And don't underestimate the cost either, we are talking about a system that has 100% global coverage, high bandwidth (there are up to 20,000 planes in the air at any given moment). high reliability, high storage capacity etc.

Not to mention administrative headaches, like who gets to control the databases etc.
Mostly it's a situation of cost vs benefit. There are a LOT of airplanes in the sky at any moment. Black boxes are storing several minutes of audio, system logs, the settings on every control, etc... And it only stores a few minutes, writing in a constant loop so the memory requirements are limited. If we were streaming all of that data, that is a LOT of data. Think Twitter levels of streaming data ingestion. Or more.

All that data would need to be streamed over satellite uplinks, taking a lot of capacity from other, more profitable uses of that infrastructure. Most of the data would then be thrown away as we need that data only very rarely. And when we need it, we don't need all of it, just the last minutes. And the number of times we can't recover the data is even more rare.

So we would be going to a huge effort to recover data that is nearly always available anyways. So the actual benefit is tiny to the point of being non-existent.

Then realize that in the times we most need it are the times the data uplink is most likely to fail. So under practical circumstances, we would be less likely to get the data we want, or still need to recover the physical storage device anyways.

So gains are functionally nil (or negative) while costs are tremendous.
Perhaps they should be able to be remotely triggered to send data back.  This gets around the issue of every plane sending all it's data at once, and reduces the signal to heartbeats.  It still allows the data to be retrieved before the black box has physically been collected.
  
On the negative side, it would require the black box to have a transmitter and battery backup capable of sending the data after the rest of the plane has stopped working.
Would you be able to give me a brief summary of why the bandwidth is an issue? I imagine almost all the data a black box receives is quite simple (a number as a temperature reading once a second, for example), which, compared to most modern data transfer, takes a minuscule amount of bandwidth to transfer. Is the issue that the bandwidth is so limited that even these tiny transfers are an issue, or is it the case that (some of) the data streams are so large even modern bandwidth is too narrow?
Because we don't have effective LEO Internet constellations yet which would permit streaming all recorded parameters.

They \*do\* stream GPS coordinates... if you design them to, and pay the subscription fee.  The benefit of this is that searchers have an easier time, and the cost is all on the airline - so unless required by law, it's not really economical to pay this extra money for no benefit.
Frequent flight status transmissions are possible and are being increasingly adopted. The system uses radio downlink where available, and satellites such  as INMARSAT in remote areas. 

The protocol is called [ACARS](https://en.wikipedia.org/wiki/ACARS)
Because rolls royce engines have a good tracker and are constantly updating in real time every measurable variable during operation. This is all live streamed to rolls royce HQ where there is someone monitoring them all the time. 

I think the real question isnt that enough to track and monitor commercial airlines
When I did my engineer capstone project in school I helped a team develop an emergency deployable glider that contained a secondary black box recorder. The glider would capture images of the plane as it deployed and safely navigated to the ground away from the crash site. It would also relay information about the location of the plane. It’s a bit different than just transmitting raw data but I thought it was a good idea. I believe at the time Boeing was developing a similar technology though.
Along with the amount of data and satellite bandwidth/coverage issues, consider that a black box would have much more recent data. According to wikipedia, ADS-B sends updated data every second, and it's sending that limited data set. A local recording system can not only handle more data, but may be able to record after a 'damaging event' that isn't immediate total destruction - for example, it may be able to record the last actions the pilot took, which sensors failed, etc. - even if the comms are inoperable.
Maybe I’m underthinking this.  But there are 87,000 flights everyday, in the US alone. (According to Google).  That’s an incredible amount of data to handle.  Especially given that in 99.9999% of the time it’s meaningless information.
It wouldn't need to be streaming constantly, but it should start a stream when any parameter is outside its normal values, or if the pilot presses his new "emergency stream" button.

This solves the problem with "It's too much data to handle if all planes would constantly send everything".
In an episode of Doctor Who, the black box was reconceived as a home box for spacecraft--it flew itself home to make sure the flight data was preserved. Scifi? For now. But with our drone technology improving, maybe in 100 years its an idea that could be considered? Even if it doesn't fly to a location but just ejects and lands safely nearby.
Because satellite internet can't do anything in real time, and having a plane talk to 1 or 2 satellites is really hard and there's no network on the ground a plane can reliably stay in contact with.  Radio networks lack the bandwidth.

However if starlink is a success when SpaceX launches it, this could change everything.

SpaceX will likely engineer a satellite array specifically designed to be installed on aircraft.

This would give every aircraft in the sky access to one gigabit per second of Internet bandwidth at less than a hundred milliseconds of latency.

SpaceX's starlink could really change everything.
As someone who works in the realm of networking I would assume it's because there just aren't persistent communication points available (satellite is out of the question because you have to aim for it, planes fly too high to reliably pick up cellular comms).  You might be able to design something that once you get back into cellular range - it could backfill data.  


One day you may be able to build some type of mesh network out of planes over populated areas but they'd still be unreliable over the ocean.
it is more stuff that can break, think about your hammer, you dont have a screwdriver in it or have it as a ham radio, simpler systems don't break as easy, the same reason that survalence video is in low quality.
 Innovation is a dirty word because the overall safety record in airline transport is so fantastic.  If it is not broken, don't fix it. 

Aircraft the world over still use garbled AM radio over a shared frequency that distracts pilots a hundred miles away.  ADS-B is a big step forward, but it is not a great technical solution.  For one thing it is not encrypted or authenticated, so it is easily spoofed by a malicious player.
Because manufacturers aim to make money and comply with government regulations and no more. If it isn't a feature that contributes to the bottom line, they won't include it. You can't blame a business for wanting to make money.
There is no technical limitation to what you suggest. It is just overkill. For the 99.999 (how many 9's?) % of flight that take off and land safely, such an exorbitant use of bandwidth is simply waste. Also, oceans cover a big part of the world - I don't know if you appreciate how MUCH area this means. Using satellite transmissions is just expensive.

So there won't be too much incentive or need for what you suggest to happen. It isn't as though this system PREVENTS crashes so ultimately all this cost is just diagnostic.
To an extent, there is. Most airlines participate in Flight Operations Quality Assurance or FOQA. If certain flight parameters are exceeded they are recorded. We’re talking things like heading, airspeed, bank angle, descent angle and rate, thrust lever angle and autoflight modes in use. However things like voice recorders most likely will not be transferred in real time due to privacy concerns.
This is called bioregenerative life support, and in falls in the broader category of Environmental Control and Life Support Systems (ECLSS). Short answer is yes, but not infinite and not without disadvantages.

Note that plants aren't the only way to recycle oxygen. Currently they use water electrolysis to inject oxygen into their atmosphere, and use the resulting hydrogen to apply the Sabatier reaction with CO2. This yields methane as a waste product, and captures oxygen from CO2 in the form of water, so they can later apply electrolysis again and recycle oxygen.

A clear advantage of using plants would be trapping carbon into edible forms, so you'd be recycling not only oxygen but also food. However, not all plants have a 100% edible mass, and those who do usually offer very few calories if not negligible (e.g. lettuce).

A problem with plants is that they could die under some conditions, so this system isn't 100% reliable. And in any case, whether bioregenerative or artificially regenerative, you can never achieve a 100% reuse of ECLSS resources. But it's ok, you can get close enough and have them for a long time relying on very little supplies.

They are actively researching on this topic. Recently they've been able to grow lettuce in the ISS.

Whoa something I can answer - I worked on the Lunar Greenhouse Module at the University of Arizona! 

Bioregenerative Life Support Systems could continually produce oxygen and some amount of food (our goal was always atmospheric conditioning with food production as a windfall of that). In fact, you kind of have to decide if you'd like the plants to focus on growing leaves for oxygen production or fruits for food production. Either way, there are a lot of challenges in feeding those plants. We proposed using composters to recycle human waste to make the system fully sustainable, as plants "breathe" pure water out which could then be utilized as drinking water as well. I don't know all the details about that part of it, as that wasn't my section. We also intended to use Fresnel lenses with fiber optic cables to deliver sunlight directly to the plants, with blue and red LEDs as a backup (it'd be placed underground because the moon lacks an atmosphere to burn up space debris).

Unfortunately, hydroponics would have difficulty in a low gravity environment like the ISS without some real specialized design, which the plants may not like. The moon would give us an environment that is a little more familiar to us and the plants. 

Edit: the guys in charge of it did an AMA a couple years ago if anyone wants to look around for more info without leaving Reddit: [Link](https://www.reddit.com/r/IAmA/comments/3npxoj/move_over_hollywood_we_are_roberto_furfaro_and/?st=J8LQQOVW&sh=91652fb3) 
Plants are very finicky. While having them in space would be the most logical way to produce life support, it is difficult because of a lack of gravity. While we have been able to grow sunflowers aboard the ISS, the lack of gravity showed some problems in their development.

Plants require gravity to tell then which direction their roots should grow. Obviously, down means soil. Without gravity, plants become lost and will have more difficulty growing, resulting in limp produce.

While there has been some success, growing plants in space is still unreliable, but will improve in the near future.
You would probably be interested to read about Biosphere II. In this experiment, researchers attempted to create a self contained eco-system including humans. It was completely sealed off from the rest of the earth.

Unfortunately, it failed as they were not able to replenish the oxygen quickly enough.

This doesn't mean it's impossible, but it didn't work previously.
Shockingly (/s) it didn't turn out to be quite so easy : https://en.m.wikipedia.org/wiki/Biosphere_2
In terms of "garden" you'd be talking "another iss". The experiment to grow food on the iss also was a bit trickier than intended (more time than expected, process revised)

1. You need about 5 large trees (or equivalent in other plants) to supply oxygen for one human.
2. Plants grow awfully poorly with no gravity. They managed to grow some lettuce, flowers, and other plants on ISS, but the success rate, yield, general health of the plants was way low. You'd need a centrifuge to simulate the gravity.

They also need water, sunlight (or equivalent artificial light), right temperature, soil, and so on.

In short, the kind of project to get it into orbit and get it working would cost about 10x the cost of ISS and be more complex than ISS itself. Yes, it would work. No, it would be completely impractical.

OTOH, cyanobacteria, the kind of bacteria that are responsible for creating most of oxygen on Earth, should be more viable. Creating devices similar to solar panels, but being essentially thin, flat greenhouses, to keep water with cyanobacteria, circulate oxygen and CO2, keep the panels aimed at the right angle relative to the Sun to supply the bacteria with enough light but not boil them, heating to carry them through the night side, etc, could be a thing - structures similar to solar panels of ISS, but taking care of oxygen needs instead of producing electricity would be an option.
If you find questions like the interesting you should check out The Expanse series (book and now TV show on syfy). It's a "realistic" space opera set a few hundred years in the future after we have colonized the solar system and it describes a ton of (fictional/hypothetical, obviously)  stuff like this. 
A recent post showed that photoplanktin is responsible for most of our oxgen.  This is a surface area problem, as those things like hovering around the surface. 

It may be possible to create some kind of ecosystem to sustain photoplanktim growth with bio waste from the astronauts, but this would probably require lining the entire hull with a water layer to have enough surface area.
During World War 2, German engineers tinkered with a concept for a space station that relied upon greenhouses filled with pumpkin plants to replenish oxygen for human passengers. Apparently pumpkins are among the most efficient plants in terms of converting CO2 into oxygen. Whether or not such a space station was viable is debatable. 

The primary source is [behind a paywall](http://content.time.com/time/printout/0,8816,852344,00.html), but I wrote at length about the concept [here](https://www.damninteresting.com/the-third-reichs-diabolical-orbiting-superweapon/). 
The Biosphere 2 experiment was created to postulate this question.  What came to light was that Oxygen gets absorbed by everything(Soil, metals, clothing, water, other biomass) and was not sustainable even by the most oxygen generating plants.  Even worse, there wasn't enough biomass available to feed the plants the CO2 it needed to create enough oxygen.  Adding more biomass would have just used up more oxygen.  Ultimately, it was seen as impossible to create an environment organically without human intervention(pumps, filters, computer regulators).  

If you could create the perfect system for 1 person, it would need 7-9 full grown trees just for the oxygen, or your typical backyard swimming pool sized algae farm.

https://space.stackexchange.com/questions/2218/oxygen-self-supply-by-plants

https://www.youtube.com/watch?v=-yAcD3wuY2Q


[removed]
Short answer yes.
Longer answer is what you are really seeking is to create a fully self sustainable biosphere. Basically you would need a proper balance of energy (sun) with a practical balance of different life forms and nutrients and everything in a mini Earth. On the ISS significant modifications would be required to allow it to be 100% self sustainable and not required people to replace solar panels or light bulbs or whatever other artificial systems are in place (heat being a big one).
Others have detailed the feasibility/reliability of this process, but a major issue (at least in long term applications) is the lack of nitrogen produced. Most of the air we breathe is nitrogen, without the correct proportions the over represented oxygen becomes a fire hazard. Humans don't do well with too much of anything, oxygen included. A simple solution is to bring nitrogen (I imagine it can be synthesized in some way or another, not too sure about that side of things) but then you can't really say you have "unlimited" breathable air. Obviously producing oxygen is better than bringing it with you, but there are still major downsides to consider. 
[removed]
Well I’m not exactly qualified in environmental control, biology or deep space exploration theory. But having a rudimentary understanding of energy, chemistry, and biology; absolutely plants would provide oxygen. However. Plants need to consume (outside various minerals) Carbon and nitrogen. Well soil won’t have an infinite supply of nitrogen. Okay what about hydroponics? Sadly no. Even nitrogen in water can be exhausted. So the first problem comes from limited nitrogen. Secondly, so plants are supplying you with oxygen. They don’t live forever, when plants die they will return a portion of their energy and nutrients back into the soil. However. It is impossible to have a perfect transfer of energy. Between being in the soil, absorbed by the plant, constructed into the mass of said plants, finally being broken back down into the soil. They over all density and quantity of essential nutrients would decrease over time. Plants also have the habit of producing, methane along with oxygen. Specifically when they decompose. Obviously this best demonstrates the loss of matter. Humans can’t breathe methane. (Not in abnormal quantities).  It’s an interesting though experiment. With research and conjunction with people who’s job this actually would be interesting. 

What about plants growing in zero gravity? There’s an easy solution in my mind. Centripetal Force. At a rate that simulates the rate of gravity on earth. But how to power such a system? Carbon fuels wouldn’t do the job for long. Solar? Potentially. But very costly and not yet ultra efficient. Nuclear? Maybe. Could a nuclear reactor even be created in space? It’d be a daunting, impressive challenge to overcome. Considering the limitations of modern reactor design. Limiting it to the dependency of water. 
Reading a lot of these responses, I can't help but wonder how fragile the oxygen balance in our atmosphere actually is.  Since it's hard to replicate in a closed system, how easy would it be for too many/too few plants or too many/too few humans to throw off the balance and cause an extinction event?
In theory, yes, but there is still much research to be done on building sustainable enclosed biotic systems. Scientists already tried this with the Biosphere 2 experiment in Arizona and while it started off well, it failed rapidly. By the end, plants began to die, and based on what little I know from reading about the project, it's theorized that it was due to a lack of biodiversity of microorganisms, insects, algae, funguses, and other small life forms. All of these factors are detrimental to the long term survival of an enclosed system, and it would require lots of work to maintain....Work that wouldn't necessarily *guarantee* long term success of a system like this. 

Economically speaking, it is much more efficient to use a rebreather or chemical oxygen system. Oxygen is the same regardless of how it was made as far as our bodies are concerned, 
I've occasionally wondered about the concept of building a massive generation ship for long-distance space travel, large enough to contain its own large ecosystem. Does anybody know if research has been done on something like this?
I thought plants also consume oxygen when they are not photosynthesizing (ie. at night). Is that minimal compared to how much they create and would plants need a day/night cycle? 
If mold can grow out of control and be quite deadly in space then why not find a plant life that can do the same yet is beneficial at producing oxygen? I am thinking some kind of moss perhaps unbound by gravity it might excel better than other plants.
[deleted]
[removed]
[removed]
[removed]
The easy answer is no. If you mean combustion (or burning) of the bread, then there would be less calories because once combustion occurs (even partial) the byproducts are either indigestible or barely so. 

If you mean dark toast, the kind you might get at 6 on the toaster, it has the same calories. The Maillard reaction is what drives browning and it is a complex process where proteins denature and bind to other proteins as well as carbohydrates and so forth created an amalgam of mixed molecules. Essentially this is what leads to that caramel/nuttiness you get when things are browned. However, this conformational change and denaturation does not decrease the calories because the overall building blocks are the same and still digestible.

However, if let’s say a byproduct of a Maillard reaction is an indigestible molecule that was previously digestible, you could argue that it is now lower in caloric value because it is no longer bioavailable energy. 

Side note, a lot of people are talking about measuring calories by using a bomb calorimeter aka burning the item. This is no longer the method used for finding caloric value of food. Instead they find the net average of Atwater bioavailable nutrients and then use standardized values (e.g. 4 Kcal/g for Carbohydrates) to calculate the assumed caloric value. Again, this is obviously dependent on bioavailable sources of energy, not overall stored energy.

A perfect example of how a bomb calorimeter is not a feasible option, is Lettuce. Excluding the water (which is 95% of the material) lettuce is primarily fiber. Insoluble fiber in this case or in other words fiber we cannot breakdown (Cellulose). This material has no caloric value to us because it is not bioavailable (aside from small amounts created by gut fermentation thanks to helpful bacteria). So a piece of lettuce has a net caloric value of basically 0 in the Atwater system. In a bomb calorimeter however, it might have a much higher value because inside each of those cellulose walled cells is stored sugars, proteins, and so forth. Additionally, cellulose is essentially a starch made up of Beta-Glucose, however Beta-glucose is in a different conformation than Alpha-Glucose in starches we digest which means it is incompatible with our enzymes. However, combustion wise, cellulose and amylose (Alpha-glucose polysaccharide aka starch to most people) are equivalent in “Calories” in the context of a bomb calorimeter.  

Again, this is not the case in bioavailability. The only animals that can actually get the full caloric potential from plant material are foregut fermenters and hindgut fermenters, aka Cows and Horses. This is why they need multiple stomachs or a large cecum, in order to host helpful microorganisms to breakdown cellulose. Even Termites are not able to digest cellulose, but usually carry symbiotic organisms that can. 

https://www.scientificamerican.com/article/how-do-food-manufacturers/

Addl. Note: /u/chuggsipas pointed out the fact that to be totally accurate about this discussion we have to really highlight that a Calorie at its base definition is the amount of energy required in order to raise one gram of water, by one degree Celsius. It’s important to distinguish this because while I do mention that a bomb calorimeter is not used for nutritional labeling values, it is the correct way to calculate calories in its true context. Another thing chugg brought up, and I absolutely agree with, is the fact that nutritional calories are a terrible measure of how our body uses energy. We do not just ingest and combust whatever is bioavailable, there are a multitude of processes that are dedicated to metabolism, storage, availability, etc that are not taken into account by flat caloric values. In fact evidence builds every year that quality of foods and caloric sources are more important than the overall calorie value. However, on some very basic level you can get a vague idea of your energy intake with the Atwater calorie system. 

Edit: Added some clarification in regards to glucose in Cellulose.

Edit2: Fucked up and did L/D-Glucose instead of Alpha/Beta. Corrected that :X 

Edit 3: Just wanted to say thank you to anyone who challenged or questioned anything I wrote. I definitely needed to add some information and make changes here and there. I appreciate it, especially since that’s what healthy discussion is about, and no one can be 100% correct, 100% of the time without some input from others! 
[removed]
To go along with this question, as a banana goes from green to yellow to brown it gets sweeter because of I assume sugar.  Does a yellow banana have more calories than green-yellow banana?  I've always wondered.
Heating up food enough to cause a chemical change (toasting or burning) reduces the total caloric content. However, the heat also tends to make those calories more accessible by breaking down the sugars so your body is more likely to absorb more of them. 
Wow, so much misinformation on this. A calorie is a unit of energy. You can measure it by taking a piece of food and putting it into a bomb calorimeter, incinerating it, and measuring how much energy is released. So if you burned a piece of toast, you've already already burnt it and released its energy. Therefore your number of calories in the toast is less than the original amount of calories in the unburnt toast.  

Any answer that talks about digestion is completely incorrect, which is why calories are such a controversial measure for food energy. It doesn't take into account digestibility.  To make calorie counting work, you should also be measuring the amounts of material you excrete and incinerating it to see how much unused calories have just gone through your system.  Even then you're missing the fact that some calories in food are more difficult for your body to process and consume more energy to digest.

So by burning the toast, you have reduced its calories by definition.
[removed]
Burnt has less. Lightly toasted has more.

Generally cooking food slightly increases its calories which is why cooking was a useful invention for us.

It's also why we've been doing it long enough to have evolved to have less tolerance for raw meat and a better time processing cooked foods.

Part of the energy you gain from food gets spent processing raw foods. If it's cooked, your digestive system has less work to do. Less calories spent, higher net caloric intake from the food.

I don't know how much the difference is but I can inhale a medium rare ribeye. But if I eat the ribeye raw, as I often do (merely buying from a butcher, removing the paper, seasoning and eating raw) it takes me a lot longer and by the end my jaw is extremely tired etc. That's to say nothing of the extra internal digestion that must occur.
Basically to count calories they take that piece of toast and burn it completely and measure the amount of energy released. That is its calories. So if you burn some of the toast you have released some of the calories.
As a breakfast chef, and a chemical engineer grad, I've burnt a lot of toast.  

Bread to toast and burnt toast. 

Energy content of bread/Starch/white > toastedBits/caramelisedStarch/brown > burntBits/carbon/black 

And then if you really burn the crap out of it you'd have CO2 and you're contributing to climate change.

Maybe there might be a slight increase in the specific energy, calories per unit mass as water is driven off but the energy per slice would definately be less.

And finally the only energy put into the bread by the toaster is heat, which is activation energy for the Malliard (carmelisation) reactions and the combustion when it burns. Hot toast would technically have slightly more energy than cold toast, though that's not relevant for dietary calories as it will only burn your mouth. There's no endothermic chemical reaction happening.
Technically, burning organic material reduces the available calories. Yet a partially burnt (cooked) meal is one of the defining traits of humans, helping our species to consume more overall calories by making more previously tough and parasite filled food sources softer and cleaner.
Heating up food enough to cause a chemical change (toasting or burning) reduces the total caloric content. However, the heat also tends to make those calories more accessible by breaking down the sugars so your body is more likely to absorb more of them.

No.  Calories are still determined by burning a sample.  Since some of it is already burned, it would have fewer calories.  Sadly, this 2 century old method of burning food to determine calories is still used, even though most people digest their food with chemicals, not fire.
Related to this, I remember in high school burning a marshmallow to determine its caloric content and wondered then as I do now, if there is really a strong correlation between how much heat something gives off and the number of calories the body derives from it. I believe there are things, like wood and coal to name obvious substances that burn but nonetheless have no actual calories in the sense that a human eating them would get no energy or weight gain from doing so.
This, i feel qualified to answer. 

The burnt part of the toast is literally burnt. So, that is fuel spent. 

We measure calories as the amount of heat generated when we burn some thing like, say, toast. 

If that blackened part of the bread is present, then that means fuel was spend to generate a black layer of carbon. 
That means less calories for some one to take in. And, we can't use carbon as food. It passes right through us in small quantities. 

Its not a very elegant way to say it, but what i said is true. I'd speculate its likely a negligible amount of calories lost, though. Now maybe a nutritionist or chemist can comment about how much caloric loss actually occurs. 
[removed]
No. The most basic explanation is that a calorie is the amount of energy it takes to power your body. So if you think about lighting the bread on fire, as it burns, the bread turns to ash, which means all of the calories are “spent” (aka: broken down). So if you have two of the same sized piece of bread and toast one, that one will be “partially used” which means your body won’t be able to use it for energy. 

That said, the actual difference probably doesn’t amount to more than maybe a 1 or 2 calorie difference, but you’d need to run an experiment blah blah blah. It’s certainly not enough for you to notice if you ate bread vs toast in the morning or if you’re calorie counting. 
Burning Fat isn't just a coined term. Our body actually burns the food (well, oxidizes it, but it looks exactly the same when you write the chemical equation down). So burnt food has fewer calories then unburnt food because it has been oxidized, so our body doesn't get as much energy from oxidizing it again. 


However cooked food has more calories then uncooked food because our body is better at digesting it. 
[removed]
Hello everyone.

Just a reminder, [medical advice](https://www.reddit.com/r/askscience/comments/s4chc/meta_medical_advice_on_askscience_the_guidelines/) is not permitted, whether you are asking for it or providing it. Please try to avoid questions regarding your travel plans or how to handle potentially infected materials. If you are unsure of what to do, please contact a qualified health professional.

Thank you.
[removed]
Live map of the virus

Source - Johns Hopkins University’s Center for Systems Science and Engineering

https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6

Edit: source
Why has this epidemic seemingly (from even the very early days when only a dozen or so we're infected) been responded to SO fiercely and described as so dangerous? Compared to SARS and the avian and swine flus, this seems like it was understood to be apocalyptic. I don't recall clean room people movers and PPE suits with only a few hundred sick.
It seems like we get a virus scare every couple of years, the last one being Ebola. Is this one any worse than previous viruses?
[deleted]
For the average healthy person what risks are there?
What is the official incubation period (time from infection to manifestation of symptoms) for this novel coronavirus? For other coronaviruses it's about 2-3 days, and for SARS it's 3-5 days. I've seen some reports of this one having a longer incubation period (at least a week) but I don't know how reliable those are.

This is epidemiologically significant because the longer the incubation period is, the more difficult it's going to be to control spread.
[removed]
Were entire cities quarantined this quickly during the SARS outbreak in 2002/2003?
Is there a reason so many novel diseases seem to come from China? Is it a population thing - causing or spreading? Is there a hygiene issue? Or is it just that these diseases get more press?
This Lancet article just came out which tracked the outcomes of the first batch of patients to have the disease. It says all confirmed cases were admitted to hospital and 15% of them died. The first doctor death has also occurred.

Given this, do you still think it's likely this virus is less deadly than SARS? Or is it unfortunately comparable?

https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30183-5/fulltext
The Coronavirus is 89% the same as SARS in a gene sequence. What exactly does that mean? Would the virus behave similar to SARS?
[removed]
What are the most common ways the virus spreads? What can we do ourselves to prevent infection?
Who can get infected with the virus? Are dogs and cats venerable as well?

Also can migrating birds carry the virus to different parts of the world?
How do people actually die from this? I’m guessing people’s lungs shut down, but how does that happen? 

If a young, otherwise healthy person contracted the virus and started experiencing severe loss of lung function, could they be placed on a respirator until the body fought off the virus? How did the prior health issues exacerbate the effects of the virus in the fatalities we know about? 

I can’t find anything by simply googling about how the virus works and what measures providers can take to help someone survive if they seem to have a bad case, and I’m curious about what it looks like practically.
Any sense on how the virus might affect pregnancy? Coronaviruses seem to have wildly different impacts depending on severity and I can’t find any information on this one yet.
How long does the virus remain suspended in the air? How long does the virus survive on surfaces?
How quickly can it proliferate/how close is a cure?
Are all kinds of N95 masks adequate for personal protection? 

Just wondering because I was going to purchase some medical spec/cdc approved medical masks and just wondering if maybe that’s a bit over kill and regular N95 masks are just fine.
What is special about the corona virus?
I understand that it’s new, but so is every year’s flu virus, isn’t it?
This might be stupid, but how do doctors (say in the US, not near the epicenter) test for this specific virus? Do they have to swab and take a super close look at it and then just compare it’s characteristics with what China has reported?


I got the flu this week and my flu test took like 15 minutes from my nose to being positive and a doctor telling I have it, but this is new so I don’t know how they know it’s the Wuhan virus without it being like, “in the database” I guess.
When was the first case reported? My husband and I went to Macau in late October 2019 and connected through Taiwan. About a week after we got back we both got extremely sick. At the urgent care center we were tested for flu and streptococcus (both negative) before getting chest X-rays. We both had some lung inflammation and they prescribed antibiotics thinking it was pneumococcal pneumonia. The antibiotics made no impact and I couldn’t get out of bed for 3 days after that doctor visit. It was like the worst flu of my life. At the urgent care center (in the SeaTac metro area) they told us we were the fifth family to come in that week after recently returning from China extremely ill from a mystery illness.
What's the incubation length and is it contagious before symptoms begin to show?
 What is the mechanism of this virus?
How do we know this virus is genuinely "new?" Is it possible that it has been knocking around for some time and this is only the first opportunity we've had to identify it?
Chinese colleague said more young people are dying from this, and at least one person was under 40. Any truth to this?
Is it way worse than China is leading on?
How does the virus spread exactly?
Do any of you think this may be similar to SARS, since both were a corona virus?
If someone hasn’t gotten their flu shot yet is it still worth it to get? Would it also help at all with the coronavirus? Or even just to prevent them from getting both the flu and the coronavirus at the same time?
[removed]
Realistically, how worried should we be? On a scale of 1 to 10? 1 being Keep a few masks laying around and carry some hand sanitizer, 5 being start making plans to limit how often you leave the house and stock up on food and bottled water before any cases reach your area, and 10 being get the guns ready and pack a survival kit and head out into the sticks or at least barricade your doors and windows and don't leave for anything?
I work as a vet tech and we vaccinate animals against coronavirus. Is it the same virus? If you can vaccinate dogs, can you also vaccinate people against it?
[removed]
[removed]
1) How much of the knowledge of the virus comes from independent sources rather than the Chinese Government?

2) Is the Virus much different from standard coronaviruses?

3) I saw videos of extreme fevers, seizures and nasty cases of pneumonia where they spit out lots of blood. Does it sound right?

I also heard reports of the viruses down regulating your immune system as well. Credible?
is the fear, that Chinese citizens and visitors traveling around for the celebrations could help spread the disease even faster, justified? should they really just stay put until it's dealt with?
Are there any podcasts out on current events in China? There's so much going on I could use something like that to digest all of it.
A lot of people seem to be doubting what China is reporting, yet the CDC/WHO appear to be saying that China has been way more open with this outbreak. Naturally I lean toward what the experts are saying. What are thoughts here?
[BBC source](https://www.bbc.co.uk/news/health-51048366):

> "If we think about outbreaks in the past, if it is a new coronavirus, it will have come from an animal reservoir," says Prof Jonathan Ball, a virologist at the University of Nottingham.
> 
> Sars started off in bats and then infected the civet cat, which in turn passed it on to humans.
> 
> And Middle East respiratory syndrome (Mers), which has killed 858 out of the 2,494 recorded cases since it emerged in 2012, regularly makes the jump from the dromedary camel.

How do scientists backtrack the path the virus has taken in order to narrow it down to a specific animal? That seems crazy to me.

Follow-up question that's slightly related;

Is the movie trope of 'Patient Zero' a real thing? Can we actually narrow down an outbreak to an individual person? And a really, really stupid question but also something I'm unaware of; does 'Patient Zero' typically suffer from more\less intense symptoms or differences also as seen in lots of films?
Is there a possibility to be infected if you buy things that's from China and shipped over? 


I'm genuinely curious.
Should we be treating this any differently than a nasty flu? Maybe the hype is a little over-hyped?
What information has China actually release to CDC and can we trust it?
Are our animals at risk of catching this disease? I know dogs are usually vaccinated against Corona virus.
What kind of things should one put in their emergency at home kits?
[removed]
How do doctors/scientists realise a new virus is something new?

Symptoms from what I've read are almost identical to the Flu, so how did people realise this was something different?
Won’t it be a good idea to not allow people from China to other countries until this is fixed?
I read that healthy people won't die from it, and then I read that several doctors in Wuhan already died. What gives?
[removed]
Why aren't western governments being more cautious? For example, they are not quarantining people who have been in contact with confirmed infected. The only explanation I've heard so far is that "we don't know how dangerous it is yet", and that seems pretty backwards to me. Wouldn't you assume it's very infectious and take precautionary measures until you have evidence it's not? Instead they assume it's safe until proven otherwise. To me it would seem that by the time they know more it could be too late?
Why is there cause for concern? From the infected and mortality rates I've seen, this doesnt seem to be any worse than the flu. New strains of the flu come out yearly and dont receive this kind of attention or response.
Serious question:

Why does the Chinese government always try to cover up outbreaks.
People are saying it made the jump from bat > snake > human > human. I said a reptilian to mammal jump is very odd for a virus. Has this ever been documented with any other type of virus? (Rabies possibly?)
For people with asma, is it more dangerous to contract it? Since it is a respiratory virus.
At what point was the virus tested to determine it was a 'new' virus? 
Why didn't doctors just assume it was an old type of pneumonia/flu? Were symptoms much different than others?
I've gotten sick a few times with viruses in general, and AFAIK they didn't go through the effort to sequence its genome and tell if it's a new strain or not.
[deleted]
Is the media helping or hurting?
Could knowingly traveling to other countries with a virus like this be considered a crime against humanity?
Finally shut down China’s wildlife markets. It was the source for SARS 20 years ago and now this.
Only like 4% of people have died from this virus, why is everyone so worried?
I have to say at this point I think the media is driving the hysteria. I may be proven wrong but they seem to feed off "tragedies" like this whether they are true or not.
[removed]
Is this deadly? or how severe can the symptoms be?
Are there any new cool biotechnological tools (in the making) to attack viruses?
Here is a good publication from 1/24 - a study conducted in China which followed 41 patients of which 6 (14.6%) have died - do note these were patients admitted to the hospital for pneumonia, and it does not reflect the overall cumulative  mortality rate which is much lower. 

Take the numbers with a grain of salt as this is not a large sample size. However the paper does have a lot of extra details about the patient’s medical course and about the 2019-nCoV for anyone who is curious!

https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30183-5/fulltext
[removed]
Stupid question:  How did we manage SARS?  Did it just have to run it’s course or were we able to hasten its demise?
My boss planned his first ever work trip to China, visiting Beijing and Shanghai, in March. Should he be worried? He will be golfing, visiting some flooring plants, and is visiting in Great Wall.  Should he cancel? Is there a worry he will get sick and bring it back to us? He is 65 and he was really excited to go
It is indeed possible to yo-yo in space. The only thing is that if you "free wheel it" (sorry not a yo-yo expert) it tends to float around. It will however try to keep its orientation due to gyroscopic effects. This is sometime used on spacecraft to either stabilise them or to turn them (with moment gyros).  [Here is a great video](https://www.youtube.com/watch?v=ni4j5K4Lz3o) of my favorite astronaut Dr Don Pettit inventing new yoyo tricks on board the international space station.
You don't need gravity to yoyo. Think of how you can throw a yoyo out perpendicular to the ground and have it return.

The way a yoyo works is this: the string isn't tight to the bearing which is how you can walk the dog etc. If you cause enough snap, it starts to wind, then due to the spinning, causes it to wind back on the string itself. Gravity plays no real part in basic yoyoing, only in certain tricks
Yes. A yo-yo's functionality is not based on gravity.  You throw it, the string strains, and bounces back.  To be honest, a Yo-yo in space would bounce back harder I believe.  The gravity would not be affecting the yo-yo's repercussion.
Long story short, yes, it would actually make a yo-yo more effective; or annoying depending on how you treat it.
[deleted]
A yo-yo returns solely when the string catches on itself and rewinds. Gravity has no role in this process.

At times when the yo-yo is spinning at the end of a string without winding, the string is kept taut by the yo-yo's weight and/or swinging the yo-yo, preventing it from catching on itself. Once the string is loosened (by giving it a short tug or moving your hand towards the yo-yo), it will usually rewind.

On cheaper models, there isn't a bearing. The string can be knotted around the axle too tight for it to spin freely, and so may always rewind rather than hang.
Just adding some clarification to how yo-yos return. Tugging the string adds slack to the string briefly, and during that time the string catches on some sort of friction pad on the inner sides of the yoyo, allowing the yoyo to wind back up. Most modern Yoyos have flow able silicone response pads, and older Yoyos such as Tom Kuhns rely on fabric pads or patterns engraved/cut into wood.
[removed]
Yes, You can. If you spin it around while throwing it (around the world if you will,) the centrifugal force simulates gravity's effect pulling away from the fulcrum (the part tied to you hand) and it will reel away. After it reaches its apex, gyroscopic motion retains the inertia for it to wind itself back up. 
Yes, if my understanding of this article is correct:
http://www.nytimes.com/2011/10/25/science/25qna.html 
(I hope nytimes is an acceptable source, even if not academic)

In the linked article, John D. Kubiatowicz, a professor of computer science at the University of California, Berkeley says that for a kindle with 4 Gigabyte memory, this would mean a rough increase in weight of 10^–18 grams.

For your situation of a 60GB phone, this would mean 
(60Gb/4Gb) * (10^-18 grams) = 1.5 × 10^-17 grams.  So certainly less than what you would be able to feel.

While I originally thought this would be because the storage would use extra electrons, the article/Prof. Kubiatowicz say otherwise:

"Although the total number of electrons in the memory does not change as the stored data changes,” Dr. Kubiatowicz said, the trapped ones have a higher energy than the untrapped ones. A conservative estimate of the difference would be 10^–15 joules per bit.

"As the equation E=mc2 makes clear, this energy is equivalent to mass and will have weight."

It is my understanding that most/all mobile phones use flash memory for storage so the linked article about flash memory in Kindles would be applicable here.  Please let me know (or just delete this outright) if I am mistaken.

*edited to fix exponent formatting
 


Most comments here are assuming that "empty" space on the phone is occupied by 0 bits, and that new data will switch some to 1. In reality, there's no guarantee that those bits are 0, since most operating systems have clever bookkeeping to allow regions of memory to be marked as empty without having to overwrite that entire section with 0 bits.

Your meta question appears to be: does data have mass? And the answer to that question, and in my mind one of the cool things about data, is that it does not. 

A knit quilt with intricate, colorful patterns weighs the same as a gray, featureless one. A book filled with random scribblings weighs the same as an equivalent dictionary. We're a species that evolved to match patterns; data is just patterns: human-interpretable arrangements of things. 
With NAND, the opposite would be true.  Flash erases to 1 and writes 0. When you erase a NAND cell, the hardware fills the cell with that highest possible amount of voltage. Writing a page then reduces that voltage. 
There is no clear meaning for a "full" or "empty" phone from an Information Science point of view. The least weighing combination of 1s and 0s could stand for a fully filled storage, simply because that might be the exact information that you want to save. Similarly, the highest weighing combination can be interpreted as empty, as the software dealing with storage information doesn't really bother what state the physical storage is in, it just calls it unused and therefore empty. So this is somewhat about perspectives and what you call full and empty. 
[removed]
From a comp science perspective, IF there is a difference in weight between a bit having a value of 1 and a value of 0, the answer is it depends.

The thing about digital storage is the data is not erased when you delete a file, it's only removed form the "index" so to speak. If you have a 60GB file which is just straight 1s for all of its bits, and you delete it, they don't become 0, they stay at 1 but your file system will set the memory addresses as available for new files to be written to. In the same vein you could have a 60GB file filled with straight 0s. That's why when you delete a file there are file recovery tools that can scan your drive and recover them, as long as you haven't overwritten the data with a new file, it's 1s and 0s all are there and can be read.

It doesn't end there, some chips use inverse logic, where a high (voltage) state means a logic 0 and low state is a logic 1. This is the case for NAND flash memories which most if not all modern smartphones use for their internal storage.
[removed]
[removed]
To be purely pedantic, no.  Because nano means 10^(-9), there is no perceptible change in weight.  A change in the energy levels of electrons is virtually undetectable from a weight perspective.  We can calculate out the possible differences, but they are in the 10^(-18) range of a difference.

In order for the question to be relevant, you would need to double the precision of your orders of magnitude.  To put it in perspective, you would need to take a particle on the nano level and grow it to human proportions.   Then you’d need to measure the difference in nano based weight on the enlarged particle to notice anything. 
[removed]
[removed]
[removed]
[removed]
[removed]
Mechanical engineer here. Answer is no. Data saved on your phone is stored on the existing infrastructure, so if you keep your phone in a vacuum and download a bunch of movies the mass of your phone remains constant. Similarly, your battery is the same mass whether full or empty, the electrons simply move around from what I understand. You can write and unwrite data to a storage device all day long and the mass should stay the same.
In short, the answer  would be yes. But the answer is much longer than that, since it also depends on: "how does your phone store data?" and most importantly "what is data?".

First let's use an example like a hard drive, that uses a magnetic head to record the data bit by bit in the metalic disk, each bit corresponds to a tiny sector of the disk that is magnetized either positive or negative. In this case, the difference of mass from a empty drive and a full drive would be basically none, since there wasn't really a "change of state" in the parts inside of the hard drive (except for the circuits that drive the magnetic head) but the data storage part of it mostly stay with the same number of electrons and overall mass.

Now, if we're talking about solid state data, or data that is recorded in a method that utilizes transistor states, then I'd say that the difference in mass would be less negligible but still it is just extra electrons in the form of stored data, since this time you'd have to "turn on" some transistors in order to store the data. It would also depend on the configuration of the transistors, some my weigh more empty than full if they're p-type transistors.

The difference of weigh would also be so little... I even found this example that was set by a professor, while talking about solid state data:

"Prof John Kubiatowicz, a computer scientist at the University of California, Berkeley, explained that storing new data involves holding electrons in a fixed place. Using Einstein's E=mc² formula, which states that energy and mass are directly related, Prof Kubiatowicz calculated that filling a 4GB Kindle to its storage limit would increase its weight by a billionth of a billionth of a gram, or 0.000000000000000001g."


It would, but the difference would be negligible. For example, you could also calculate your phones thrust from the light leaving the screen, or the weight of the battery charged vs not charged, ect. On that level there’s a lot you could calculate, but in practical terms it is nothing. 
[removed]
According to one of the Israeli health providers, in the week after their second shot, just 20 out of 128,000 Israelis tested positive. This is 0.015%, compared to the 0.65% of the general population who are currently getting infected each week. This would suggest an efficacy of around 95% as claimed by Pfizer. None of the 20 were hospitalised. 

The effect after a single dose is significantly smaller (and apparently less than Pfizer was claiming), though it still reduces infections by around a third. 

Overall infections among over 60s have dropped very significantly compared to the rest of the (still mostly unvaccinated) population. This has coincided with the British virus variant becoming more widespread in Israel, leading to an increase in infections among younger people.
You are probably talking about 42 doses per 100 people. All the vaccine trackers use this figure, I am not sure why because it is not very informative.

The exact, up to date, numbers are:

29% got the first shot.

13.5% got the second shot.

[https://datadashboard.health.gov.il/COVID-19/general](https://datadashboard.health.gov.il/COVID-19/general)

I think another interesting figure would be what percentage got the the second shot X or more days ago where X is the number of days it takes to get maximum immunity.
I'm seeing a lot of incorrect information and missunderstanding around the effectiveness of the first dose of the Pfizer vaccine. 

There are numbers of both 52% and 90%, these are both correct but refer to different things.

If you view figure 1. In this source 

https://www.cas.mhra.gov.uk/ViewandAcknowledgment/ViewAttachment.aspx?Attachment_id=103741P

You can see that the placebo group and active group diverge after 14 days. That is, the vaccine takes around 14 days to act. This is the key difference in the values.

The 52% is the figure if you measure from the moment of the first dose being administered.

The 90% is the figure if you measure from 14 days after the first dose was administered, i.e after the vaccine kicks in.

The vaccine isn't an instant cure, it takes time to act. But even after one dose it is highly effective.


Further source - https://www.nejm.org/doi/full/10.1056/NEJMoa2034577
Yes, a 60% drop in infections has been observed: https://www.timesofisrael.com/israel-sees-60-drop-in-hospitalizations-for-over-60s-in-weeks-after-vaccination/

Only 20 cases of infection after the second shot were registered, comprising 0.012%, iirc.
Yes some possible effects on cases and hospitalizations in early vaccinated population (healthcare workers & 60+ age group) are now being seen by comparing trajectory in this lockdown to the last one. It's not a perfectly controlled experiment, but it's promising so far. Check out Eran Segal on twitter, for example this post - [https://twitter.com/segal\_eran/status/1353811878208745473](https://twitter.com/segal_eran/status/1353811878208745473)
[deleted]
[removed]
You can just find the number of cases: https://www.worldometers.info/coronavirus/country/israel

After a steep increase, the number of cases has been going down for a week. How much of that is because of vaccinations or other causes is hard to say. The death rate is still going up, but that doesn't say much because it always lags behind the infection rate.
One thing to note about Israel is that the vaccination rates and cases are not uniformly distributed in the population. Two large sectors (Ultra ultra Orthodox Jews and Arab Israelis) account for a significant portion of cases, have lower vaccination rates, and generally dont comply as well with regulations and the state lockdown (for example, having mass gatherings, keeping schools and religious services open), etc.

So, the change in severe infections would likely be lower than what one would anticipate from a uniformly compliant and vaccinated population.

They have some early indications of efficacy but nothing had been published in peer reviewed form yet
reliminary data from Israel on Pfizer vaccine has 'promising' results

&#x200B;

Scientists in Israel, which is seeing the first real-world vaccine efficacy results, have tonight published some very promising preliminary data on the performance of the Pfizer-Biontech drug, writes Sky’s Middle East Correspondent Mark Stone.

&#x200B;

According to data release by Maccabi, one of Israel’s health providers, less than 0.01% of recipients of two vaccine doses contracted the virus. The company stressed that the data was preliminary and lacked a control sample. 

However, the data shows that out of 128,600 people who had received both doses of the Pfizer-BioNTech jab, only 20 were infected with COVID-19 a week after the second dose was given.

The company said that of the 20 people who tested positive a week after the second dose, half had existing chronic illnesses, yet none of them experienced more than mild symptoms of coronavirus.

&#x200B;

None of them needed hospital treatment or had a fever above 38.5C. It was not possible to know for sure if they would have suffered worse symptoms without the vaccine.

&#x200B;

Nonetheless, Dr. Anat Ekka Zohar, who studied the data for Maccabi sees the data as very positive: "This is very encouraging data... We will monitor these patients closely in order to examine if they continue to suffer from mild symptoms only and do not develop complications as a result of the virus."

&#x200B;

With a small population and a regular supply of vaccine, Israel is following the Pfizer protocol of giving two doses of the vaccine three weeks apart. 

&#x200B;

The Pfizer clinical trials - which is all we had to base efficacy on until now - indicated a vaccine efficacy of 95% protection from 28 days after receiving the second dose. The trial sample was 43,000 trial participants.
We should expect a minimum 2 week and possibly 4 week lag if they're using a two-dose vaccine.

That's when we'll see new cases begin to drop. Deaths from COVID usually take a couple more days at least after about a 3-4 day incubation, so it may be an additional week past that before we see serious numbers. 

So probably a little too early. Since they started their big push around the 15th of January we might start seeing early effects over the next week, though!
according to: https://coronavirus.1point3acres.com/en/world

Israel reversed a upwards trend starting on ~1/16. Though yesterday had a big spike of reports (data anomaly/data dump?)

Deaths continue to increase significantly, but there will be a 2 week lag from new cases to deaths.
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[https://www.boston.com/news/health/2020/03/25/can-you-become-immune-to-the-coronavirus](https://www.boston.com/news/health/2020/03/25/can-you-become-immune-to-the-coronavirus)

  By  **Apoorva Mandavilli**, 		 The New York Times Company March 25, 2020   

As the number of people infected with the coronavirus surpasses  450,000 worldwide, and more than 1 billion are locked in their homes,  scientists are wrestling with one of the most pressing questions of the  pandemic: Do people who survive the infection become immune to the  virus?The answer is a qualified yes, with some significant unknowns. That’s important for several reasons.People who are confirmed to be immune could venture from their homes  and help shore up the workforce until a vaccine becomes available, for  example. In particular, health care workers who are known to be immune  could continue to care for the severely ill.

Growing immunity in the community also is the way the epidemic ends:  With fewer and fewer people to infect, the coronavirus will lose its  toehold and even the most vulnerable citizens become more insulated from  the threat.

Immunity may also bring an early treatment. Antibodies gathered from  the bodies of those who have recovered may be used to aid those  struggling with the illness caused by the coronavirus, called COVID-19.

On Tuesday, the Food and Drug Administration approved the use of  plasma from recovered patients to treat some severe cases. A day  earlier, Gov. Andrew Cuomo announced that New York would become the  first state to begin testing serum from people who have recovered from  COVID-19 to treat those who are seriously ill.

“It’s a trial for people who are in serious condition, but the New  York State Department of Health has been working on this with some of  New York’s best health care agencies, and we think it shows promise,”  Cuomo said.

The body’s first line of defense against an infectious virus is an  antibody called immunoglobulin M, whose job is to stay vigilant in the  body and alert the rest of the immune system to intruders like viruses  and bacteria.

Days into an infection, the immune system refines this antibody into a  second type, called immunoglobulin G, exquisitely designed to recognize  and neutralize a specific virus.

The refinement may take as long as a week; both the process and the  potency of the final antibodies can vary. Some people make powerful  neutralizing antibodies to an infection, while others mount a milder  response.

The antibodies generated in response to infection with some viruses —  polio or measles, for example — bestow a lifetime of immunity. But  antibodies to the coronaviruses that cause the common cold persist for  just one to three years — and that may be true of their new cousin as  well.

A study in macaques infected with the new coronavirus suggested that  once infected, the monkeys produce neutralizing antibodies and resist  further infection. But it is unclear how long the monkeys, or people  infected with the virus, will remain immune.

Most people who became infected during the SARS epidemic — that virus  is a close cousin of the new coronavirus, called SARS-CoV-2 — had  long-term immunity lasting eight to 10 years, said Vineet D. Menachery, a  virologist at the University of Texas Medical Branch at Galveston.

Those who recovered from MERS, another coronavirus, saw much  shorter-term protection, Menachery said. People who have been infected  with the new coronavirus may have immunity lasting at least one to two  years, he added: “Beyond that, we can’t predict.”

Still, even if antibody protection were short-lasting and people  became reinfected, the second bout with the coronavirus would likely be  much milder than the first, said Florian Krammer, a microbiologist at  the Icahn School of Medicine at Mount Sinai in New York.

Even after the body stops producing neutralizing antibodies, a subset  of immune memory cells can reactivate a response effectively, he noted.

“You probably would make a good immune response before you even  become symptomatic again and might really blunt the course of the  disease,” Krammer said.

A crucial question is whether children and adults who have only mild  symptoms still generate a strong enough response to remain immune to the  virus until a vaccine is available.

Dr. Marion Koopmans, a virologist at Erasmus University in Rotterdam,  and her team have screened antibody responses in 15 infected patients  and health care workers.

The researchers are also using banked blood samples from about 100  people who were known to be infected with one of four coronaviruses  known to cause the common cold.

If those samples show some immune response to the new coronavirus,  too, Koopmans said, it might explain why some people — children, for  example — have only mild symptoms. They may have antibodies to related  coronaviruses that are at last somewhat effective against the new one.
Follow up question: assuming you've fully recovered, are immune, and no longer contagious, could you transfer it to someone else? For example, if I recovered and gained immunity then later visited some people with the virus, could the virus essentially stick on me and then transfer to someone without immunity?
Another question- can I get tested down the line to see if I had it during the outbreak? I heard someone say the antibodies stick around. I was extremely sick two weeks ago but recovered. Still in self-q though as my state is locked down
[removed]
Just a question I've been trying to figure out, not trying to hijack anything. But how does this self quarantine life end without treatment/vaccines? Seems the closest thing to hope is 18 months away (if successful). Are we going to be stuck at home for that long or will something happen before then that will allow us to go back out and enjoy life again? 

Thanks
Yes they do, same as almost any other infection. There’s a paper recently posted on BioRxiv describing the specificity of the neutralizing antibodies found in survivors’ plasma. Immune memory is not always lifelong but it lasts a while. It’s not really understood why some immunities are long lived and others not. 

The comment saying the virus is doing a lot of harm because it’s new is either wrong or misstated. SARS and MERS and Ebola were new and those outbreaks were quickly contained. The virus is doing a lot of harm because people are highly contagious before they present any symptoms, the virus is very resilient outside the body, and because it triggers an overexuberant innate immune response. 

If another Corona virus emerges immunity to this one isn’t likely to help. Neutralizing antibodies have to be very specific and having a kinda good antibody can be harmful if anything. Look up Dengue and Zika for one instance of this, it’s a whole other post to explain why and how that works.
Short answer: Yes. But there was some sporadic reports from China that there was a different Corona virus infecting people who already had gotten CoVid-19 (If I recall correctly). There are definitely different strains though:

https://fortune.com/2020/03/04/coronavirus-mutating-second-strain-covid-19-wuhan-china/

So potentially you could be re-infected but with a different version.
[removed]
Also, how concerned is the scientific community that we'll see mutations of the SARS-CoV-2 virus?  Is there a chance that we'll see seasonal mutations of this virus in much the same way we see new strains of influenza every year?
medcram #42 video (pulmonologist YouTuber):

https://youtu.be/q4P91VrfPGw

at the 8:50 minute mark mentions this paper:

https://www.medicalnewstoday.com/articles/contracting-the-new-coronavirus-may-protect-against-future-re-infections#Rhesus-macaques-receiving-SARS-CoV-2
Contracting the new coronavirus may protect against future reinfection

Pre-print paper:

https://www.biorxiv.org/content/10.1101/2020.03.13.990226v1
Reinfection could not occur in SARS-CoV-2 infected rhesus macaques

at 8:00 minute mark explains difference between PCR testing (for live virus), and antibody testing IgM (recent exposure), and IgG (chronic or earlier exposure).

Also that PCR may miss 30% of infected cases because sensitivity ranges from 60% to 70%

Also positive/negative can vary - which is why current practice is to require 3 consecutive day tests to show negative 
to higher degree of certainty.

This may be why there were reports of reinfection or test shoing positive later, which may simply have been due to the test weakness.
Data analyst for a hospitalist group here. From what I’m hearing it’s a yes and no. Most are, some aren’t. They’re finding that in some of the more critical patients there are cases of a weakened cardiovascular system, not always for the same reason and these patients seem to be the ones who are getting the virus again. The biggest problem I’m seeing with treatment is the virus is presenting symptoms that are typically coded to be treated with certain medications and iv fluids that are actually counterproductive and sometimes worsening the effects of the virus, or when used are causing multi organ failure in some mostly seen in patients livers and kidneys. The key is trying to figure out treatment when basically all the signs and symptoms and trainings are screaming “Hey do this!” When in fact it’s just making things worse. It sucks but they’re literally having to learn from their mistakes as they go.
Yes, but our experience with NON SARS endemic coronavirus (which causes relatively mild cold syndromes) suggests that immunity lasts only for a few months for unclear reasons. Either the coronavirus normally mutates over weeks to months just enough to where immunity becomes less effective or the initial immunological response is not as robust as it is against other pathogens. But that is with the endemic coronavirus strains. We don't yet know how COVID-19 (SARS CoV-2) will affect immunity however, the first SARs coronavirus back in 2002-03 disappeared after a year or so suggesting a far more robust and lasting immunity and so the hope is that SARS CoV-2 will result in the same situation.
[https://www.theatlantic.com/science/archive/2020/03/biography-new-coronavirus/608338/](https://www.theatlantic.com/science/archive/2020/03/biography-new-coronavirus/608338/)

&#x200B;

We don't know how long people would be immune to CoVid-19 after recovering. Likely less than a year given other corona viruses such as the original SARs and some colds. It is probably irresponsible at this point to try to give estimates.

&#x200B;

Edit: the normal flu is not a corona virus and is not a good comparison here.
[removed]
[https://qnlabs.com/blog/answering-your-questions-about-covid-19.html](https://qnlabs.com/blog/answering-your-questions-about-covid-19.html)

**QNL: Can you get COVID-19 more than once? If so, how long after you fight off the virus can you contract it again?**

**Dr. Clark:** This has a lot of people worried, so I’ll go in depth below, but to summarize: there is no scientific evidence to suggest people can get re-infected within weeks of recovering. There is reason to believe protection may last at least a year. 

Most experts think it is unlikely that most people can be re-infected in the short-term. It is more likely that the reports of reinfection are either rare events where the person did not produce an adequate antibody response, or a situation where that the person had not actually recovered and instead had a false negative.

Like with other aspects of SARS-CoV-2, we look to other coronaviruses for clues. Coronaviruses that cause the common cold do not elicit a strong immune response, but the response generally lasts on a scale of years rather than weeks. Though in one study healthy volunteers were able to be infected twice with the same strain a year apart.8 A better comparison probably comes from SARS because it causes a more severe disease. With SARS, people produced protective antibodies (that is, antibodies that protect against the virus) for up to two years after being infected, though the amounts of these antibodies decreased significantly after 16 months.9 Until we know more about COVID-19 specifically, this data is probably the best we have.   

So, as of right now, we do not know how long this antibody response for SARS-CoV-2 lasts. But a recent pre-peer reviewed article showed that rhesus monkeys could not be immediately reinfected with SARS-CoV-2, which is good news and a good start.10 In the coming months we’ll be able to answer this more clearly.  

 10.Bao, L. *et al.* Reinfection could not occur in SARS-CoV-2 infected rhesus macaques. *bioRxiv* 2020.03.13.990226 (2020) doi:10.1101/2020.03.13.990226.
Something I have been wondering as an American.  Since it is hard to get tested, is there a way to test for antigens or whatever it is to see if you HAD it and recovered? Getting that peace of mind would really be nice.
Short answer: We don't yet know.

People likely have an immune response that endures after the infection. We do not know whether that immune response is long-lived or short-lived.

A large amount of serological data is needed to answer this. Many of the medical professionals with expertise to figure this out understandably have their hands full at the moment with the pandemic.
Trevor Noah asked this question of Dr. Fauci on his last Daily Social Distancing Show, and Dr Fauci said while one can’t be certain, he felt strongly that people who survived CoVid 19 *would* be immune to it.  He did say there are always unexpected flukes, so he can’t say 100%.  

If you want the link I can find it, but it was very near the end of the interview.
According to prof at Rutgers University, people are immune to the same strain of Corona ( China epidemic for example ). People however are not immune to newer types that circulate in Europe for instance. 

Basically eventually people do build up antibodies to become immune to the same version of a viral disease.
TL:DR.
If the COVID behaves like any/most other virus we've dealt with, our bodies develop antibodies to fight the specific strain, and will be immune.

However, none can guarantee because we have not done scientific tests to prove it yet.
Short answer is we don’t fully know, it’s too new to really establish

Longer answer: you will have immunity for a bit, but the length is unknown, it could be for life, a lot like what you get vaccinated for as a child, or it could be like the cold where you might have immunity for a year...
Also, after immune to the disease can you still be a carrier of it to infect someone else? Obviously you could carry it on your hands or any other way. But could you carry it in your respiratory tract and spread it that way after immunity?
[removed]
I know this is going to get buried, but beyond the anecodotal data here: this preprint demonstrates a robust immune response along with antibody production within 15 days.  This suggests immunity for most people.  How long that lasts is anybodies guess right now, it looks like at least for SARS, it only lasted around 3 years.  

https://www.medrxiv.org/content/10.1101/2020.03.02.20030189v1.full.pdf
[removed]
Good question, that is how it should work but it may be possible for the virus to mutate.  There is unconfirmed reports of patients being reinfected with the novel coronavirus after recovering.  More data is needed.. In the mean time, stay healthy!
Debatable. I'll keep this explanation short as it is still something that is being researched right now 

There is one prevailing theory that I see come up again in textbooks and research - its that it may be that Coronavirus (at least Historically) does not generate a sufficient immune response to confer immunity. It is unknown as to why this is - but the hypothesis is that the virus' external envelope does not activate the body's immune response as strongly as other foreign materials, and thus no 'Memory T and B Cells' are made to provide lasting immunity.

The short of it is, quite frankly we don't know if being previously infected will confer immunity - if we go by what we know from other strains of Coronavirus, the answer is probably not. You may be protected against it in the short term (roughly \~100 days).  So be on the safe side and assume you can get it again.
Technically yes, but there are two strains of SARS-CoV-2, the S and the L strain and as far as we know at the moment there is no cross immunity.

There are even cases of people contracting both strains at the same time. 

So yes, you would be immune against a strain that you just recovered from but can still get infected with the other strain...
[removed]
[removed]
[removed]
Most of the major points regarding refeeding syndrome and hyponatremia have already been covered in this thread. So I'd like to point out a different issue with severe dehydration. As the body becomes increasingly dehydrated, it becomes increasingly difficult to actually get water in. 

This is especially true when you consider one of the most common causes of dehydration is fluid loss due to GI illness. (vomiting and diarrhea). This is even doubly so with infants. It's not possible to rehydrate simply by drinking. You cause more vomiting, more loss, and risk aspirating and drowning. And with dehydration comes hypovolemia. There's so little volume of blood in your veins that they are virtually impossible to start an IV without blowing the vessel. 

The most extreme treatment for this is [Intraosseous Infusion](https://en.wikipedia.org/wiki/Intraosseous_infusion) , where fluids are injected directly into the bone marrow. In laymen terms, we jam a straw into your shin and pour saline/meds/etc.

There isn't really a point of no return on life, so long as you're still alive. But that's not to say you won't have lasting ill effects. Lifelong dialysis due to kidney failure is a bitch.
[removed]
Not entirely sure there is one, but there is Refeeding Syndrome: where the influx of calories in the form of glucose leads to a large amount of insulin release, which in turn causes electrolyte abnormalities, namely hypophosphatemia and hypokalemia, which in particular can lead to arrhythmias and heart failure.
[removed]
Rabbits have a condition known as GI Stasis, where food stops being digested, due to a combination of factors, including eating less, less movement of material through the system, and buildup of "bad" bacteria in the GI tract.

Humans suffer a superficially similar problem, known as gastroparesis, where the muscles in the GI tract are not working as well as they should, thus material moves more slowly, and less nutrients are absorbed.

So even though you eat something, you may not be able to get any nutrition from it. And that is just one of many mechanisms, you could have issues with production of digestive enzymes, bile, stomach acid, etc, which can stop or slow down absorption.

Another way to look at it, though, is this; if you have a person dying of starvation (or rather, massive organ failure, because of a lack of nutrients), who just died, but their cells are mostly still alive, if you pour food and water down their throats, do they come back to life? Of course not. It's just a question of what fails and how, and that point of no return is probably highly variable, and perhaps subject to what sort of medical care a person can get.
[removed]
No one seems to have mentioned the thirst part. When extremely dehydrated, the body has attempted to maintain sodium, chloride, and potassium levels. Drinking water that is not properly balanced with electrolytes will cause brain tissue to swell, which may cause death; but it can also lead to dangerously low potassium levels which can cause muscular arrest, be it cardiac, pulmonary, etc.

Additionally, it depends on how you hydrate people. There are people that only drink distilled water, not having a clue that this is messing up their physiology to a great degree. If they become very dehydrated, the lack of ions in the water will have major repercussions.
I did my senior thesis on the physiology of the Refeeding Syndrome, so I can definitely answer about starving, but not  so much about thirst. 

&#x200B;

The Refeeding Syndrome is when someone who has been fasting for a long time gets food, and this causes their body to experience a massive shift in ion balance that makes a bad situation (low ion levels from depletion during fasting) much, much worse. Most notably, phosphates are depleted from the serum, meaning the bajillion physiological pathways that rely on phosphates all shut down. There are all the other ions that are probably out of balance too, and need to be fixed, but they only cause relatively mild problems that can be dealt with later. Hypophosphataemia (low phosphate concentration) is going to be the one that kills you. 

&#x200B;

Your body is slowly losing electrolytes (importantly: phosphates) all the time, but these are replaced by electrolytes in food and drink. When fasting, the electrolytes aren't being replenished. When you eat, insulin is released, which tells cells to start glycolysis and make energy. Energy must be used in order to make more energy, and phosphates are also needed to make energy. So, insulin also tells cells to take phosphates out of the serum, and to start using energy to make more energy. The problem is, there isn't much energy or phosphate to be used/taken out of the blood, and both are required for pretty much every process that is critical to keeping you alive. 

&#x200B;

Another issue with refeeding has to do with your stomach and intestines. During fasting, the absorptive mucosa of your gastrointestinal system is downregulated. Since it isn't being used, the body allocates less resources to it, and it shrinks and becomes less effective at absorbing nutrients. Also, the gastrointestinal system houses gut bacteria that help you break down your food and use its energy and nutrients. When you fast, these bacteria are starved. While they probably wont die out completely, their numbers will be greatly reduced. All of this means that you also are going to be less good at using the food that you eat.

&#x200B;

The solution is to manage the phosphate and other ion levels first, then slowly start to feed so that you don't make huge insulin spikes. It's better to starve for a few more days, which you'll probably survive, than to feed you right away and trigger acute hypophosphataemia that will cause tons of other problems that have a good chance of killing you. Current treatments suggest beginning with electrolyte fluids and no calories, and then feeding 25%, then 50%, then 75%, then 100% RER (resting energy requirement, the energy you use just sitting there doing nothing) over the course of 1-2 weeks. Some studies recommend giving IV phosphate supplements too. 

&#x200B;

&#x200B;

Disclaimer: My background is veterinary. While I did do some research into refeeding syndrome with humans, most of my knowledge is in regards to other animals, mostly cats and dogs.
There is also the issue that people who’ve been starving for long periods lose the gut bacteria necessary to process food.  So even when food is available they dont thrive. Even years later these people suffer from developmental and mental processing delays. There has been some research thats come out recently that suggests that famine victims might benefit from gut flora replacement as part of feeding programs.  

On a vaguely related note I read somewhere that Australian POWs of the Japanese during WWII were sometimes so severely malnourished that even after they were liberated they couldnt be returned to health. The general rule of thumb was if they weighed less than 40 kilograms they were unlikely to survive.
[removed]
MUAC-tape is used to determine if a child can still be saved after suffering from malnutrition. Kinda heartbreaking... See link -> https://motherchildnutrition.org/early-malnutrition-detection/detection-referral-children-with-acute-malnutrition/muac.htm
Even though it’s extremely sad, the Holocaust is a great example of this. I don’t know if there’s necessarily a “point of no return”, but a person can die if they overindulge too fast. Unfortunately when a lot of the allied forces were liberating the concentration camps they didn’t know this and they allowed the victims to eat at their hearts content, hundreds of people died as a result of the physiological shock from eating so much food after going for such a long period of starvation.
[removed]
I became severely dehydrated and nearly died. I was having seizures, my veins collapsed, etc and the only reason I'm alive is because they put saline directly into my jugular. Spent two weeks in the hospital but I was essentially on death's door and still recovered fully. I was pretty lucky though.
Yes, aside from refeeding syndrome, your body will break down skeletal muscle to perform gluconeogenesis to produce sugar for your brain. They found that one muscle, the diaphragm, resists this for about two months of starvation, but it too will eventually breakdown. Once the diaphragm starts this process, the person usually dies even if an entire medical team tries to save them with careful reintroduction of nutrients.
I know someone who survived the holocaust. On the day he was rescued from the concentration camp he was so famished and malnutritioned that if they had given him food he could have died. Instead he was given a piece of a chocolate bar which he melted in his mouth.  He tells the story better
I see this effect often enough at work as a 911 paramedic. Dying is dying. Doesn’t matter if its on the cusp of thirst, hunger, heart attack, or difficulty breathing, while dying the body reacts the same. As mentioned previously, this effect was also seen in concentration camps. When people are dying they have a physiological strength to hold on until help arrives. Their willpower keeps the body going (placebo effect) until help can intervene. Many times as soon as help walks in the door the victim will stop struggling to live because they can now have someone else worry about that. I see my critical patients decompensate  or get worse often due to this type of response. Edit: as for the second part of your question....yes. Many times they are aware they are dying and are terrified. I’ve also had patients tell me details of the scene afterward while they clinically dead.
Sort of. There is a point where you will almost certainly vomit if you try to eat solid food or drink too much for your now shrunken stomach to hold. At this point of malnutrition and dehydration, you will be given intravenous fluids and nutrients to keep you alive, and be given a strict regimen of liquid and puréed foods to ingest, until you can be weened back onto solid foods again,
I don’t post a lot. But this is my specialty. I don’t have flair. I am board certd in emergency medicine and medical toxicology (poisons). There is no point of no return for what you describe. Damage to most organs is temporary including liver, gut, lungs, musculoskeletal. Cardiac and neurological damage not as reversible so damage would mostly remain the same (depends on age a bit). But if they are alive and damaged, then they will remain alive and damaged. 

Source : board certification in exactly this question: emergency medicine : taking care of and stabilizing critical people. Also medical toxicology : the study of critically poisoned persons.
[removed]
This may not pertain EXACTLY to what you are asking but once someone reaches a certain point of starvation/hydration the introduction of nutrition can actually have a detrimental & potentially fatal effect.

This is known as "Refeeding Syndrome"

Below is an excerpt from the wikipedia page:

>Any individual who has had a negligible nutrient intake for many consecutive days and/or is metabolically stressed from a critical illness or major surgery is at risk of refeeding syndrome. Refeeding syndrome usually occurs within four days of starting to re-feed. Patients can develop fluid and electrolyte disorders, especially hypophosphatemia, along with neurologic, pulmonary, cardiac, neuromuscular, and hematologic complications.

>During fasting the body switches its main fuel source from carbohydrates to fat tissue fatty acids and amino acids as the main energy sources. The spleen decreases its rate of red blood cell breakdown thus conserving red blood cells. Many intracellular minerals become severely depleted during this period, although serum levels remain normal. Importantly, insulin secretion is suppressed in this fasted state and glucagon secretion is increased.[2]
>During refeeding, insulin secretion resumes in response to increased blood sugar, resulting in increased glycogen, fat and protein synthesis. This process requires phosphates, magnesium and potassium which are already depleted and the stores rapidly become used up. Formation of phosphorylated carbohydrate compounds in the liver and skeletal muscle depletes intracellular ATP and 2,3-diphosphoglycerate in red blood cells, leading to cellular dysfunction and inadequate oxygen delivery to the body's organs. Refeeding increases the basal metabolic rate. Intracellular movement of electrolytes occurs along with a fall in the serum electrolytes, including phosphorus and magnesium. Levels of serum glucose may rise and the B1 vitamin thiamine may fall. Abnormal heart rhythmsare the most common cause of death from refeeding syndrome, with other significant risks including confusion, coma and convulsions and cardiac failure.

>This syndrome can occur at the beginning of treatment for anorexia nervosa when patients have an increase in calorie intake and can be fatal.[4] It can also occur after the onset of a severe illness or major surgery. The shifting of electrolytes and fluid balance increases cardiac workload and heart rate. This can lead to acute heart failure. Oxygen consumption is also increased which strains the respiratory system and can make weaning from ventilation more difficult.

TL;DR - The body attempts to conserve energy by rationing nutritional reserves and focusing on slow burning calories such as Fat as opposed to carbohydrates. Once the body is in this nutritional hibernation if various nutrients are consumed/added too quickly the body can't digest/metabolize them and overloads.

Think of your computer running on battery saver and you suddenly open hundreds of applications at once.
I know from sitting with relatives in Hospice, that there is a point where their bodies can't absorb even IV fluids, it just causes painful swelling. That's why it's helpful to moisten their mouths with ice chips or a sponge.

I don't think that's directly because of dehydration, but because of the overall terminal condition. But I suppose if the person was that close to death, the initial cause is kind of irrelevant at that point.

The people I was with were not fully conscious at that point, but could become agitated/distressed. It's not easy to tell what people perceive in that state. The nurses & hospice experts told me they were only reacting to internal sensations like difficulty breathing, not responding to outside stimuli like being spoken to. But it's a bit mysterious.
To add to this discussion, the recorded record for most days without eating is held by Angus Barbieri, who fasted for 382 days with periodic doctor supervision.

https://en.m.wikipedia.org/wiki/Angus_Barbieri%27s_fast

Supposedly the longest someone has gone without water is 18 days. Among people who dry fast (no food or water), it's not uncommon for people to go 5-7 days without adverse effects.

The body can create water from burning fat and breathing air. From what I've read, 100 grams of fat can be made into 107 grams of water (where the hydrogen from the fat particles are combined with oxygen from the air to create H2O).

So as long as a person has enough fat, and they don't lose too much water to sweat and respiration - they can probably go quite a long time without food or water.

Bears don't drink during hibernation. And trans-ocean migratory birds don't drink when they are flying hundreds or thousands of miles over sea water.
[removed]
[removed]
Absorbing nutrients fast enough is never really the issue. By that point, they would already be killed by refeeding syndrome. Pretty much all the way up until the point that your digestive system permanently shuts down, you can recover. However, you can get to that "point of no return" if you eat too fast. When you eat something, your body needs to spend a little energy to extract the energy and nutrients in the food. When you are starving, you don't have enough energy and nutrient reserves to digest the food. Digestion is an autonomic process, so there isn't much anyone can do do stop it. Your digestive system will increase your heart rate and draw out nutrients from your blood. When starving, this is all that's left. Your vital organs have nothing left to sustain themselves and shut down. 

To summarize, that point of no return is when the first morsel of food goes down your throat.
I have often considered going on a “hike” and never coming home. I figure if I want to die that badly, my thirst and hunger will force me to consciously make the decision to die over and over again. On the flip side, I figure (in reality) that I’ll get so hungry and thirsty, I’ll give up and decide to live. I figure I am far too fatigued and indecisive to actually go through with it. It’s the only method of suicide I’ve ever deemed “acceptable”, I guess because it would take an immense lack of hope and tremendous willpower to literally starve/dehydrate yourself to death. However, I also know that suicide isn’t the answer. I’m not sure why I felt compelled to post this comment. Guess I just needed to get it off my chest. I hope no one minds too much. Talking about it seems to alleviate some of my discomfort.
I have a small piece of petrified bread. Yes, you read that right. It is light as a chunk of drywall and rock hard.

A far removed family member held it when she died of starvation in ww2. She was eating it, but it just wasn't feeding her fast enough or properly. She was just a child. The chunk of bread has been passed down to family members and now it's with me.
It’s not really a PONR in regards that they can’t process it fast enough, it’s that they can become dehydrated or malnourished enough to where ingesting food or water can make them sick and they can die from complications from that. 

There are people who fast for months (with medical supervision and supplements) and at a certain point your body can’t handle real food without a warmup and eating real food can cause blockages and other life threatening complications. 


In parts of Africa where malnourishment is a real problem for children, we have invented this great stuff called plumpy'nut (it’s also dirt cheap to make and shelf stable) which is really a lifesaver for these kids dying of malnutrition, as it can be eaten with relative safety for people dying of starvation and is so nutrition and calorie dense it will bring back kids on the brink of death.
I know there has been POWs (concentration camps and otherwise) during World War II who were rescued but had to eat in moderation at first or risk death. Had something to do with starving then eating more than their body could handle. It was in The Pacific series. Just looked it up now and apparently their body could spend the rest of their energy just trying to digest the new food. I promise a Google search will yield more results than my patience now lol.
The analyst in me demands that I give you an answer that is simultaneously correct, and almost certainly useless to you in context. Yes: there must exist such a state, at the very instant before death. So, for some final interval, the victim is technically alive but beyond recovery.

Which is about as useful as declaring that a fall doesn't kill you, just the sudden stop.  Still, medical research plays a role in identifying and reducing that terminal stage, so maybe not totally irrelevant.
There is a point of no return. It depends on the separate functions / decay of the vital organs: brain, heart (with blood-vessels), kidney, liver, by-kidney and gut-function. And they depend all on each other through biochemical processes. 'Point of no return' cannot not be expressed without ambiguous mathematical terms! Generally speaking, that 'Point' will be reached soon after even infusion fluids and / or dialysis of the kidneys did not recover the fluid-status of the patient.

If the patient is past that 'point of no return' consciousness/awareness depends on the rest-brain-function!
Everything is answered by now, bit here's some related info: "a point of no return" is such a philosophical question, but it kind of exists. A body is made up of many organs with many functions and a multiple organ failue is a thing. 
No matter what caused it, but if a person looks okayish and is brought the the best hospital in the world and gets the best treatment, they might still die. Kidneys don't do their job properly and it worsens function of every other organ. Guts don't do their job properly and it worsens function of every other organ. The lungs, the heart, the liver, the immune system, the endocrine system, the brain,... One organ dysfunction worsens everything and they fail one by one.
[This guy](https://en.wikipedia.org/wiki/Harry_Daghlian) died of something similar.  Basically, he had so much radiation exposure that it destroyed his body's ability to absorb nutrients.

And he knew.  As soon as he dropped the test article, he ran the math to figure out how much he'd been exposed to, and started getting his affairs in order.

edit: corrected the link.  Right place, wrong accident.
[removed]
In simplest terms, yes. Often times severely dehydrated people or people suffering multiple symptoms (vomiting, nausea) cannot drink without purging further.

While working in the wilderness there was at least one incident (that I experienced directly) of a hiker who had to be helicopter rescued due to severe dehydration, brought on by vomiting and elevation sickness. While he made it to the hospital, he still died later due to inflammation and swelling of the brain.
There comes a point with dehydration where your kidneys fail. If we're talking about a survival situation this would be the point of no return. If not and you can get proper medical attention you could survive, albeit a few kidneys short. If your body is a car, water is the oil. If you run a car without oil the engine will seize up and it's done. Same thing will happen to the body
They had this problem with concentration camp victims when they liberated the camps. The doctors had never experienced something like this before so were not aware of the best way to go about helping them. They tried to feed them normal food in the hope that they would regain the weight quickly. Many died within days. They had to switch to a high calorie paste food substitute. A lot of the knowledge of treating people with anorexia comes from the trial and error.
Point to note. They were not experimenting with them they were trying to help. They just didn't know how.
A lot of trial and error went on.
[removed]
From a medical standpoint, I would imagine you could still give them IV nutrition. There’s a pretty common liquid solution that hooks up to your vein known as TPN (Total Parenteral Nutrition) which is full of everything your body needs to survive. I don’t see why there would ever be a time it couldn’t be absorbed fast enough as it immediately goes into your veins. But I guess it’s kinda cheating because it’s not real food.
Ok, this will likely get buried, but I'll give an explanation.

After you ingest (swallow) water, it goes to your stomach. There, it is slowly released to your small intestine to be absorbed and pass to your bloodstream.
It takes water between 10 and 40 minutes to pass to your small intestine and be absorbed. The speed of this depends on how much water, the temperature (cold water will be released slower), if there is something else in the stomach, etc.
But, in general, it doesn't matter whether you chug a bottle of water down in 10 seconds or slowly sip it in 10 minutes. It will all end up in your bloodstream in about the same time.

The only thing that can make a difference is whether you vomit the water that you are ingesting. You stomach vomits its contents in response to a series of things (whether it senses that something you ate may be rotten or dangerous, whether there is a bad smell, etc), and one of those things that the stomach takes into account is how full it is. So ingesting water in small sips can help avoid vomiting, and that is what's recommended when for example children have gastroenteritis. But if you don't vomit the water that you swallow, it doesn't matter whether you swallowed it in 10 seconds or 10 minutes.

Edit: Wow, thanks for all the upvotes. Turns out it didn't get buried after all! This is 1/4 of my total karma right there! Glad you liked it!

Edit2: it's actually not clear whether cold water will be released (and thus absorbed) slower. As someone pointed out below, studies on this seem to be contradictory.

[removed]
[deleted]
[removed]
Piggybacking off this question: 

How much water does someone REALLY need? Drinking the "8-10 glasses" a day gives me clear urine and over 25 trips to the bathroom, whereas if I drink 5+ glasses a day, my urine is still clear and I feel hydrated without the bladder pains/constant bathroom trips. 
[removed]
I imagine there will be individual variety to this, but is it possible that your body learns how to deal with your particular style? Meaning, is it an adaptable feature? Sorry if it's a stupid question, but I got really curious :) 
Short answer, yes. 

Long answer, plasma osmolality and volume of ECF are the characteristics your body uses when it is regulating diuresis. If you sip two liters of water over two hours your body will lower production of vasopressin/ADH to allow excess water to clear as required. 

If you chug that same quantity in one minute your body will shut down production of antidiuretic hormones all together as plasma concentration drops through the floor. You will then overshoot ideal osmolality and become slightly dehydrated, because your kidneys can make very dilute urine astoundingly fast.

It's the same concept as eating a lot of sugar, your body can create the hormones to bring you back to "normal" but resources are wasted in the process and extremes are not fully accounted for.
I didn't connect it before but would this explain why if you find someone who is dying of dehydration or starving you need to give them water/food in small amounts? Giving them a normal or larger portion would actually make the situation worse?
For dysautonomia (autonomic nervous system dysfunction) water bolus therapy is a thing.  Basically chugging 2 8oz glassess of water can quickly increase your blood volume (and possibly stimulate your vagus nerve) and lesson symptoms (high heart rate, low blood pressure). 

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3904426/


https://www.ncbi.nlm.nih.gov/pubmed/10662747
[removed]
*this will probably get Buried 

But small sips of water over time helps to retain it, and not signal the brain to expel water. While absorption time will always be the same, if you chug a 500ml water bottle, you will almost certainly have to urinate quickly after said drink. I always drink in small sips throughout the day, as if I just *slam* the water bottle, too for lack of a better term "get it in me" I know I will have to urinate several times in the next following few hours or so. Your much better off to take small sips through out the day, and of course depending ok your ambient temperature, physical exertion and humidity etc 
[removed]
I hope this gets seen but how does liquid go from the stomach  to the intestines without stomach acid also getting in the intestines? 
[removed]
[removed]
[removed]
[removed]
[removed]
The University of Wales's Centre for Explosion Studies, in research commissioned by the Institute of Physics, "estimate that severe structural damage would have been sustained by buildings up to half a kilometre away," razing everything within 40 metres, and destroying Westminster Abbey.

[Here's a New Scientist article.](https://www.newscientist.com/article/dn4338-gunpowder-plot-would-have-devastated-london/)

The author notes amongst other things that they assumed for this calculation an equal amount of TNT, a more powerful but better studied explosive. They justify this increase in explosive yield with Fawkes' expertise as someone well versed in the use of explosives for military purposes, though it's not clear how much of a difference it would make. [Wikipedia](https://en.wikipedia.org/wiki/TNT_equivalent) lists the relative effectiveness of black powder as half that of TNT.

I’m honestly ~~surprised~~ blown away this hasn’t been posted yet but they attempted to recreate this scenario on an English program in ~~2017~~. [check it out here, it’s a HUGE explosion.](https://youtu.be/XTwbkYYdZBw) Cant answer any questions about physics, but I hope the video gives you what you are looking for. 

*edit: skip to ~49 min in to see the explosion. Also Richard Hammond from Top Gear is in it. 

Not 2017, made in 2005 apparently. You all are right, Hammond looks way too young for this to be last year. 
[removed]
It would have easily destroyed parliament.

The BBC reproduced it on a show a few years back in show called "The Gunpowder Plot - Exploding the Legend"

The short video link...

https://www.youtube.com/watch?v=_rEvCf0dH9I

The full show...

https://www.youtube.com/watch?v=XTwbkYYdZBw

Basically it would have leveled a 40/50 metre area around parliament and send debris up to a few kilometers away.
Sidney Alford, a well respected expert in explosives, recreated the plot based on historical data. Here's the video:

https://www.youtube.com/watch?v=XTwbkYYdZBw

It's an hour long so if you want to see the explosion skip to [50:00](https://youtu.be/XTwbkYYdZBw?t=3000)
Richard Hammond did a documentary on this a while back. Really interesting stuff. They ended up reconstructing what the Houses of Parliament were like back then, planting the gunpowder underneath, and blowing it up. There was nothing left.

Check it out:
https://youtu.be/2jCZtvbFv7A
There’s an old(ish) documentary piece from British television called “The Gunpowder Plot; Exploring the Legend” presented by The Grand Tour’s Richard Hammond.

It tries to get close to what the actual explosion would have been like with some kind of version of the explosion

https://youtu.be/XTwbkYYdZBw
Richard the hamster Hammond did an expereiment to find out. They built a replica of the House of Lords on a remote military facility. The 'cellar' was in fact an undercroft with 9 foot thick mediaeval walls so much of the blast was directed straight upwards.
Black powder is a low explosive, which means I deflagrates rather the detonates.   What that means in lames terms is explosives are measured in how fast they burn, which is what a conventional explosive does, just at a very rapid speed. So as a low explosive black powder in and of itself is not very destructive when compared to a high explosive. What makes black powder effective is containing it. Contain it in a barrel and it will propel a projectile. Contain it in a pipe and it becomes a mechanical explosion causing damage by way of the pipe breaking at great velocity do to build up in pressure (think coke bottle shaken then tossed up in the air). Because it’s a low brisance (ability to cut) it’s not very effective at damaging hardened structures. The main way it would be effective is if it was able to build up enough pressure in the tunnel or building that it was placed it. And other factors like the building materials, amount of earth it was under etc would all factor in. With that, the quality of black powder and amount of moisture as well. There are instances of huge black powder explosions, and instances of not so huge. There would have to be some real study and testing done to say for sure, but what I can promise is that there is no chance it would have gone unnoticed. I hope that answers a least part of the question. I will clarify where I can, if you have questions. 
Adding on to the discussion, a BBC production featuring renowned scientist Richard Hammond (/s) actually demonstrated the effect of igniting that amount of gunpowder would have on the style of building and the inhabitants inside. [Link](https://youtu.be/_rEvCf0dH9I?t=50)

&#x200B;

Brilliant show, but I always found it very sobering
bbc and "hammond the hampster" already did the empirical.  


full show

[https://www.youtube.com/watch?v=2jCZtvbFv7A](https://www.youtube.com/watch?v=2jCZtvbFv7A)  


and for those of a short fuse just the interesting bit  


[https://www.youtube.com/watch?v=\_rEvCf0dH9I](https://www.youtube.com/watch?v=_rEvCf0dH9I)
One of the guys from Top Gear has a video on You tube showing what a portion of that explosive charge would do to a building...I don't have a link to offer but you can probably search for it.

It's rather "impressive". The house of lords would have been seriously damaged if not destroyed.

edit> This *may* be the video I watched: https://www.youtube.com/watch?v=2jCZtvbFv7A
[removed]
This confusion comes from a misunderstanding about what compiling is.

These days we commonly say we don't work in bits or machine code. That isn't exactly true. People who work in hardware do work in bits when they're developing a new kind of circuit.

When microprocessors were first created everybody worked with bits. Flip this to on and then that causes this and you can flip these to represent a number and flip this switch to move it into a special register and picked up by the clock.

At first that worked fine. Kind of the way you don't need labels for the 20 or so light switches in your house. But after a while they wanted a) more complicated systems and b) people with less experience to be able to contribute.

So that's when early forms of what we would call assembly language came on. It abstracted all the little switches away into a smaller set of instructions like move data, add data, subtract data etc.

Pretty soon after that things got too complicated again so some people who found themselves writing the same combinations and order of these instructions sought to abstract these hundreds of lines of simple instructions into a single line instruction. So they wrote a program that would take a program of five lines in this new langauge and convert it into the hundreds of lines of assembly language.

This has happened multiple times over and each time the compiler/interpreter etc is written by people who *can* write in the lower level language the compiler produces but would rather save themselves the keystrokes.

EDIT: My source is 22 years of programming, 18 years of software engineering. These days I leverage some of the lessons and motivations for developing compilers into developing custom tooling for other software engineers in mid to large scale organizations.
Early on in the history of computing, programs would be written as a sequence of operations in assembly language, which basically means they would be written in terms of actual physical operations for the computer architecture to perform, e.g. copy this numeric value to that memory location.

That's obviously very time consuming and difficult, and it means that programs have to be rewritten for each new type of processor that you run them on. So to make it easier, higher level languages that are more human readable were created, with commands that basically work as shortcuts for all the repetitive number-juggling required in assembly language. So you have to write a compiler in assembly language, which doesn't require compiling. 

It's interesting that you ask "did **he** compile it at all", since the first person to write a compiler was actually a woman, Grace Hopper :)
[removed]
Please bear with the first bit of this as I establish a framework understanding.

Machines execute machine code. Compilers do *not* compile directly to machine code. They compile the code into to *assembly* which is then *assembled* into machine code.

Machine code has two parts, ~~zero and one~~ operands (what to do and where (registers) to do it\*) and data. Depending on the number of 'bits' the machine is, these can all have varying lengths of bits.

Now that we understand what machine code is and how it's structured, let's look at what assembly code is.

Assembly code is *human readable machine code*.

We (humans) give the operands *mnemonics* (names) and logically divide this into three parts to make things easier to remember for us. No matter what, the same operand mnemonic will *always* turn into the same sequence of binary. 

So assembly let's us turn a 'line' of binary from

    010011110101000001010010 010100100100010101000111 01000100010000010101010001000001

Into

    OPR REG, DATA

So now we know what assembly is, and how it's written. How does a machine turn assembly into machine code?

Why with an assembler of course! Usually assemblers are 'programmed' on paper, by hand writing the assembly you would normally type to later be run through an assembler. The next step is. . . Translating the assembly of your assembler to binary yourself! Hooray how exciting!

Even still we keep it a bit more human readable than direct binary we use a system called hexadecimal to represent binary. A symbol of hex is four bits, and is as follows.

    0 0000
    1 0001
    2 0010
    3 0011
     ...
    F 1111


Once you have your program translated from assembly to hex, you queue the program in a hex editor. A hex editor is a peripheral attached to the machine which stores the binary of your program which you enter using hex, and sends it to be executed once you are finished.

So, now we understand how we get machines to do anything. Let's see how we can reach the ability to compile a compiler.

1) Hand write an assembler in assembly/hex.

2) Enter your assembler into a machine with a hex editor

3) Write your compiler in assembly and assemble it.

4) Write your compiler in the language your compiler compiles.

5) Assemble the compiler.

6) GOTO 4

That's how you get a basic compiler. 

Fun fact: A compiler written in the language it compiles is called a *self-hosting compiler*.


***

Things you may want to dig into for further reading.

Linkers (part of modern compilers, much the same way an assembler is one part of a modern compiler)

Assemblers

Intel Syntax vs AT&T Syntax

Machine Organization (You can't write working assembly unless you know what is where and how it works)

Endianess (relevant to the structure of machine code)

Bitwise Operations (extremely low level data manipulation functions)

CISC and RISC (machine architecture paradigms)

Forth (A super early language that blurs the line high and low level software)

gcc (GNU Compiler Suite)

Source: Hobbyist embedded developer.

I hope this adequately answers the question OP!
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
I worked as a SysAdmin for a Burroughs (B5500 and) B6700 (mainframe) computer. Our Algol compiler was written in Algol (as were most of the others). (I agree with the answer on how that first happened, way back before my time.)

When we re-complied the Algol compiler, we had to do so three times, in sequence. (The compiler was distributed as source on tape along with running object.) 

The first time was to assure that our local changes (called patches) didn't create any syntax errors in the compilation.

The second time (using the output from the first time) was to assure that what we created was able to read a program as complex as the compiler and produce an object output.

The third time was to show that what we produced was indeed a competent compiler.

In between the stages, we had to anoint each output as a compiler with the MC (make compiler) command from the operator console, otherwise the output would not be executable.
[removed]
Programming isn't magic. It just cofidies a procedure. That procedure can be done by hand. 

So a human can make diagrams and flow charts and whatever, and then manually code the assembly. And assembly maps 1:1 with the machine code, in general (and especially back in the day)
[removed]
[removed]
Watch this series and it will show you exactly how they are programmed. He builds an 8 bit computer but hand. With only the most basic logic chips. And he goes over exactly how each one works, and how each section of the computer works. It's a really really good series. I highly recommend it to anyone who wants to know how a computer works.

Building an 8-bit breadboard computer!: https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU
[removed]
[removed]
This is a video of me presenting at RubyNation 2016 with a home brew z80 Computer I built.  In it I write a program, hand compile it, and program the computer by flipping dip switches.

https://vimeopro.com/user24051491/rubynation-2016/video/177832832

And by the way, you mean ‘she’.  Admiral Grace Hopper is considered the first person who wrote a compiler for a higher level language.  It was called FlowMatic, and was the precursor to cobol.

Lots of good answers, seems odd to me that, at least on the first page, nobody pointed out that the creator of the first compiler was not a "he" but a she: U.S. Navy Admiral Grace Hopper, one of the most important pioneers in computer science. She led the design team for the ENIAC, she is credited (somewhat apocryphally) with coining the term "bug" (according to the story - the first computer bug was an actual insect, a moth that got stuck in one of ENIAC's relays), she created the COBOL language and she invented the first compiler.
[removed]
[removed]
[https://www.archive.ece.cmu.edu/\~ganger/712.fall02/papers/p761-thompson.pdf](https://www.archive.ece.cmu.edu/~ganger/712.fall02/papers/p761-thompson.pdf) is a great exploration of your question, with a side effect of destroying any trust you might have had in digital technology in general
I am going to suggest you watch this entire series in order to understand computers better. You will understand how a process works and how code is executed.  


ML programs do not need to be compiled. SOME, but not all, Assembly needs translated, but this is not the same as being compiled. For example, LDA $0c00 -vs0 LDA SCREEN. Screen being defined as $0c00 earlier, in headers, and is a pseudo-op. Before the code is run; or as the code is written depending on the assembler,  all instances of screen are replaced by $0c00. This is different than compiling; compiling takes code and turns it into other code. print "werd". the compiler has a routine it uses for print, and adds that to program. There will sometimes be different code for different types of printing. It maybe faster to use one routine to just print from where you left off printing -vs- printing to a specific location.  


ML was written via switches, sometimes hardwired switches, and sometimes even knitted wires. Punch cards too. Basically either the program is hard wired in ram, or was entered by throwing switches and then another switch to push the data to memory. Then come interpreters... they dont compile the code; they just interpret the non-ML and run things in real time. Think most flavors of 8-bit basic, or even quick-basic 4.5. ML monitors are a kinda interpreter, but used to edit, view, and run programs.  


The first compiler would have been written in pure ML i would think; and thusly would not need compiled.  


 
TL:DR; you would write the compiler in *assembly*. So no.

&#x200B;

Also if you haven't heard of assembler and are masochistic enough to want to learn it, I recommend a game called "Human Resource Machine" which gently introduces you to the idea without you even knowing you're being taught assembler. 
[removed]
[removed]
Electricity has been known for a long time. Egyptians noted the similarity between electric eel shocks and lightning.

Pliny the elder (and many others) noted that these shocks could be transferred, that objects when rubbed often attracted things, that so did magnets, and that the three phenomena were connected. Thales of Miletus came up with the theory that when Amber underwent friction, it became a lodestone, and if rubbed further produced lightning proving it was a magnetic force behind lightnng. Both though in terms of "Gods" or "Souls", which in terms of philosophy might be better thought of as a "motive force without a clear origin".

Which is a pretty solid conclusion if you discount Thales mixed up electric fields and magnetic ones. And, you know, thought everything was water (not as stupid as it sounds.)
I hope I'm not breaking any rules here, but r/askhistorians gets this question from time to time. [They have an FAQ section about it here](https://www.reddit.com/r/AskHistorians/wiki/science#wiki_ancients.27_views_of_static_electricity).
The scientific community (including Ben Franklin) thought of electric current as some sort of invisible fluid. "Positive" objects possessed a surplus of this fluid and negative bodies didn't posses "enough fluid" to be "balanced."
There is a great [BBC documentary](https://youtu.be/Gtp51eZkwoI) on the history of electricity if you haven't seen it yet. 
If you read Peter Winch and his "Understanding a Primitive Society", you could assume that they thought things like: "We encounter the rage of spirits all the time and it's not shocking (sorry) because we know what's going on, but what on earth did people think was happening before we understood shamanism?"
[removed]
Static electricity is a fairly complex thing that's misunderstood. I have trusted the writings of William Beaty, a UW professor and researcher who has a website dedicated to clearing misconceptions about such stuff.

I know this may be off-topic, but if anybody is looking to learn about static electricity more without being subject to misconceptions on the Internet, I recommend these articles by him:

-["Static Electricity" Means High Voltage"](http://amasci.com/emotor/voltmeas.html)

-[Static Electricity Misconceptions](http://amasci.com/emotor/stmiscon.html#one)

-[Humans and Sparks](http://amasci.com/emotor/zapped.html)
[removed]
People have known about the existence of static electricity, lightning and magnetism for a very long time but could not explain it, and they never figured it out that it's the same thing.

Lightning was usually taken as a battle between gods. It's powerful and deadly and only gods possess such power. The static electricity had never been believed to be small lightnings, up until the enlightenment age in the 1700s. In fact, if you say you see small lightnings on your finger back then, they would laugh at you silly. You needn't need a time machine to know what static shock is called back then, because even up until today, we have whole tribes who never knew electricity and they just call it pinching if it hurts them or small fire if they see the static discharges on animal furs. Rubbing things is what tribal people do to make fire. They are not surprised, just because touching would give some pain because fire is pain too. In fact, where I live, people continue to call electricity as "fire". The ancient egyptians called the electric eels "angry fish" and didn't say it shocked them, more like pinching, numbing or biting. It's more about poisonous than electrical. Many plants and animals can give you sharp pain, sharp itch and numbness. Electric eels aren't any more special than a box jellyfish or fire ant or poison ivy.
The most amazing thing I read was the arthurian chroniclers writing about phenomenon that turned up in a most fantastical way. Like paganism and many religions they did all they could to explain what they saw. All it was in the end was aurora borealis 
I've been writing an essay on witches for APUSH and I've actually found several testimonies where the witch was accused for small shocks that in description sound a lot like static electricity. Though many of the ones I've been using are not well documented and have been passed on my oral tradition and secondary sources
[removed]
When the ancients thought of electricity as like water they were not very far off. The analogy between water and electricity although imperfect, goes pretty far and can help you understand Ohm's law, V=IxR, or voltage equals Current times Resistance. Thinking of electricity as water voltage would equal pressure, current would equal flow rate and resistance the inverse diameter of the pipe (large diameter means less resistance).  
Quite probably in the case of very ancient people, they thought it was a tiny bug, or didn't have a well communicated philosophy on it; although I'd imagine walking around barefoot on mud probably makes it pretty hard to build up static; rubber soled shoes, car tires, carpet floors, and other insulators are what allow a human to carry a charge that then dissipates when they change medium.

I've noticed in anime they say it's because 2 people are "secretly mad at each other about something".  While I have done zero research, and anime definitely does not qualify as a "source", a lot of those kinds of gimmicks in entertainment are derived from old wives tales, like how in western culture we have "it's dangerous to open an umbrella indoors" and such.

I bet if you were in Salem in the late 1600s they'd have called it witchcraft and burned you.
I've been writing an essay on witches for APUSH and I've actually found several testimonies where the witch was accused for small shocks that in description sound a lot like static electricity. Though many of the ones I've been using are not well documented and have been passed on my oral tradition and secondary sources
[removed]
[removed]
[removed]
Short answer: maybe, maybe not. No one is sure. 

Longer answer: The alphabet we use today is something that evolved over ~3,000 years, through 4 iterations minimum: 

Phoenician ⇒ Greek ⇒ Latin ⇒ modern languages with letters like J, W, and ß, as well as diacriticals like å and diphthongs like œ. 

As a result, there’s no one answer for where the order for a given modern language comes from. The alphabet for English is different from that of French, or Swedish, or Polish. Consider the Hungarian alphabet, which looks like this:

a, á, b, c, cs, d, dz, dzs, e, é, f, g, gy, h, i, í, j, k, l, ly, m, n, ny, o, ó, ö, ő, p, q, r, s, sz, t, ty, u, ú, ü, û, v, w, x, y, z, zs

Now, we can look at that and see that the basic order of the Latin alphabet remains. The additions are just stuck in after the related letters whose sounds they modify. As with English, we have the Latin alphabetical order, plus the later medieval insertions of j and w. The insertions are obvious and easy, because they simply follow the letters they were invented to modify/clarify. 

But for the Latin order, it’s trickier. Latin is a blend of Etruscan and Greek, and they adopted neither wholesale. For example, Etruscan had 3 letters for what we would think of a ‘k’ sounds today: C, Q, and K (probably ‘kay’, ‘qoo’ and ‘ka’ but this is a guess since Etruscan remains untranslated). G was a lesser-used sound in Etruscan, but /k/ was very important, so they invented C from G, swapped kappa and gamma in the order, then inserted the ‘other’ k sound further down the line from Ϙ or qoppa , a pre-standardized Greek letter used in some cities. 

Long story short, there’s some interesting linguistics research that suggests certain groupings of letters were created intentionally by different groups at different times, and then just kept out of habit by adopters. 

So for example, the Beth Gimel Dat sequencing mentioned in a number of other posts was adopted directly into Greek as Beta Gamma Delta, but then the Gamma was replaced with C in Latin. 

EDIT: Thanks for the gold, kind stranger!
[removed]
Semiotician W. C. Watt, after much research, concluded that the alphabet ordering descended from an early organization that grouped the letters by their sounds, which might have been used as a teaching tool for language (although that original organization/artifact is now lost). He called it the "[Ras Shamra Matrix](https://www.degruyter.com/view/j/semi.1989.74.issue-1-2/semi.1989.74.1-2.61/semi.1989.74.1-2.61.xml)."

Apparently he hadn't really even considered the question until a student asked him, and he realized he didn't know. However, he thought about the alphabet (A-B-C-D), realized that earlier organizations, like in Hebrew, had Bet-Gimel-Daleth (B-G-D), which are phonetically related, and so it might not be arbitrary. This sent him off on a research quest trying to figure it out.
[removed]
[removed]
[removed]
[removed]
[Hindi](https://en.wikipedia.org/wiki/Hindi) alphabets are ordered in two different categories: all vowels preceeds the order followed by all the consonants. The ordering is very clever and when you speak in the order, you can notice the subtle movements of the tongue and muscles in a particular way. 

For example, the first four consonants are 'Ka', 'Kha', 'Ga', 'Gha'. When you pronounce it you see your tongue rises with every alphabet.

[See this chart](http://hindilearner.com/images/alphabets/consonant.png) for all the consonants in order.

Edit: A word.
[removed]
The orderings are a stroll through the history of civilisation, likely a partial result of naming things via [acrophy](https://en.wikipedia.org/wiki/Acrophony). The initial block of letters were originally based on Egyptian hieroglyphs, passed through Phoenicia, and onto Aramaic/Semitic languages and proto-European. Later, Greeks (c. 10th BCE) and then Romans (c. 6th BCE)  formalized the ordering, through various tweaks. Monks in the middle ages set the version of Latin letters used in English.

The first letter, "A" is actually the representative of the "ox" (or, rather, it's earlier incarnation, the Auroch, aka the "ur-ox") -- you can still see the form, if you imagine it turned over: ∀

Consider that oxen were the first large creatures to be domesticated, and the first thing that would be used to create a stable, agrarian community. In Arabic and Hebrew, the word derived from the "Aliph/Alef" literally means "tamed" (animal).

Similarly, consider the Proto-Germanic/Norse rune-system, in which the first character "Fe" (ᚠ) represented "livestock" or "wealth"-- while the second character "Ur" (ᚢ) was "Auroch."

Likewise, the second Latin letter, "B" represents "house" - and "bet/beyt" in Semitic languages is still the word for "house."


[removed]
Not particularly, no.

Just like most abstract/arbitrary things people have invented, there's no real reason that can be pinned down.

In fact, if you look at other languages, you may not even see any similar correlation. For example, Japanese writing (that isn't kanji) is in the form of phonemes, and the order is completely different.

A(Ah) I(ee) U(oo) E(Eh) O

Ka Ki Ku Ke Ko

Sa Shi Su Se So

Ta Chi Tsu Te To

Na Ni Nu Ne No

Etc. You can look up hiragana on Google if you want to see the rest, but the point is there's almost no overlap in terms of order other than the A(Ah) sound is still first. It's language and writing character set dependent!
The arrangement of the letters in the alphabet are strictly a historic precedent. While some proto-alphabet may have had a logical order, we don't know what that logic may have been. The exact order of our modern alphabet is apparently not set up in any coordinated, logical way. For instance, they are not arranged vowels first, voiced plosives with their unvoiced plosives next or fricatives last, etc. They're arranged in a seemingly random jumble. Additionally, the number of letters we have aren't even entirely indicative of all the sounds possible in the English language, so we end up combining some letters to approximate sounds like "ch" in "Charlie". That means we don't even have a logical reason for the *number* of letters, other than again, historic precedent.

While there is no *logical* reason for their arrangement, the *historic* precedent is important as part of the fabric of society. We are all taught *by rote* the order of twenty six letters so we can search for information in a library using their order as an agreed-upon standard. That agreed upon standard also allows us to spell words that allow us communicate concepts from the past to those living in the future.


A while ago I realized that the alphabet song, where the major pauses are in the song, coincide with which hand you type them on a keyboard, with minor exceptions.  abcdefg (left hand) hijklmnop (right hand) and then it kinda gets fucky.
[removed]
In Devanagri script, primarily for Hindi (which I speak and write), the alphabets are arranged phonetically. In so much that of the 36 alphabets arranged in 7 rows, the first 5 rows ascend from sounds emitting from closer to the throat to the tip of the tongue.
And on each of the 5 rows, alphabets ascend from throat to tongue as well (i was quite amazed when i discovered it. It wasnt taught to us like this in school).
The last 11 alphabets are kind of arbitrary. Perhaps they couldnt fit them in the scheme anymore!

TL:DR - Hindi script (true for all devanagri scripts i guess) is arranged phonetically in order of which part of your mouth creates the sound.
For English, the answer is that the order is pretty much random, but based on what came before.  The fact that "new" letters to English were usually just tucked on the end ("u" is the last letter in Old English alphabet; "j" is our newest letter but is a variation of "i" so it was inserted after it) proves it 
I don't see how you could do the same in standard Chinese. You can do wordplays with sounds (like 布吉岛 instead of 不知道) for example that can be understood in text messaging. You could also theoretically express a sentence by using the right characters with the wrong radical and people would probably be able to understand more or less but it wouldn't be fluid like we read your title's sentence in English.
It works well with Japanese if the sentence is purely composed of Hiragana or Katakana but not as well when there are Kanji (Chinese characters in Japanese) because reasons pointed out by other comments. I would assume it works well with Hiragana and Katakana as they are relatively closer to Alphabet in terms of how they are 'read'


For those who can Japanese, if the original typoglycemia sentence is translated into Japanese Hiragana, it would look like this:

こんちには みさなん おんげき ですか？　わしたは げんき です。
この ぶんょしう は いりぎす の ケブンッリジ だがいく の けゅきんう の けっか
にんげんは たごんを にしんき する ときに

その さしいょ と さいご の もさじえ あいてっれば
じばんゅん は めくちちゃゃ でも ちんゃと よめる という けゅきんう に もづいとて
わざと もじの じんばゅん を いかれえて あまりす。
どでうす？　ちんゃと よゃちめう でしょ？
 “研表究明，汉字的序顺并不定一能影阅响读，比如当你看完这句话后，才发这现里的字全是乱的。”

The above sentence is scrambled, but I can read it almost as fast as reading the unscrambled version. So based on a sample size of 1, I would say yes it does apply to Chinese. 

If you are curious what the sentence says: research shows, the order of words does not affect your understanding, for instance, after you read this sentence, you would realize the order of the words are scrambled. And the unscrambled version is:  “研究表明，汉字的顺序并不一定能影响阅读，比如当你看完这句话后，才发现这里的字全是乱的。”
It depends on what is the equivalent you are thinking of for Chinese. Since in English the letters are scrambled, the most direct equivalent would be swapping the positions of strokes around within a character.  I would say in general this produces completely gibberish characters that are near impossible to decipher. For English you basically just need to mentally shift alphabets left and right to decipher a jumbled up word, but in Chinese the strokes can move "360 degrees" within a character so the permutations are endless. Take a common character like 我 (wo), which means "me" or "I". Maybe some slight shifting of the strokes is ok, but swapping the strokes around creates mutant characters that literally don't exist in the Chinese dictionary and would be unpronounceable and have no meaning.
I would just like to issue a correction, Korean does not use a syllabary. They have specific vowel and consonant symbols so it is an alphabet. The difference is they join them into syllable blocks so from someone unused to the language it can look like a logograph or the Japanese syllabaries. But unlike Japanese where for example this か is specifically the symbol for the syllable 'ka' in Korean you'd have ㄱ for 'k' and ㅏ for 'a' together 가  for 'ka', and for 'ra' you'd have ら a completely different symbol in Japanese and in Korean it's  라 where 'a' is still ㅏbut now you changed the consonant part to ㄹ for the r/l sound.
in arabic language i can kind of figure out what are trying to say if i hear it even if it's broken but visualy it's kind of hard to read depending on the sentence each word can be confused with another diffrent word entirely and sometimes there are words that will read the same even if you mirror them other than that i can still kind of read it just painfull to look at and probably look like am reading gibbrish for the most part
[removed]
One way to tackle this that hasn't been mentioned yet is via information theory.

You can read the text because English has some redundancy in its information content.  If I give you the letters "sentenc_", you can guess that the missing letter is "e" -- the e is pretty much redundant.  If I gave you "thi_", it might be "this" or "thin", but probably not "thib".  If "albe" and "tihs" and "setencne" were all valid English words, deciphering your topic sentence would be a lot harder!

We can distinguish between the "symbol data rate" and "information rate" of a written language.  The symbol data rate is the number of data bits needed to describe a random sequence of scrambled characters, taking into account the frequency of the characters.  Since English has 26 letters, you'd think that you'd need 5 bits (2^5 = 32) to represent them all, but since "e" and "t" are so common, the symbol rate of English is actually about 1.5 bits per symbol.

The *information rate* (entropy) can be obtained by asking native speakers to predict the next letter, or else by using a data compression algorithm to re-encode the text without the redundancy.  The information rate of English is less than the frequency of random letters, about 1 bit per symbol -- so English has a redundancy rate of about 50%.

Remember, it's this redundancy that makes it possible to read incomplete or error-filled text.  What is the redundancy in other languages?

[This paper](https://pdfs.semanticscholar.org/a44d/9b998c1451328bcb4517ed9c1930171e0a79.pdf) calculates information rates for a variety of languages.  Since Chinese has a much larger number of symbols, each symbol has more information content -- but of course, some still occur more frequently than others.  For Chinese, the symbol data rate is about 4.8 bits per symbol.  The *information* data rate is about 3 bits per symbol.  Thus, the redundancy of written Chinese is also about 40%.

Japanese as you'd imagine is somewhere in between.  Symbol rate of about 4 bits per symbol, info rate of about 2.6 -- about 40% redundancy.

There is one interesting exception: Korean.  It's symbol rate is about 3.6 bps, information rate 3.3 -- about 10% redundancy.  This may be because the Korean writing system was specifically designed to represent Korean, rather than evolving naturally over thousands of years.  (Romanized versions of Japanese and Chinese also have low redundancy.)

The upshot: the writing systems for most natural languages have similar amounts of information redundancy, which allow you to read them even if they're garbled.

https://www.britannica.com/science/information-theory/Linguistics
https://pdfs.semanticscholar.org/a44d/9b998c1451328bcb4517ed9c1930171e0a79.pdf
It's "well known" because a paragraph from 2003 talking about the phenomenon went viral. I've copy pasted it here:

"Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae. The rset can be a toatl mses and you can sitll raed it wouthit porbelm. Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe."

The ideas presented in the paragraph are actually not super well supported. You can read about a researcher's response to it here:  [https://www.mrc-cbu.cam.ac.uk/people/matt.davis/cmabridge/](https://www.mrc-cbu.cam.ac.uk/people/matt.davis/cmabridge/)

EDIT: To summarise what they've written about it - there wasn't actually a study done at Cambridge University that found this. Many of the words aren't jumbled at all (to, a, at, the,...), and there are many four letter words which are easy to figure out (since they only swap two letters). The paragraph is also written in a fairly predictable pattern, and many of the longer words aren't jumbled much at all (Cambridge only has two pairs of letters swapped!).

I know it's not actually answering the question you've asked - but the premise that your question is based on is not entirely factual, so it needs addressing.
[removed]
Slightly unrelated question: At what level of fluency (might not be the right word) do you have to have if English - or other languages where this is possible - to be able to read and comprehend these types of jumbled sentences?
[removed]
Yes, but it’s more difficult, and context really matters. It’s often used to escape censorship. 

For example, 我知道你不懂中文 (“I know you don’t understand Chinese”) could be written, completely insensibly, as 喔之道泥布动钟闻. In this sentence, every character has a very different meaning from the first, but the pinyin (sound) is nearly identical.

I mean, again, it’s definitely not the same in Chinese, and it’s much easier if you only change 1-2 words (我知道你不动中👃）  also, emojis are hella used.

Tl;dr the Chinese version of what you’re looking for involves substitution, rather than order change.
**It's _NOT_ that the first and last letters are in the right place, but specifically that the words simply aren't very jumbled up.** Longer words that are jumbled up more  slacinnigifty will not be easily dibreplacehe, and will take both time and context to figure out.

There isn't any modern language that I'm aware of where you can totally scramble up longer words and have them be easily decipherable, no. In fact it seems rather impossible or at least defies the entire concept of having an advanced language. However, a very simple langauge that uses pictograms (such as just using emojis) to convey ideas won't have anything to scramble, and in that sense could work.
[removed]
So, as u/polygonsaresorude said in another comment, the idea that just the first and last letter have to be in place has been disproven by a few studies. [This one in particular](https://academic.oup.com/cercor/article/27/11/5197/3056465) shows that phonological processing (reading letters) happens in stages in different parts of our visual cortex. Basically: we see dots-->lines-->corners-->things that may resemble letters --> letters --> letters in all fonts, colors, and sizes, even when turned backwards or upside down-->letter pairs. Our visual cortex doesn't read each individual letter at a time. To maximize efficiency, it processes pairs of letters simultaneously. Therefore, in those pieces of jumbled text, usually just the pairs of letters are switched around. The words are harder to read if you really move an individual letter from the front to the back of the word without its pair. 

Note that this is just for words that can be read phonologically (according to alphabet rules). We process words like "yacht" in the temporal lobe because the brain has to recognize that this is an exception word and reads the word as a whole. 

Chinese languages are not processed in this scaffolded form like other languages. You cannot switch the graphics within pairs. Instead, every graphic representation is processed semantically, as a whole word, much like we in English process "yacht." In sum, no, you can't switch the graphics around in Chinese like we can in English.
Arabic is an alphabet based language...abjad is a type of alphabet. While it is possible to understand jumbled words, there is a risk of changing the meaning, as sometimes changing letter order is how a word is conjugated. Ex. Switching the middle two letters turns it into a different word entirely..example in english lair vs liar, switching the middle letters makes a totally different word.

تصبر - be patient تبصر - look
I disagree that you can’t play around the Chinese characters while still keep them readable.

One perfect example is the so called Martian language in Chinese internet community. About a decade ago, a noticeable number of the younger generation in China started using the Martian Language when posting on the internet. They used this “language” to distinguish themselves from the older generation and who were not fashionable enough. Later on, it became a symbolic behavior of the Smart subculture. What the Martian language do is basically replacing each correct Chinese character with another one that looks similar but with more strokes or less. Sometimes a few random meaningless decorative characters were added too. For example,

Simplified Chinese: 我很怀念过去,但现实告诉我那只是过去

Martian Language: 〆、莪佷懷淰過厾，泹哯實哠訴莪哪呮湜過厾ゾ

English: I yearn the past a lot, but the reality tells me that it’s only in the past.

You may noticed that each corresponding character has some common part with the original character. How normal Chinese character is converted doesn’t have a unified rule. It largely depends on the tool being used. But normally people are able to read those sentences without too much trouble. In Chinese language, it doesn’t make much sense to randomly scramble the strokes in a character especially when typing with a computer, but the Martian language shows that it’s still possible to read Chinese when the characters are changed.

Interestingly, the Smart subculture later was considered not so smart by the majority of the internet users. Then less and less people use Martian Language.
Korean can't necessarily change the order of the syllables, as it can change the meaning entirely:

**네가** (nega, but also commonly pronounced "niga") means *you*

**가네** (gane), means *I guess/I see you're going*

But, what it *can* do is change up the letters within the syllable blocks, replacing them with similar ones (ㄱ for ㅋ, ㅔ for ㅐ, etc.) or throwing in empty letters, so that a fluent Korean reader would still be able to read it:

**안녕하세요** (annyeonghaseyo, meaning *hello*) can be written incorrectly as **않영햐새요**, but it would still be legible to someone who knows Korean.

Here's a funny and related story ([pic here](https://file3.instiz.net/data/file3/2018/01/29/9/6/9/969f65bcfe54d280b5a9dbe54993926d.jpg))- A Korean person left a review on AirBnB about there being roaches, but wrote about the bad parts in this cryptic way and left the compliments in normal spelling. Evidently the AirBnB owner ran the review through Google Translate, was only able to read the compliments, and thanked the Korean guest.
[removed]
Arabic is like this by default. There's two basic ways to write Arabics, with vowel markings and without.

With the vowel markings, it's basically the same as alphabets like  كَتَبَ (Kataba - to write) vs كُتُب (Kutub - books) 

Without the vowel marks, كتب depends on context. If you see اريد ان كتب - then people know it's "to write" because "I desire to books" doens't make sense.
As an Arabic speaker, I’ve never really thought about it until now but I would say it still works in Arabic but is much harder to understand. Arabic relies heavily on accents to determine the meaning of words and so sometimes the same word or very similar words will mean different things based off accents. By mixing the letters you lose any track of where the accents are. I think I would still be able to understand some words but I would have a much harder time than I would reading a mixed up sentence in English.
[removed]
I speaks both Chinese and Japanese and I could tell you that similar things happens in both language, although in a bit different way.

As you all know Chinese and Japanese (Characters/Words) are based on blocks of "graphics" (We call "stroke"), so it's still okay for us to read if parts of the "graphics" are not "drawed" correctly. Actually in old time Calligraph (writing with brush pen), drawing part of stroke in a different way is recognized as an art form of writing. Showing the high intelligence of the writer.

However, in the other way, a lot of Chinese and Japanese words have similar sound but different graphic (like 'see' and 'sea') and sometimes people just got mixed but we could still read and understood the meaning the writer wants to say, however its recognized as evidence of low writing ability....
[removed]
There is a game that people play in Korean that is similar to this. I don't know the name of the game, but they do it with full sentences.  For example if you took the common thank you, in Korean, "cam-sa-ham-ni-da" 감사합니다 they would write it like this for the game ㄱ ㅅ ㅎ ㄴ ㄷ, which removes everything but the initial letter/sound and the players would have to guess what word or phrase they're trying to say, first one to answer, wins the round.   Not the same as jumbling up letters in English, but along similar lines, to me, at least.
Definitely not. In my native language, Malayalam, to add the "o" sound to a letter (each letter represents a syllable defaulting have the "a" at the end, for example "സ" is "sa") you need to put a specific type of "e" behind the letter you want to change, and put a specific type of "a" in front. So if you jumbled the letters, it would be illegible.
In Vietnamese, the language is tonal like Chinese but because of colonization, we use Latin characters with symbols on vowels to express tones. A lot of the time when you send a text message (so this is a modern concept) we send text without the symbols. Meaning there are many many possibilities of each word, string of words and sentences. But you use context clues to understand what the person sent you. 

For example, without symbols. Ba , con doi bung qua

Examples of words With symbols for tone:
Ba=dad or three; bà=Old woman
đói=  hungry ; đôi= pair; đổi=swap/switch

With context it Translates to : Dad, I’m really hungry!



This question is through the lens of a western/English language so the answers ITT are quite interesting.
As a fluent Chinese reader and amateur Japanese reader, I can safely say that this phenomenon exists but in a different way.

Single alphabets most likely do not possess meaning, unless they form a pattern which is known as a word. Chinese and Japanese like wise has strokes like 一 丨 丿 丶 which makes no sense individualy, but form meaning when gathered like 术 (arts, skills) and 木 (wood). Notice the similarity of the characters? Bogus handwriting of displacing the dot on 术 from the right side to the left most likely does not affect fluidity of reading in context, and more complex characters like 躁 and 噪 may be sometimes misused even by Chinese users. Still, it would make sense in context.

Chinese is also still readable as a scrambled sentence like when you displace the verbs and nouns here and there, which is not surprising given the way people creatively come up with slangs to simplify their lives. The brain simply recognizes the important phrases and ignore the grammar integrity.

Furthermore, when multiple characters form a phrase recognizable enough, the phrase can be cross recognized when placed across lines or even scrambled everywhere. The unique 'block' characters allow phrases to form crossword girds and make things mind boggling while still barely making sense at the same time.
The analog to letters in Chinese characters are radicals. Each character consists of multiple (usually 2 or 3 radicals), with some characters having special strokes.

Usually the first radical indicates a sort of categorization of the word, and the second radical is a phonetic.

Note: this is true only of simplified chinese, since traditional chinese is descended from a time when the pronounciations of words was different and characters were separate from the pronounciation.

Now radicals can be arranged in several ways - it can be left/right (most common), it can be top/bottom, it can be outside/inside. If it was three radicals, the common arrangements are left/right-top/right-bottom, top/middle/bottom.

So if we take the analogy to the further extent, the question is whether Chinese can be read if the radicals were placed in a wrong arrangement. Further to complicate, when certain radicals are placed in certain spots, the form changes. For example the radical for heart if placed on the bottom is 心 but when placed on the left it becomes 忄

For example 态 vs 忲. They both have exactly the same radicals in a different arrangement, and they mean completely different things. The first one is one's emotional/mental state, the second means extravagance

Also 杏 vs困 vs 呆 all same radicals. First is almonds, second is to restrain, third is to be blank. 

There are many characters where this is not possible, but there exist characters that there are alternate arrangements that get different meanings. Hence the answer to this question posed by the OP is that it really depends on the character in play. There are some character that so obviously must be a particular character that no matter how you re-arrange it, people will know you made a typo, yet there are some that can be morphed into characters with a different meaning.
[removed]
[removed]
After reading about this effect about 10 years ago I wrote some code to randomly mix up letters in words on web pages. Even with longer words scrambled much worse than in the headline example, it was still easy to read. Such a strange brain phenomenon.
I would say something like this might be possible for Korean, not really possible for Japanese and Chinese. Korean has certain pronunciations associated with each part of a character, so there might be a way to switch their orders or use similar sounding parts to construct a character that could be unscrambled easily. This is impossible with Chinese since each Chinese character has an assigned pronunciation. Some people will use different but similar sounding Chinese characters in their sentences to be cute or try to achieve comedy, but changing them too much or rearranging the order of the characters in a word would make it too hard to unscramble
Not chinese and japanese because those languages are very verbal. Almost like the written system was made to actually reflect what speaking sounds like.  


For example in English we don't specify how a word sounds and thus we don't look to convert characters into their audio version. We convert the entire word into a sound. Sound unique to the word rather than the component letters.  


So we can read your title because in English my brain is only looking for "You're" characters not the order or anything (because we are taught knowing it doesn't work that way). In Japanese for example summer is  なつ  (Na Tsu) Using the  な will always result in a Na and つ

always sounding like Tsu. Because of this, native speakers brains are wired to trust character sounds .
[removed]
[removed]
We can calculate things like gravity and energy density of the universe base on how the galaxies behave on a cosmic scale.  We can also calculate how much stuff is out there by direct observation.  When we look at the cosmos and look at how the galaxies behave, there is not enough material to generate the gravity (even when accounting for all the gas and dust that may obscuring it).  So there's some matter that doesn't emit light but still generates that gravity we see.  We call it dark matter, because that's what it is: dark.

There's also an expansion to the universe that suggests that the energy density is not what we can directly measure.  There's a bunch of stuff out there causing the universe to expand at an accelerated rate.  We call it dark energy, because, hey, we have dark matter already--why not call it "Dark Energy"; that way it sounds cool.

So there's like 5 times as much matter as what we can see, and like 3 times as much energy density as what can be explained by that matter.  So that's where the percentages come from: just add up all the stuff we know about that makes up the universe even if we don't yet know what that stuff is.
In layman's terms: dark matter is *something that generates gravity that we can't see*.

We can calculate the gravitational forces that impact everything we can see - stars, planets, black holes, dust, etc. We can calculate how much gravity the things we observe is generating. The problem is, they don't add up. There's way more gravity affecting everything than what is being generated. All of that unexplained gravity is just generalized as dark matter until a better explanation comes along. Same kinda deal with dark energy.
For dark matter and ordinary matter, we basically measure the *total* matter first (since it all gravitates the same), then differentiate between the two using a second effect. 

The total matter (23%+5%=28%) is roughly given by the ratio of heights of the first and third peaks in the [Cosmic Microwave Background angular powerspectrum](https://en.wikipedia.org/wiki/Cosmic_microwave_background#/media/File:PowerSpectrumExt.svg) because the third peak mostly evolved (by gravity) when the matter content of the universe was negligible, and the first peak mostly evolved when the universe was matter dominated. So the third peak is kind of a calibrator letting the first peak tells you how much matter there is.

To differentiate just the "ordinary matter" part, you can get that from the CMB too, but an even cooler way is by measuring how much of various atomic elements there are in the universe in little pockets that appear to have been pristinely left from the Big Bang. In these pockets, you see that something like 24% of the atoms are Helium-4. The amount of Helium-4 formed by the Big Bang depends really sensitively on how much ordinary matter there was, i.e. the ordinary atoms that are the building blocks of Helium. Run the math and that 24% number tells you about 5% of the universe is normal matter. Then 28%-5% and you get the part which is "missing", i.e. dark matter. 

As for dark energy, well we've just measured how much matter there is (I actually cheated a bit above, those things tells you the total amount of matter, not the percentage relative to everything), so we can calculate how much all that matter gravitating in on itself should slow the expansion of the universe. Then you go and measure the expansion and find, lo-and-behold, not only is the expansion not slowing, its accelerating! Using supernovae as beacons to precisely measure the acceleration rate, you work out how much dark energy there is, giving you that 72% number.

There's probably like 10 other ways to measure these things than what I just described, cosmologists are constantly looking for new ways to do so and comparing the answers against each other, in hope to find a crack in the model pointing to some new things we don't understand!


I actually wrote about this during the last bit of my degree in physics. I would link it to you but a) it’s a bit complicated for non physicists and b) it’s in spanish. So let me explain it as clear as I can:

As you may or may not know we use General Relativity to study cosmology. The usual model uses the FLRW solution to Einstein’s field equations(EFE). This solution is used for a homogeneous and isotropic space-time (that basically means the contento of matter/energy is the same everywhere and in all directions). So the equations that stem from the EFE using this solution do not t fully describe the behavior of space-time, we need something else: A content inside this universe. For this the simplest model to use is a fluid (a perfect fluid, a fluid which has some properties that make it easier to use than a more real fluid). When you look at galaxies in a sufficiently big scale, they can be modeled using a perfect fluid. Now we can work using the EFE and the equation for the fluid to have a description of the evolution of the universe. 

So suppose we have the usual stuff that we know: matter and light (yes, light can be modeled as a fluid). We do a study using these two fluids and... we arrive at the conclusion that the universe is expanding but it is expanding slower every time. But then we have a problem, we observe the universe and we find that the prediction of the model is wrong and the expansion of the universe is accelerating. We need more matter/energy in the universe for this to happen so we take another usual model called: ΛCDM (a model with Cosmological Constant (or Dark Energy) and Cold Dark Matter) and we test it and we see that it makes a much better adjustment to the equations. From this we infer that we have around 30% of CDM and 70% of Λ.

You might still be wondering how we arrive at these numbers, I haven’t explained that yet. So now I’ll do it.

To perform the studies I roughly mentioned above we use data from a particular type of supernovae: supernovae type Ia. This kind of supernovae emit a pretty standardized amount of light and energy. So the data used is something called “distance modulus” it’s some kind of comparison between absolute and apparent magnitudes (the apparent measures the brightness of the object how we perceive it and the other measures how bright it would be at 10 parcecs away from us). This quantity is linked with the distance by a little equation. And the other data needed is the reshift of the object (basically how far away it would be, or how “red” it’s light appears to be due to its relative motion from us because of the expanding universe). We have this observational data captured from huge supernovae samplings. And we have the equations that give us a theoretical expression (an expression that has Dark Energy, CDM and redshift as parameters) for the distance modulus. 

Now for getting those numbers we need to make a statistical study if said data. We “compare” the data with the theoretical input and try to minimize the difference with the observational data of the distance modulus of the supernovae. What we do is we sample a lot of values for Dark Energy and CDM and obtain a range of values using a Markov Chain in the Metropolis-Hastings method (at least that’s what I did), obviously we use a computer for this. We input some arbitrary values for DE and CDM we make a comparison between the calculated distance modulus using these values and the observational data then we put some conditions that make the program make the decision if that value is useful or not and if it is it keeps it and if it’s not it “walks a new step” (that is it defines a new value for DE and CDM) and once again the program tests if these new values for DE and CDM are useful or not and so on until the program finishes the amount of steps we want it to take. We end up with a bunch of chains made of different values for DE and CDM. We plot some histograms and take the averages and what would you know, the averages are the 30% and 70% you know about. Does that mean that’s what the universe is made of? No. In this case the averages are also the values of DE and CDM that are most likely in our universe. 


I hope this helps you understand a bit more on what is actually done in this type of research and I really hope you read my comment because it took me like half an hour or more to write. If you have any questions you can ask me and I’ll gladly try to answer them.

Edit: I messed up a bit (not counting all the typos) where I said you need a content for the EFE to be useful. That’s not entirely true, once you have the perfect fluid (in this case) there’s still something missing for the equations to be useful, we need more information about the fluid: the Equation of State. It’s a relation between several thermodynamical properties but in the case of a simple fluid used in cosmology we only need an equation that relates the pressure of the fluid p and it’s energy density ρ (for example, for light I think it’s p=1/3 ρ).
There are several answers giving our estimate for dark matter within a galaxy using rotation curves, which is true.  But overall in the universe, our estimate comes from the scale of the fluctuations of energy density in the early universe.  The cosmic microwave background (CMB) is radiation left over from the period in the universe when the temperature finally fell just enough for electrons to join with nucleons to become atoms, and with no more free charged particles, photons became free to travel in the universe.  What we can do is look at the CMB radiation and map the scale of energy fluctuations over very large scales in the early universe.  There were several forces at play determining the shape of these fluctuations.  Gravitation tried to pull things together, whereas radiation pressure pushed things apart, and the curvature of the universe played a role.  If you plot the angular power spectrum of the CMB, as a functino of the multipole moment, you get a multi humped shape, and these humps gives us information about the shape of the universe (essentially flat, which gives us information about dark energy) and the ratio of matter not experiencing pressure (dark matter) to regular matter.  This is really where we get the estimates in percentages of dark matter, dark energy, and regular matter.
Basically from gravitational equations.

The famous 'discovery' of dark matter is from astronomers measuring the rotational velocities of stars in a galaxy. 

When you look at planets orbiting the sun the inner planets orbit much faster than the outer planets. We expected to measure the same kind of difference in speeds of stars in a disc galaxy (like our own). We did not.

Instead they found that the outer stars are basically going the same speed as the inner stars. Almost like the entire visible galaxy was actually set in an invisible sphere and the spheres' rotational velocity was essentially the rotational velocity for the entire system. (Think about one of those cool hand-blown glass marbles and how the designs inside all rotate around the center at the same rate).

The only solutions, mathematically, that fit the observed data are: our formula for gravity is wrong (it might be, but it's far too predictively accurate to be off by enough to account for these observations) or, there's a whole lot more mass than we can see in the system.

By 'can't see' scientists basically mean 'non-interacting' with other matter. So they crunch the numbers to figure out how much mass the dark matter is compared to normal matter (x% normal matter + y% dark matter = z rotational velocity). This is where the percentages come in and therein is the answer to your question.

Dark energy is a little different but also kind of similar. Instead of using gravitational measurements we made velocity measurements of red-shift (talking about the expansion of spacetime  with respect to entropy) and found our x% normal energies did not equal the z velocities that we observe. Again, crunch some numbers and solve for the unknown y% dark energy and bam.

There are a couple good documentaries on the idea around. YouTube has several good explanatory videos as well. I would recommend PBS Space Time, the host is a little cheesy but very informative.
Imagine you have a big pool of water that is completely calm. If a single rain drop hits that pool is causes a [circular ripple](https://i.stack.imgur.com/fWFL6.jpg). By looking at how that ripple propagates through the water you can learn about the water. For instance, the pattern of the thicker circles and narrower circles tells us about the phase velocity and group velocity of water waves. Importantly (for the analogy), if you drop this pool of water (making it essentially weightless), you would remove the restoring force that makes the wave propagate and you would see the ripple stationary on the surface of water. 

Now imagine that there isn't just one rain drop, but many rain drops falling on the surface of the water. You drop this pool of water, making it weightless, and you see a messy pattern frozen on the surface caused by the interference of all the water waves with each other. But, if you take all the high points in this mess of water and  superimpose them on top of each other you would get an image that, on average, looks like single a circular ripple of water. You can then do the same analysis you would with a single drop of water to learn about the water itself. 

We do a very similar analysis to find out the composition of the universe. At one point the universe was dense, and all the matter and energy was sloshing around inside an expanding universe. These are called baryon acoustic oscillations. However, at some point the universe cooled and became sparse enough that it stopped sloshing around because it couldn't interact with itself any more. The restoring force for the sloshing was removed, or the "pool was dropped". The remnants of this is called the cosmic microwave background (CMB), which is analogous to the frozen pattern of raindrops on our weightless pool. 

The CMB is a measure of how much matter there is in a part of the universe. If you take all the hot spots (and cold spots), with lots of matter (and little matter), and superimpose them on top of each other we get a [pattern very similar to the ripples in water](http://inspirehep.net/record/1224727/files/Figs_spots_pol_stacks30.png). By studying the peaks of those patterns we can lean about the type of matter and energy that caused the ripple. In that image the measured data are on the top and the bottom is what we get from the best fit model (LCDM a model with cold dark matter and dark energy) that tells the the numerical values for the normal matter/dark matter/dark energy composition of the universe that you mention in your question. 

edit:  Force instance -> For instance 
[removed]
Dark matter and Dark energy are two very different things. It's like comparing apples and oranges.

So, lets deal with matter first. The combined mass of all the stars in the milky way accounts for around 10% of the mass of the Milky Way. All of the dust and gas clouds account for around 10% of that. So the mass of the milky way is ~89% Dark matter.  Planets aren't included in the calculation, as the total planetary mass would be insignificant at the orders of magnitude of mass being discussed.

So what is dark matter? We have no idea. Although it sounds very sinister, it's called dark simply because we cant see it.

Dark energy is the energy that is making the space between our galaxy clusters expand at faster than the speed of light, and increasing. If there was no such thing, the expansion would be slowing down, or the universe would be contracting under gravity.

Exactly what is dark energy? Again, we haven't got a Scooby.
Has anyone considered the energy does not emanate from within the universe but from outside. For example, if you were in a boiling pot of water and all you could observe was the water and could not see past the pot. Examining all the observable properties of the water you couldnt account for the energy causing the heat. You would indeed conclude there is dark energy somewhere in the water. But you'd be wrong. It's coming from outside the water. Could it be a force from outside the universe is causing the ever quickening expansion? 

In fact, if something were in essence pulling apart the universe wouldn't that account for the energy in a black hole. Giving black holes giant suction in the same way when water gets pulled through a drain. Instead of water the universe is filled with gravity. 
Dark matter is estimated for the missing mass that keep stars in galaxies as they are now. Visible matter alone cannot make spiral galaxies stable as they are now, the outter stars in the spiral arms would fly away. Dark matter is the mass we cannot see. It's perhaps black holes, dark stars, cold gases and could even be some exotic matter that is dark; hence, "dark matter" name.

Dark energy is estimated from the acceleration of the universe expansion. It's the missing energy needed to match the observed acceleration rate. Since energy and mass are just two sides of the same coin, they can describe dark energy as missing mass too.
There have been some excellent discussions here about the matter side of things, but there is a component you and others aren't talking about: curvature. The curvature of the universe is a very important part of how we calculate the 71.4% figure.

We know^1 the universe has at least three components we can measure (matter, light, and curvature) and one that we can't (Lambda/dark energy). These quantities are related via the Friedman Equation such that

Omega_matter + Omega_light + Omega_Lambda = 1 - Omega_curvature

The three terms on the left can be written as Omega_Naught, so this is often written

Omega_Naught = 1 - Omega_curvature

Today, the energy density in light (via photons) is essentially zero (~1e-5 ), so we'll discount it for now. The total matter density is measured directly from the power spectrum of the CMB and is 28.6% with visible matter only contributing 4.6%- the rest is in dark matter. We have measured the curvature to be 0 to within a few parts in a thousand (from the Planck satellite) and the ESA's Euclid mission will likely affirm this to an even greater precision. That means the above equation reduces to

Omega_Naught = 1

or

Omega_Lambda = 1.0 - 0.286 = 0.714.

Even if Omega_Lambda isn't Einstein's Lambda, it has to be something that accounts for the remaining energy density and causes the acceleration of the universe.

---

[1] *How* we know this requires several lectures of observational cosmology, but the crux is the famous "banana diagram."
Dark matter is a bit of a misnomer. Dark mater could in theory not even be matter but some as of yet unexplained gravitational phenomena. We know something is going on because galaxies should fly apart under their own spin yet they stay stable for billions of years. Galaxies should also all collapse into each other over time. However we dont see that. In fact, we see galaxies moving apart at an ACCELERATING rate; kinda like a big explosion. It's been almost 15 billion year since the "big bang" and we would expect galaxies to start slowing down or even collapse in toward each other by this time but again that's not what we see. Thus dark energy was invented to explain this.

Really dark matter and dark energy are just two things in physics that are major unexplained phenomena so we slapped a label on them that kinda makes sense in a round about way but doesn't really make any sense at all.

Dark matter could be extradimensional stoner aliens just as much as it could be matter that doesn't interact with light. Hell dark matter could even just be some form of energy we don't know about or wouldn't expect because energy also affects gravity just like matter does (but it takes a LOT of energy). The truth of the matter is that we need a lot more information before we can begin to understand what exactly is going on.

I would highly recommend you read astrophysics for people in a hurry by Neil deGrasse Tyson. He covers this in pretty good detail over 2 chapters. He’s also quirky and comical. 

PS— I was a physics major for 2 years & have a BS Mathematics degree.

The book is a great read, pretty cheap too. 
Could it be that our models are simply wrong, rather than postulating another *"dark this or that"* ?  
  
It seems to be another placeholder, like *"the aether"* of the 19th century, for the errors in our theories that will eventually prove the theories to be inaccurate.  
  
I haven't heard  much from the dark energy skeptics in popular science...
Dark matter and dark energy are essentially placeholders, we have what we've observed in the universe, and we see bits that don't quite line up.

Now, this could mean either we're wrong or the universe doesn't behave the way we think it does.

So we do more tests and make more observations and we keep finding the discrepancy, so we decide it must be a lack of understanding. Now we could try to adjust our theories to accommodate them, but afaik, noone's successfully done so, so we went with the simpler method of "there's  something we can't observe, but is having this effect".

And then you calculate how much of it there is by the discrepancies we've found
Dark Matter is relatively easy to understand.  If you look in the Solar System, Mars revolves around the Sun at a distance 1.66 that of Earth.  That means, in one orbit it goes 1.66 the distance Earth does.  But the actual time it takes is 1.88 years instead of 1.66 years.  That is because being farther from the Sun, Mars needs to go SLOWER to stay in its orbit (and not fall into the Sun or fly away).

At a galaxy level though, this is not what is observed.  The equivalent of Mars needs to go faster, and have a 1.66 years orbit instead of 1.88 years. It needs to go faster because there is more mass, and this mass is distributed weirdly (as if there was another part of the Sun somewhere between Earth and Mars).  This mass gives no light and does not seem to interact with regular matter in any way.  This is Dark Matter.

Dark Energy is more along the lines of  /r/blackmagicfuckery.  If you launch a rocket at 60 miles per hour, in 1 hour it would be at 60 miles away from you, in two hours it would be at 120 miles away, and so on.  But when galaxies do this, it goes from 60 miles after 1 hour, to 180 miles away in 2 hours.  Source of this is what we describe as Dark Energy.

The "180 miles away in 2 hours" and "1.66 years orbit instead of 1.88" gives us the 23% and 72% numbers.  (Actual numbers are not 180 or 1.66, but you get the idea).
Also, it'll be interesting to know how flawed our existing methods of estimation and calculation are and how many more dimensions exist. Maybe a few hundred years down the line we will know that what we see and know currently is nothing we compared to the reality. Maybe there's a lot more things than dark matter and dark energy 
I saw Lawrence Krauss explain this really well once. 

https://youtu.be/pnmC3zfIV8E

The whole thing builds to the answer, but you could probably skip to about the 30 minute mark and get a decent idea shortly into it from there.
I believe those inferences come from the observed rotational velocity of galaxies being far beyond gravity field that observed matter could cause in the case of dark matter, and the apparent acceleration of the expansion of the universe seeming to run opposite of what mutual gravity of all objects including dark matter should cause, dark energy seems to be a repulsive force that overpowers gravity caused by both matter and dark matter on an intergalactic to cosmic scale.  That’s my understanding I don’t think the percentages are “accurate” but rather vigorous  estimates as accurate as possible.  The observed effects of dark matter and dark energy are correct, the existence of those two are not uncertain.  Nailing down exact percentages though most likely can not be done.  That’s my basic understanding.
Check out Kurzgesagt - In a Nutshell. They make neat, easy to understand introductory videos on a wide range of complicated scientific subjects. It's a good start to get the basics and move on to more thorough videos. They have one on Dark Matter and Energy that's good.  

https://www.youtube.com/watch?v=QAa2O_8wBUQ
You’re right that there is a similarity between the Michelson-Morley experiment to detect the ether and these attempts to detect dark matter, with their expected seasonal variation. 130 years apart!

However the ether was hypothesized purely as a medium for light waves to oscillate in. That turned out to be unnecessary. The ether would exist uniformly everywhere. Dark matter on the other hand isn’t a medium but a particle, and it’s highly clumpy in where it appears. It has structure that changes over time. Clouds of dark matter collapse and rotate. So it explains different phenomena and has a bunch of qualities the ether was never thought to. 
Dark energy and it's percentage has ties back to the 1920's when enstien was kicking around the idea of a cosmological constant in his field equation(iirc) He couldn't quite get the maths to workout with a static universe so he threw out that part of the theory. 

Later when Hubble discovered the universe is expanding the calculations were tried again and have proven to be correct if the constant is expressed as a positive. 




The model for the behavior of the universe is reasonably well locked in, and since they would have had an effect on the CMB, we can extract these percentages from it. I have written a small research paper on it, if you would like to take a look, but it sits about at the top of Undergraduate Level Physics
> I'm aware of what dark matter and dark energy are. I'm by no means an expert, but I do have a basic idea. I'm wondering specifically how we got those particular numbers for them.

Without trying to be rude, I don't think you are aware. If you were, this question answers itself.

Dark matter for example is simply a name for of mass that we can't see inside a galaxy but we know must be there. We know it must be there because of how fast the galaxy is rotating, and how much gravity must therefore be present to cause it to rotate that fast (not enough gravity and everything would fly apart).

In other words that's how we learned dark matter exists in the first place. We calculated the amount of matter we can see and it's not 100% of the matter we know must be there because of the rotation, so the remaining percentage we say is dark matter.
Rephrase your original statement: 23% of the observed gravity in the universe is from unidientified sources within our universe while our universe is under the influence of gravity causing an expansion of matter that is not explainable by the gravity emanating from matter within our observable universe, and only 5% of the total gravity observed is from identified matter.

That sentence reads close but not exactly to my understanding of the subject.
I felt this was covered well in 'Astrophysics for people in a hurry', if you have 3 hours to listen.
As a follow up question: do people inside the field consider it to be in shambles because we have so little understanding of such massively impactful phenomena? I mean literally as the universe expands faster and faster the observable universe is getting smaller. Future generations could very well think our galaxy is the only one. That’s unsettling to me. Do physicists have a fire lit under their ass or what?
Neil Degrasse Tyson said that we might as well call dark matter and dark energy “Fred” and “Wilma” respectively because there are really just place holders for the terms until we understand them. They have NO idea what that stuff is comprised of. Essentially giving it that names gives off the vibe of “hey we know what this is” when from what I’ve seen on documentary’s they have no idea. 

*source- endless hours of 4:00am documentary watching 
The thing is, they could just be matter and energy, but we are incapable of observing them outside of how gravity on very large scales acts.

It's entirely possible that 95% of the universe is just simply beyond our ability to observe it.

We call them dark because we can't directly observe them. Not necessarily that they're some odd phenomena. It's entirely possible they're just normal matter and energy, just not observable in our current observable universe.
Dark energy and dark matter are nothing more than peaks and troughs in the fabric of the universe. Much like a blanket on a bed, it doesn't lay completely flat, I'm not talking about lazy job either with ceases. That's all it is, just bumps and dips in space time 
Not every phenomenon can be measured directly.  This is similar to the problem of measuring the width of a lake, like for building a decorative bridge for example.  Despite I know a few people crazy enough to swim from one side of a lake to the other with a tape measure or rope, that is not a feasible way to measure it.  I forgot all the words for the tools used to measure, but surveyors use a series of instruments and follow up with trigonometry to get the job done.

As for astral bodies, we are basically making an educated guess based on the facts we know about physics and nature and the theories that have not yet been disproved.

Most scientific discoveries made were derived from events that have occurred in our environment.  That really does not say that dark matter does or doesn't exist.  I wondered how scientists knew that the Earth's core is made of iron when no one ever dug through the magma layers to see it.  Mad scientists made discoveries about how iron acts when under certain conditions, conditions that don't naturally exist on Earth's surface, but may exist given what conditions we assume should exist for iron deep underground.

Maybe another example is a really hard sudoku puzzle.  We know a blank spot is this number because of that.

People are still making discoveries, but sometimes discoveries are wrong.  It happens, and we learn.
Not a physicist but you just gave me an idea. How does enthalpy work with antimatter? Since it's a compliment to matter and matter runs order to disorder, would antimatter possibly be opposite? What if order to disorder with matter is 'recharging' the world of antimatter, and the big bang event is a reversal of the roles?
Is it possible that the gravitational constant might not be truly constant throughout the entire universe?  Like maybe it is a certain value within our galaxy/solar system but has a different value elsewhere.  This might account for our observations of distant objects seemingly affected by more gravity than possible. 
Related question: whereas we can 'weigh' ordinary and dark matter by looking at how gravity is affecting visible objects and light itself, what's the equivalent procedure for 'weighing' dark energy? How do we determine what percentage of the energy in the universe is 'dark energy'?

EDIT: based on OP's edit, I see I'm actually just rephrasing the original question.
Here is a really stupid question regarding dark matter; could the missing matter simply be "normal" matter we can't see? As in, its not reflecting enough light for us to detect? So in essence, the amount of matter we are calculating to be there is just way short of how much is actually there. So instead of saying our mass calculations are incorrect, we are making up a mysterious thing called dark matter to account for the missing mass??
Very short summary: universe is expanding , it shouldn't be. The fact it's expanding means there has to be extra mass (and energy) hidden somewhere. Astronomers introduce fudge factor which is the difference between our theoretical calculations and the experimental values. They give the fudge factor a cool name.
[removed]
I know what it is
[removed]
[removed]
Has anyone hypothesized that dark matter is all of the matter that has been absorbed by black holes? I'm not particularly educated in this area so I'm am not aware of any fine details, but my current understanding is that matter gets pulled in and compressed to the nth degree. But does it just stay right there and never moves on the other side or do things drift around?
It’s the lack of a specific energy needed to make the universe expand given the known circumstances. We then calculate by how much is it off ...

Huge oversimplification - but maybe it’ll help further down the chain
Canadian checking in here. 

**This comment has been updated with better info and links for the sake of clarity, see below for new info**

Original Comment:

>As far as I can tell from my research into how this affects Canada, there is only one undersea fiber cable linking Canada's internet to the rest of the world that doesnt go through the US first. That link goes to Greenland and reportedly has had frequent issues since it was built due to poor construction. Aside from this Greenland link, all other wired Canadian internet traffic goes through the US first before going to the rest of the world. The US could effectively cut Canada off from the internet if it wanted to.

>However, there is a proposal to built a new, modern fiber link through the Canadian arctic that would link London, UK with Tokyo, Japan. This would significantly reduce latency between Western Europe and East Asia while also bypassing the mainland US. It would also provide gigabit internet access to thousands of remote Inuit communities in the Canadian Arctic, which could have life changing effects on their economies.

**UPDATES**

Thanks to /u/RcNorth and /u/markszpak for highlighting [this more detailed map](https://www.submarinecablemap.com/#/submarine-cable/gtt-atlantic) than the ones I based the previous version of this comment on. This more detailed map clearly shows that there are 3 fiber links from Halifax to the UK in addition to a fiber link up to Greenland that I mentioned previously.

However as described by /u/SoontobeSam:

>As a former network operations technician for a Canadian ISP, this is correct, telegreenland's cable is the only subsea fiber I am aware of that does not enter the US before Canada, our other main access routes are in Toronto and Vancouver, but both connect to the US to access international networks.
I can also confirm that their network uptime is mostly ok, but when they do have issues it takes forever to get any progress and dealing with ongoing non outage issues is difficult, also Newfies and Scots have a serious language barrier even though they're both speaking "English".

So while my initial remarks regarding the US basically being the gatekeeper for Canada's access to the wider Internet may be more or less correct, I was incorrect in saying that the Greenland fiber link is the ONLY fiber link Canada has to the rest of the world. While the Toronto, Halifax, and Vancouver links /u/SoontobeSam mentioned appear to all go through the US in some way first which technically restricts Canada's direct access through those links.

**Arctic Fiber**

By popular request here is [the link to the site](http://qexpressnet.com/system/) for the fiber link through the Canadian Arctic that I mentioned previously. The project was formerly known as Arctic Fiber, but has been re-branded as the Quintillion Cable System after the name of the company task with installing the cable. Yes, you read that right, this project has gotten the green light since I last checked up on it (I didn't have time to check on my way to work when I commented originally). They just completed Phase 1 which covers Alaska, and will be starting the Phase 2 to expand through Asia to Tokyo soon. Quintillion has also built a terrestrial link through Alaska and down to the mainland US in order to provide connection to existing connection hubs on the west coast.

*UPDATE 2: Just have to highlight these two awesome users comments:*

User /u/KrazyTrumpeter05 posted [an awesome comment](https://www.reddit.com/r/askscience/comments/7ez85x/with_all_this_fuss_about_net_neutrality_exactly/dq9ir7v/) with more info about Canadian Fiber connections, and also linked to [this 293 report](http://subtelforum.com/products/submarine-cable-almanac/) they claim to have played a major role in writing about Internet Fiber connections around the world. Thanks for the fascinating info!

User /u/Fochang1 posted [this fascinating comment](https://www.reddit.com/r/askscience/comments/7ez85x/with_all_this_fuss_about_net_neutrality_exactly/dq9ic13/) about how South American/Caribbean nations have a similar issue with the US acting as their Internet gatekeepers. They linked to [this insane Internet Exchange Point](https://en.m.wikipedia.org/wiki/NAP_of_the_Americas) in Miami that routes most of South/Central America's internet traffic. Thanks for sharing this incredible perspective that Canadians like myself would otherwise be oblivious to!

**Some thoughts on the impact of Arctic Fiber**
The fact that this project is actually being built is incredible, because it will mean a huge boost in connection for remote arctic communities that open up massive new economic and information exchange opportunities to these historically very isolated regions. I can't wait to see what the Inuit peoples of Canada's arctic will do with this new link to the outside world. Reconciliation between Canada's indigenous and non-indigenous peoples has become a major focus for Canada in recent years, with the Canadian government [set to fully implement into law](http://www.cbc.ca/news/politics/wilson-raybould-backs-undrip-bill-1.4412037) a 2007 [UN declaration on the rights of Indigenous peoples](https://www.un.org/development/desa/indigenouspeoples/declaration-on-the-rights-of-indigenous-peoples.html). There is a long way to go for reconciliation, and it has been a very rocky road so far, but I hope that this new Fiber link will open up new ways for a large portion of Canada's indigenous population to showcase their own culture to the world and make new economic opportunities for their communities in the digital marketplace.

If you for some reason read through everything to this point, thanks for reading :)
[removed]
I would like to know what the ISPs are thinking of Elon Musk's (and others) notion of covering the planet with satellite based service, and how would they compete with that? It seems inevitable that this is in some form the future of internet. And then as an aside, will the competition be companies throwing up MORE satellites? 
I’m unclear if this FCC “ruling” will only allow them to throttle connections to their subscribers down stream. This post infers that services hosted in US data centres can have their UP connections throttled no matter where the user is. Nothing surprises me about what is happening there, but this would really be pushing it. If US data centres are affected like this, Canadian & Mexican data centres are about to see a lot of new business.

Edit: Make more readable.
[removed]
[removed]
I don't see how any of it would have any impact on anyone outside the u.s.

If you're in Canada or elsewhere, and you're accessing a service in California, the Layer3 provider isn't going to be throttled at all.

The throttling has to happen ONLY at the client level inside the U.S. at the modem for the service to be able to be upgraded as a sellable package, therefore if your ISP is Comcast, their Network HAS to stay fast all the time so they can market those individual services to paying customers selectively.
[removed]
That all depends on who is between you and the services you want to use. ISP are currently using their position between consumers and service providers to try and double dip on everyone. Charging consumers more to access specific sites. And services more to access their customers. If you are not their customer you will likely notice nothing. 

See [this](http://ec.europa.eu/eurostat/statistics-explained/index.php/File:Internet_access_in_households_by_degree_of_urbanisation,_2016_\(%25_of_all_households\)_YB17.png) and [this](http://overflow.solutions/demographic-data/what-percentage-of-american-homes-have-a-computer/) and [this](http://www.pewresearch.org/fact-tank/2014/09/19/census-computer-ownership-internet-connection-varies-widely-across-u-s/). Also check [here](http://www.worldometers.info/world-population/us-population/) and [here](http://www.worldometers.info/world-population/europe-population/). You can see the percentages of households with a computer and internet access is in both around 70% - 80%, just pick 75% for ease of calculation, US having 330mln citizens and EU having 750 mln, that means that there are at least, respectively, 250 mln US citizens with access to internet, and in EU 560 mln. To achieve the best internet experience the datacenters for all big companies that everyone uses are spread across the world, with the concentration of datacenters being proportional to the traffic. Knowing EU has twice as many internet users as US, logically there should be, and probably are - more datacenters in EU than in US. This means that impact on EU will be low but it can lead to other unforeseen consequences outside of "just the internet". Same counts for other regions like Canada, South America, Asia, Australia (they have shit internet anyways) and Africa. Biggest impact (in order) I expect this to have is: Canada, EU, South America, Asia, Australia, Africa. Look for example how IBMs datacenters are [spread around the world](https://prnewswire2-a.akamaihd.net/p/1893751/sp/189375100/thumbnail/entry_id/1_ny63t7c4/def_height/2700/def_width/2700/version/100011/type/1?.jpg). Other big companies probably have the same spread. This would confirm which regions of the world would be fucked most by US net neutrality laws being repealed. Also check out [Azure's datacenters spread](http://africatimes.com/wp-content/uploads/2017/05/microsoft-804x452.jpg) and [Google's datacenters spread](http://farm3.static.flickr.com/2345/2404505335_9f06ed86ac_o.jpg). 

[Here](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/complete-white-paper-c11-481360.html) you can see US will account for "only" (still a big chunk though) 30% of the worlds internet traffic in 2021 (maybe that will change with the laws repealed though). But there's still the 70% of traffic outside US.

[Worldwide](https://data.worldbank.org/indicator/IT.NET.USER.ZS) 45% of the population has internet access one way or another (for 7 bln people that makes 3.15 bln). Of those 3.15 bln users, US counts for 250 mln, or just shy of 8% of the worlds internet population. This means other internet traffic is generated by 92% of the rest population in the world.

So, as of now the 8% of internet worldwide internet population located in US generates 30% of the internet traffic while 92% of the world generates 70% of the traffic.. well good for Americans I guess, that will make the impact a bit heavier on the rest of the world?..

As for other unforeseen consequences, think of intercontinental shipping, they need to send data between them to coordinate stuff, slow it down and it could mean your shipping container gets delayed by a week? Maybe this is an exaggeration but who knows..
The real issue is when people in Germany, Japan, the US, or anywhere make a new web service that everyone loves. It could change everyone’s lives for the better in tangible ways and it would not matter. Either this shit gets fixed or an innovator’s only hope of making *any* money is getting bought out by a giant conglomerate. They won’t be able to keep the lights on.
I think it's a good thing to happen to other countries. If it's to expensive for companies to use US located data center, they will move to other countries. If companies can't have a fair market and can't compete with big companies because of the speed lane charges, the innovation will leave to other countries where they can.

At the end US will destroy it's innovative market and as soon as they realise that Google is about to move it's HQ to Canada, Europe, Japan what ever, they'll roll back the deregulation.

Back then in the pre 2000 time, telecommunication companies charged high amount of money for using their lines. Today we pay less per month and have flatrates. 

I'm the whole thing is just temporal and net neutrality will come back. Sooner or later.
For me (australian), its the notion that the government and corporations see this as the "profitable and successfull" way to run your telecomms and that it could be adapted by my country. I also consume a lot of my media from america. These laws will discourage content creation. If you enjoy podcasts or independant youtube content consider that this could seriously hamper the creators ability to release content.
[removed]
I’m not an expert on anything but..,

In Australia most of our cables head straight to the US. I don’t know how it will be affected but my guess is our speed will not be a priority to US companies.

As mentioned here most of the services we use are from the US. Like Facebook, google YouTube etc. maybe why censorship is so heavy. 

Thirdly our country is a client state so most of the bad ideas from the US are rolled out here a few years later. So if it passes there it will probably be here in a few years.
New Zealand here. We don't have net neutrality.
There's nothing stopping isp's here from charging for content except for healthy competition.

If my ISP screws me I change to one that doesn't

This is what the US lacks. people don't have a choice in ISP they have to use what they have access to.


As I see it, small startups in the US will fail to compete with the big corporations, reducing competition on the global scale. That's the only global impact I can see, but there's probably more that I don't understand. 
As far as I can tell, as long as our countries maintain decent net neutrality laws, America is the only place that's gonna be badly affected. If you happen to be connecting to an American server it might be a bit slower but their ISP's can't charge us fees.
There are multiple aspects to this: commercial users like US datacenters that get throttled might go overseas. Even if only to Canada, Mexico, or even France (St. Pierre/Miquelon). 

personal users: it will just be BOHICA.


[removed]
Does this not create a massive new opening for a new isp that simply chooses NOT to throttle based on payments from websites? Wouldn't this just mean everyone would move to them? 

Or is the start up cost for a new isp simply too high for this to ever be a possibility? 
[removed]
Why is no one talking about the impact of ISP's power to control our access to honest information and news?  HuffPo, Young Turks, ... Where did they go?  If you think the corporate media is useless now, just wait till you can't get any unfiltered information. That is the biggest risk. Everyone is talking about Netflix. 
Geesh from these comments, i gather many people get their information from only one news source.  You can bet the sources that are objectionable to corporate media will the first to go.  That is why I cited examples. We'll always have access to corporate propaganda as long as they control our world.
Far too much, they leave important regulatory bodies in the hands of a single person, and that person can go from reality TV to the highest office in the land, it's insanity how fragile the entire internet has become. 

Losing net neutrality would mean all the top websites in the world will be damaged or become monsters seeking to destroy competitors just because they happen to exist, youtube will become the only video host, facebook the only social media, at least in America. It'll turn into the wild west with people competing to control the ISPs golden boy spot.

No more indie websites, no more innovation, no new social media unless backed by billionaires, all because they can't afford to bribe the ISP more than the leading competitor, who will be owned by a mega-corp with billions to spend. 
Online games may be affected in Europe if the server is based in the US (could even just be the login/account verification systems).

If the game maker refuses to for example pay Comcast 1/2 the monthly subscription per month then Comcast could introduce game-destroying latency into the data stream whenever game data passes through their network.
And because Comcast/Verizon/AT&T etc have state-wide monopolies, there's no way for a game company (or end user) to be able to re-route the data as at some point it MUST pass through that state's officially-sanctified monopoly internet provider.

And you can bet your bottom dollar, that Blizzard (World of Warcraft) will be specifically targetted.  One single game alone makes them between 750 and a billion dollars PER YEAR...and the ISPs want a big piece of it.

Your question depends on what 'our internet' covers.

If you're talking about Reddit then the front page will tell you it's mostly American.

You can actually test for your own connection with trace route. You can see the number of hops your data makes. It will tell you where the packets went who handled it for how long.

Most services are American base. You have news article saying the internet broke when s3 breaks. 
http://money.cnn.com/2017/03/02/technology/amazon-s3-outage-human-error/index.html
 
If your a regular Chinese user. Then the answer to your question. Is very little is American. Most Chinese rely on home grown services.


Editorial: routing table are designed to route efficiently. If you add political, or corporate prioritization... Then the network topology changes. You end up with a fragmented internet. American, Chinese, EU, 
That is not even close to an accurate description of what the routing is like or what the FCC rules cover.

An ISP (like Spectrum, AT&T, Comcast, etc.) can throttle data to and from their subscribers, and that is it (this is also what they want to do, and it would be bad for consumers).

Backbone providers on the other hand, can apply QoS (quality of service) rules to any traffic passing through them. This is already done for certain types of traffic, but currently it is largely a good thing. Currently it would require blatant collusion for the scenarios being tossed around.

Overall, the idea of net neutrality is great, but the current FCC regulations were not really about protecting the consumer. Repealing the rules is also not in consumers interest either. Either way we lose, there is not enough competition to cause the providers to pass on the potential revenue stream, nor will government interference improve the situation much (if at all). 
I will answer this a bit differently:

Do you use google, YouTube or facebook in Canada? They would be far less likely (esp YT) to exist if net neutrality wasn't a thing when they were made.

Even if they weren't US companies, lack of net neutrality kills startups' access to this large single-language homogeneous-ish-culture market.

If they go through with killing net neutrality, say goodbye to a large chunk of future innovations that need the internet.
The US is the [origin of](http://en.wikipedia.org/wiki/Internet_protocol_suite) and the [center of](https://en.wikipedia.org/wiki/List_of_countries_by_number_of_Internet_hosts) the internet. The US is 5% of the earth's population but has literally the majority of internet hosts in the world.

Center of the internet is not an exaggeration or just a reference to the US having the most used sites and services. [The US is literally the physical, central hub of global telecommunications infrastructure.](http://4.bp.blogspot.com/-M6U-SWhzb2o/U5x0i0cUxmI/AAAAAAAADSM/AO17KV5kWOI/s1600/communications_global-traffic-map-2014-3000X2160.jpg) 
At the moment, the USA seems to be on a mission to isolate itself substantially. The country has embarked on a course of national suicide seemingly without regard to who gets taken along with it. In this case, the world must act decisively to minimize the global impact of the American trajectory. Alternatives to routing traffic through the US should be given high priority. Failure to do so will almost certainly have deleterious, if not truly catastrophic consequences for the world. 
This is only one of several spheres in which the world has to uncouple from the US to save itself. 
[deleted]
We don't really know right now. The most strict parts of net neutrality are only about 2 years old meaning that for years ISPs didn't do all of the things people are currently scared of. While some ISPs have been caught engaging some unethical practices it wasn't as wide spread or frequent as people's looming fears.
Very interesting question and there has been some evidence for social distancing diminishing other community diseases.


Here's a chart of Taiwan's influenza-related out-patient clinic weekly ratio data, 2020 is the thick blue line:
https://i.imgur.com/ayTcvyH.png

Source: https://data.cdc.gov.tw/en/
[removed]
I read an [editorial published in BMJ](https://blogs.bmj.com/bmj/2020/03/11/carl-heneghan-assessing-mortality-during-the-covid-19-outbreak/?utm_source=twitter&utm_medium=social&utm_term=hootsuite&utm_content=sme&utm_campaign=usage) wherein the author concluded that, based on what he’s currently seeing, overall mortality from respiratory illnesses in general is actually *lower* than the historical average at the moment (or at least appears as though that’s where we’re headed).

I suspect any large-scale patterns like this will become more apparent in the coming months, but it’s intuitive when you think about it - all of the hypervigilance surrounding this COVID pandemic is surely going to reduce the transmission of other diseases. Will be interesting to see what happens.
[This chart of the 1918 Spanish flu shows why social distancing works](https://www.google.com/amp/s/qz.com/1816060/a-chart-of-the-1918-spanish-flu-shows-why-social-distancing-works/amp/) 

Thought this article explained social distancing really well, compared it to cities that enacted social distancing during the Spanish flu.
[removed]
As long as we know, it takes around 2 weeks for cases to drop after a full-on lockdown because of the incubation period.

Here is the timeline for Hubei, China
https://miro.medium.com/max/7168/1*r-ddYhoUtP_se6x-NOEinA.png

Source: https://medium.com/@tomaspueyo/coronavirus-act-today-or-people-will-die-f4d3d9cd99ca
[removed]
[removed]
I think its probably much too early to draw any meaningful conclusions with regard to COVID-19. But, we do have a robust set of evidence that social distancing and quarantines do work with communicable diseases. COVID-19's apparent long life outside the body likely diminishes the impact somewhat, but it should still be effective if the quarantine and distancing are done correctly.
[removed]
[removed]
[removed]
[removed]
[removed]
On NPR today they were discussing this very topic with a pathologist and the short answer is we know it breaks transmissions of the virus, but frankly we won’t know how well we are successfully doing so until it’s all over.
If the lockdown were perfect and totally stopped new infections (which it isn’t, and didn’t), we’d still see hospitalizations GROWING for a couple weeks, and related deaths GROWING for ~4 weeks after that.

Then, in our perfect fantasy scenario, hospitalizations would drop pretty precipitously, and related deaths would taper off from 3 to 6 weeks after that.
[removed]
I wouldn't think we have good numbers on that. I have a workmate who had the flu shot last fall and just returned back from visiting relatives in Seattle. They didn't test him for anything when he came in with fever, coughing, and shortness of breath,  just said it is the flu. The doctor claimed his shortness of breath is from him having mild asthma. Now that may be true, but without testing how could he know? I'm waiting to hear back if they can fit me in for testing tomorrow or Sunday.
Are you asking if this pandemic will reduce seasonal flu cases this year or next year, since people are now more educated to cope with viruses? Seasonal flu is way down here (Europe). Same precautions work for flu and corona. It should be pretty obvious effect.
[removed]
[removed]
[removed]
As several people have pointed out, the question is not ["what is Jupiter's structure"](https://www.reddit.com/r/askscience/comments/80cpqc/when_did_scientists_realize_that_jupiter_had_no/duunoak/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=askscience) or ["When did scientists realize that Jupiter had an atmosphere"](https://www.reddit.com/r/askscience/comments/80cpqc/when_did_scientists_realize_that_jupiter_had_no/duupzwm/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=askscience), it's "when did scientists realize it had no solid ground?".  It's about the *history* of knowledge of the interior composition.  Since, [recent spacecraft results](https://www.reddit.com/r/askscience/comments/80cpqc/when_did_scientists_realize_that_jupiter_had_no/duuorz9/?utm_content=permalink&utm_medium=front&utm_source=reddit&utm_name=askscience) suggest it may gradually transition into something like a small solid core, let's interpret the question as, "When did we first realize that Jupiter wasn't a rocky planet"?

To solve this problem, you need to calculate the mass of Jupiter and its size, so you can find its *density*.  The mass of Jupiter can be calculated using Newtonian orbital mechanics, provided you know the orbital period and orbital diameter of its moons.  Period is easy, but a major historical problem in solar system studies was finding the relative scale of *everything*: it's easy to measure angles and relative sizes with a telescope, but we need the actual length dimensions.  You need a baseline distance to start with, such as the distance from the Earth to the Sun: this was worked out [in the late 1600s](https://en.wikipedia.org/wiki/Astronomical_unit#History): from that, the size of Jupiter, the size of its moons' orbits, and thus its mass and density could be found.

So even before 1700, astronomers realized that Jupiter was 300 times the mass of Earth, but with a density much less than that of rock.  Since the density known today is about 1.4 times that of water, I suppose that without knowledge of high-pressure physics they couldn't rule out a liquid water or ice planet, but scientists have known for over 300 years that Jupiter is definitely not a rocky world.  H. G. Wells was a fantastic writer, but he maybe wasn't so meticulous about doing his library research.

The story is laid out by Reta Beebe in [*Jupiter: The Giant Planet*](https://www.amazon.com/JUPITER-Smithsonian-Library-Solar-System/dp/1560986859) (1997).  

Giovanni Cassini observed the different latitudes of Jupiter rotated at different rates in 1690. He concluded the planet could not be solid - ie was liquid or gas because a solid planet would not allow differential rotation
[removed]
[removed]
[removed]
The Pioneer and Voyager missions helped with this somewhat, though indirectly. The affect of the planet's gravity on the *known* speed, direction, and mass of the space probe allowed us to calculate fairly precisely the amount of mass Jupiter would have to have in order to produce the seen affects.

We've known the distance to Jupiter and approximate size of the visible part of the planet, so once we knew the mass it was fairly simple to work out the density. Knowing the density allowed us to then say "it is either all heavy gasses, or there are a lot of light gasses and something--perhaps Earth sized--at the center that is a solid core". Considering that a few hundred Earths could fit inside Jupiter, the idea that the core might be Earth sized is impressive to us but in reality is some single-digit percent of the overall mass and volume of the planet. It would compare favorably to putting a peanut in the middle of the Capitol Rotunda and asking someone to find it blindfolded, using only one hand, while crawling on hands and knees. An Earth-size core is very (very) vanishingly small under the incredible amount of matter Jupiter has on hand.

The other fun thing with Jupiter that helps us determine its makeup is the fact that it is esssentially invisible to radar. This means it is comparable to clouds and air on Earth. We can see Venus on radar, we can see Mars, we can see the four big Moons of Jupiter on radar. But we can't see Jupiter itself except for the occasional wisp; on radar the moons of Jupiter appear to circle empty space. This implies that if there is anything solid there it is so buried as to be effectively non-existent for human purposes.

[This is different from *radio* astronomy, which uses radio dishes to study radio emissions from planets and stars; in radio frequencies Jupiter is VERY loud].
One thing to add is [differential rotation](http://astronomy.swin.edu.au/cosmos/D/Differential+Rotation), which basically means that different parts/regions of a celestial body rotate at different speeds. So a solid planet would rotate in alignment across all latitudes while gas objects (sun, Jupiter) typically have equatorial regions that rotate more quickly than areas closer to the poles.

> Examples of differential rotation are found throughout astronomy. In stars (including the Sun) and the gas giant planets, the equatorial regions rotate faster than regions closer to the poles, meaning that equatorial sunspots and cloud formations will move across the face of the object faster than their polar cousins.

In 1690, Giovanni Cassini was the first to discover the differential rotation within Jupiter's atmosphere. What I don't know though is whether that was exactly translated to: 'Jupiter must be a gaseous or gaseous-like body.'
[removed]
u/astromike23 had a wonderful answer to this question that I'll repost:  

"For the interior of Jupiter, let's imagine taking a descent from cloud-tops down to the core based on our best guesses of what lies below.

You start falling through the high, white ammonia clouds starting at 0.5 atmospheres, where the Sun is still visible. It's very cold here, -150 C (-240 F). Your rate of descent is roughly 2.5x that of Earth, since gravity is much stronger on Jupiter.

You emerge out the bottom of the cloud deck somewhere near 1 atmosphere. It's still somewhat bright, with sunlight filtering through the ammonia clouds much like an overcast day on Earth. Below, you see the second cloud-deck made of roiling brown ammonium hydrosulphide, starting about 2 atmospheres.

As you fall through the bottom of this second cloud deck, it's now quite dark, but warming up as the pressure increases. Beneath you are white water clouds forming towering thunderstorms, with the darkness punctuated by bright flashes of lightning starting somewhere around 5 atmospheres. As you pass through this third and final cloud-deck it's now finally warmed up to room temperature, if only the pressure weren't starting to crush you.

Emerging out the bottom, the pressure is now intense, and it's starting to get quite warm, and there's nothing but the dark abyss of ever-denser hydrogen gas beneath you. You fall through this abyss for a very, very long time.

You eventually start to notice that the atmosphere has become thick enough that you can swim through it. It's not quite liquid, not quite gas, but a "supercritical fluid" that shares properties of each. Your body would naturally stop falling and settle out somewhere at this level, where your density and the atmosphere's density are equal. However, you've brought your "heavy boots" and continue your descent.

After a very, very long time of falling through ever greater pressure and heat, there's no longer complete darkness. The atmosphere is now warm enough that it begins to glow - red-hot at first, then yellow-hot, and finally white-hot.

You're now 30% of the way down, and have just hit the metallic region at 2 million atmospheres of pressure. Still glowing white-hot, hydrogen has become so dense as to become a liquid metal. It roils and convects, generating strong magnetic fields in the process.

Most materials passing through this deep, deep ocean of liquid metallic hydrogen would instantly dissolve, but thankfully you've brought your unobtainium spacesuit...which is good, because it's now 10,000 C (18,000 F). Falling ever deeper through this hot glowing sea of liquid metal, you reflect that a mai tai would really hit the spot right about now.

After a very, very, very long time falling through this liquid metal ocean, you're now 80% of the way down...when suddenly your boots hit a solid "surface", insomuch as you can call it a surface. Beneath you is a core weighing in at 25 Earth-masses, made of rock and exotic ices that can only exist under the crushing pressure of 25 million atmospheres.

You check your cell phone to tell you friends about your voyage...but sadly, it melted in the metallic ocean - and besides, they only have 3G down here.

TL;DR: You would stop falling about 10% of the way down, where your density matches the density of the surrounding hydrogen "supercritical fluid"."
Haven't scientists been arguing about this and really don't know whether it does or not?

If I'm not mistaken, our first glance at Jupiter caused scientists to say that it was a big gas ball, but then people started saying that with its size, the gas near the center may have caused the matter to condense and create a solid. 

Am I wrong?
[Here is one model of what the interior of Jupiter is hypothetized to look like.](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Jupiter_diagram.svg/2880px-Jupiter_diagram.svg.png) Note that they do not know for sure that this is the interior composition. This is based on spectroscopy and other measurements which are plugged into computer models. Note the thick layer of the metallic hydrogen. This layer is roughly the thickness of **NINE** Earth's put end to end (9 times the diameter) (!!). So yeah, we aren't getting anywhere close to the core.

It is entirely possible that we might never know for sure. The reason for this is the intense conditions on the interior. I like to think of Jupiter as a "failed star" instead of just a planet like Earth with a huge atmosphere. I~~f there was a bit more mass in that region of the solar system when it was forming, Jupiter would have further condensed and we would have become a twin star system~~. **EDIT: As /u/feng_huang pointed out below, Jupiter would need to be about 100x more massive to have condensed into a star. One of my astronomy professors used to say that when the solar system formed, if Jupiter was more massive it would have sucked up some of the other gas giants mass if it went into a star. However, according to [this article](http://www.askamathematician.com/2011/06/q-how-close-is-jupiter-to-being-a-star-what-would-happen-to-us-if-it-were/), that idea is based off a old, outdated model of nuclear physics.....that astronomy professor was really old so I guess he wasnt staying current.**

It is believed that half, or more than half all the stars in the universe are binary star systems, so this type of formation is very common. 

And intense conditions on the interior might be an understatement. Models predict that near the core the temperature reaches 43,000°F (24,000°C), with pressure at the core being  650 million pounds of pressure per square inch. For comparison, at the depth of the deepest point in the ocean, the pressure is 16,000 pounds per square inch. We don't have a totally solid understanding of how matter even behaves at those conditions. For example, liquid metallic hydrogen, which they think makes up a large percentage of the interior, has actually never been observed so is a theoretical and exotic phase (in a sense) of hydrogen.

Edit: whoops. Just realized I totally didn't answer your question, I meant to add a section at the end of my post about when we started figuring out the interior composition, but I got typing and forgot. Will update something when I get outta class. 
Depends how you define solid ground. There's probably a mostly metallic hydrogen (basically condensed and then solidified gas) core but we don't know for sure. In a very hypothetical situation where an object fell through the gas and liquid layers of Jupiter, it would encounter this metallic core and experience something similar to solid ground, although at those pressures and gravity we really don't know how the material behaves. It could be like a regular hard metal or more like a flowing semi-solid core, but either way so dense that it functions as a solid during an impact.
The most recent announced findings state that Jupiter [has a "large fuzzy" core](https://www.space.com/37005-jupiter-fuzzy-core-nasa-juno.html). More Juno flybys have happened since this article, so it may be that more is known, but is waiting for publication.
I'm confused, how did they know it had a low density by simply observing it in a telescope? As others have mentioned, couldn't have been a solid ball of rock with very thick clouds?
Short answer is we don't. No one has ever measured what is on the interior of Jupiter. All we know is that it's too dense to be completey hydrogen based on it's size and it's not dense enough to be made of mostly solid. We can make educated guesses about what is inside based on common sense but we don't know for sure. 
On the topic of Jupiter, I have always wondered would would happen if you jumped through Jupiter's atmosphere, or fired a gun? Would you float. Would everything that falls into end up at its core or does it just burn up before it gets there?
Wait what? It has no solid ground? 31 years on this planet and this is the first I’m learning this.

I’ve heard it being called a gas planet but I thought it was because there were many gas types on it. Is it really absent of a solid ground? 

Is it? What happens if you go down into it? What?

🤯
Jupiter *does* have a "surface" of sorts, it's just relatively miniscule compared to the overall volume of the planet itself, which is mostly gas (and by "relatively miniscule" I mean it could be a mass as large as several Earths for all we know).

Could that surface/core have any sort of volcanic activity?  Beats me - we have yet to find a way to peek beyond the massive gaseous veil of that planet.  Though I would guess if the core is large enough it could, given the enormous pressures it would be subject to.  It could even be just a gigantic molten ball.
If you think that is weird. You should read some sci-fi novels before 1960.

Asimov's lucky stars series had venus as a water planet. Mercury was tidally locked and had a life belt. And jupiter's moon Mimas was a big snowball

In E. E. "DOC" Smith's Space hounds of IPC saturn had a surface and humans could stand on it unprotected
It does have a solid ground.  However the proportion of atmosphere depth related to diameter of solid core is much different than what we are used to being earthlings.  It is a gas giant because it formed outside the region that our star would blow/evaporate the gas away.  So it would have a solid core much larger than earth. 
Arthur C. Clarke proposed in "2061: Odyssey Three" that the core of Jupiter is an Earth-sized diamond, and as far as I can tell it's still in the realm of "possible but unlikely." Jupiter is enriched in carbon relative to the sun; we just don't know if carbon rains out of the atmosphere, if it stays dissolved, or something in between (e.g. diamond drizzle).
Jupiter is so massive that the temperatures and pressures at its core are extremely high. (Wikipedia says "The temperature at the core boundary is estimated to be 36,000 K (35,700 °C; 64,300 °F) and the interior pressure is roughly 3,000–4,500 GPa.") Weird things happen to matter in those sorts of conditions: instead of being in the solid, liquid or gas phases we're familiar with, stuff tends to become a [supercritical fluid](https://en.wikipedia.org/wiki/Supercritical_fluid) (or perhaps even heading towards [degenerate matter](https://en.wikipedia.org/wiki/Degenerate_matter)).

Not only do we probably not know enough about the composition of Jupiter's core to make a good guess at what compounds -- if that concept still even applies -- are present and in what mixtures (other than that it's mostly hydrogen and helium), but even if we did I'm not sure the compounds themselves have yet been adequately studied for us to know how they'd behave at such high temperatures and pressures.

In other words, the concept of a solid surface on Jupiter may not make sense to begin with, and even if it did, we may not yet understand either Jupiter or high-pressure physics and chemistry in general well enough to accurately predict whether it exists anyway.
"Now that she's back in the atmosphere
With drops of Jupiter in her hair, hey, hey
She acts like summer and walks like rain
Reminds me that there's time to change, hey, hey..." 

So from the evidence provided, it would appear that Jupiter is actually liquid based. The gaseous layer is just evaporated water, and the radioactivity is hardly lethal otherwise she wouldn't have been able to visit the planet. 
**Ph.D. in Vaccine Development here.**   
Short answer: Many many reasons   
Slightly longer answer: 

* Combination of impact and design difficulty - Understandably, vaccines were developed for the most common/most impactful diseases (the need) + a combination of how easy/straightforward it was to design the vaccine, based on what we knew about the pathogen, its structure, the immunodominant antigen, and most importantly the correlate of protection. So you could think that a lot of the impactful diseases, that have the advantage of straightforward design - e.g. low mutation ability, limited immunodominant antigens, clear correlates of protection, e.g. antibodies have been addressed - Rotavirus, pneumococcal pneumonia, Measles, Mumps, Rubella, etc. etc. 
* Others are ones that have high impact but pose considerable scientific challenges - Flu, HIV etc. There's been lots of work and $ put into them but their high mutation rates mean they are scientifically hard to crack. That's why while we have \*generally effective\* flu vaccines, we still need to rely on strain coverage and annual seleciton. We've been making steady progress towards a universal flu vaccine, but it's still a while away. For e.g. for flu, we know the correlate of protection, but don't have a stable/conserved immunodominant antigen yet... for HIV - mix of both, some potential immunodominant antigens, but the high mutation rate is a fucking nightmare. RSV is similar - there has been a lot of interest due to the high impact, but the concern is scientific (antigen selection and immune response) than lack of resources 
* Then there is the need aspect, in the other direction - Vaccine development is expensive, and takes time. So you need resources, and in current society - conpanies investing in this need to recoup their costs - so this goes into the argument on funding. Someone else in this thread brought up chikungunya - Yep, decent example - mostly a disease affecting developing countries, but other than a few labs, and a few startups focuisng on emerging diseases, there wasn't that much effort in there - from the U.S. side - till recently. It has been in development in other countries, but like I said, this takes time.... and resources 
* mRNA vaccine tech - This tech has been in development for the last 18 years or so. It is not new! The problem is, as most of the "easy" targets were taken up by other vaccines as mentioned earlier - they had a higher bar to prove. For a new tech, they need to convince regulators, that their vaccine is as good/better than established ones, as opposed to just proving they work. Plus the cold chain concerns etc. are a factor. But the key advantage for them was always the speed of reactivity - Once a pathogen is identified/sequenced, it's much quicker to design an mRNA vaccine than a protein/live inactivated etc. so companies like Novavax, Moderna, Biontech have been working on mRNA vaccines for a while as the USP is ability to react quickly to novel antigens - as what happened here. 
* This is a litmus test for new vaccine tech, and it will be fascinating to see where we go from here. As this tech takes center stage, we can expect more investment in this tech for other diseases.
HIV is a highly mutagenic virus - there are a handful of different COVID strains circulating around the world, but in HIV the virus continues to mutate readily after infection, leading to the presence of multiple strains per individual [(source)](https://link.springer.com/article/10.1007/BF00163230). This makes it a much more difficult task to come up with an effective vaccine for HIV.

As for the others, its more of an economic reason than a biological one. The scale of the COVID pandemic has caused governments and industry worldwide to pour money into the development of a vaccine -- and that level of demand just isn't there for RSV/EBV/etc. Would love for someone with more of a background in healthcare economics to chime in here.
They didn't develop it in 8 months. The Moderna vaccine has been in development since 2013 for a different strain of coronavirus; they reworked it. Here's a video from the MIT course on the pandemic from one of the researchers who made the vaccine, Kizzmekia Corbett.  [https://www.youtube.com/watch?v=xpqfdr9FPWM&feature=emb\_logo](https://www.youtube.com/watch?v=xpqfdr9FPWM&feature=emb_logo) The MIT course also has other researchers (one studies HIV) who discusses how and why there is no vaccine for HIV because how quickly it mutates. (Full MIT Course here: [https://biology.mit.edu/undergraduate/current-students/subject-offerings/covid-19-sars-cov-2-and-the-pandemic/](https://biology.mit.edu/undergraduate/current-students/subject-offerings/covid-19-sars-cov-2-and-the-pandemic/) )
There is a huge reason why the Covid vaccine was able to be developed so quickly that I disturbingly don't see in other responses.

For vaccine approval, efficacy must be proved. When the incidence of a disease is low, this takes a much much longer time to prove. When the incidence of a disease is high, a vaccine can prove efficacy much more quickly. (It should be noted that incidence and prevalence are close to equal in Covid-19). 

The 3 MAJOR reasons the vaccine has come out so quickly:

1) Covid-19 had a high incidence world-wide
2) Covid-19 has a low mutation rate
3) Covid-19 has no 100% effective prevention


Yes, affecting countries with more financial power is contributing, but that is not the primary reason.
[removed]
I work in a malaria research lab and unlike Covid-19, where all virus particles have the spike protein that we can “teach” our bodies to recognize with a vaccine, malaria has SIXTY different unique proteins they put on the surface of cells making it way more impractical to get a vaccine that will broadly give you protection. 
Also for reasons we don’t completely understand, the immune system is less effective at developing immunity to some diseases. For example, even adults who have lived in malarious areas all their lives can still get low levels of malaria infection and malaria immunity can drop off fast once you move to malaria free regions and are no longer exposed (as little as six months). 
So we also have to figure out how to get a malaria vaccine to produce more effective immunity than repeated infections with the disease itself which is a big hurdle.
[removed]
Generally, we have not had success vaccinating for viruses that form a chronic infection. HSV1/2, HIV, Mono, Hep C, Epstein Barr, etc... We have a vaccine for Chicken Pox, but we actually give you a weakened version of chicken pox that forms a life long infection in you as a vaccine. So, all we really do is skip the acute phase of the virus. That's right, you have chicken pox for life. It's likely we could make a similar vaccine for Epstein Barr that would give you the life long infection without the acute phase of the illness, but they just haven't yet. HPV and Hep B CAN form a chronic infection but they typically don't. This is why we have vaccines for them now too.
There are many types of viruses that employ different methods to evade the immune system. Covid 19 has a particularly immunogenic coat protein. Certain viruses (HIV) cover themselves in our natural glycans (sugar molecules) making our immune system unable to bind to them, others (rabies)exist within the nervous system (where our standard immune system can’t reach), or (EBV) fully integrate their genome into our cells and ONLY emerge in an immunocomparmised state.

Study virology! It’s awesome.
Work was previously done on SARS viruses that are similar to Covid-19. The work previously on MERS helped researchers tremendously. The spike protein on Covid-19 has not changed. The other viruses that are difficult to get vaccines for are not SARS viruses. Many mutate too much.
[removed]
[I found think link helpful when explaining this to someone else!](https://www.healthline.com/health/hiv-aids/vaccine-how-close-are-we)

A brief summary of the key points:
1) vaccines are made to mimic a person's immune response. Since no one has ever recovered from HIV, we can't mimic that. It's like if i asked you to mimic my great aunt's voice - you can't because you don't know what it sounds like (I don't even know what it sounds like).
2) vaccines don't prevent infection, they just help protect against the disease. They're made to buy the body time to fight the infection and give the body the tools necessary to fight it. But HIV lays dormant in the body for a LONG time before it progresses to AIDS, and while it's dormant, it hides and reproduces in cells without anyone knowing. Then that means the body has to fight even harder to get rid of the infection completely, IF it can find all the copies.
3) most viruses that have vaccines enter through the gastrointestinal or respiratory systems. HIV primarily enters through the genitals or blood, which we have MUCH less experience in fighting, let alone making something to fight it
4) HIV mutates too quickly, it's constantly changing and evolving, and we have no way to track those changes or predict when the next change will happen or what it will be. What's effective as a vaccine now, by the time appropriate testing is done, it might be useless!
5) most commonly, a killed or weakened strain of a virus is used to develop a vaccine so that the body can learn to fight it off. However, in initial trials, a killed or weakened form of HIV was not effective and didn't produce an immune response. The standard form of the virus is too dangerous to use for these purposes.

Also, there is the prEP pill is also available to help the body prevent HIV, but it's a regularly taken pill, NOT a one-time shot vaccine.

Edited to add: HIV, while it's been around and harming humans for a long time, was largely ignored until the 1980's because of its association with the LGBTQ+ people.
What makes a lot of viruses so hard to vaccinate against is how often they mutate.  For example the seasonal flu vaccine is basically a best estimation every year of which [handfull of mutations/strains](https://www.cdc.gov/flu/prevent/vaccine-selection.htm) are going to be the worst that year.  Some of the viruses that don't mutate quickly are the easy ones that you get vaccinated against once and are good for life.  The faster mutating ones are just hard as heck to lock down.

One example is Polio.  We were so dang close to eradication back in 2017 when we had [22](https://www.who.int/news-room/q-a-detail/does-polio-still-exist-is-it-curable) reported cases globally.  ~~But then if i remember correctly it mutated and that spiked.~~(edit:  Sounds like I didn't remember correctly.  I'm thinking of a different event)
Because there is rarely a clear incentive for the entire industry, regulators, and politicians to focus on one thing. 

Also, drugs are approved based on the benefits outweighing the risk. A global pandemic makes that an easier bar to clear. For a non-emergency situation the FDA would (correctly) require longer term safety and effectiveness data. For example, we know the vaccines are effective in the short term, but we don’t know how long that effectiveness lasts. In a normal situation you’d need that data before seeking approval.
It's hard to test for vaccines because you need enough people to catch the illness to power a study that actually proves a vaccine works.  But they are easy to test now.  That and a ton of funding.  It's possible the mRNA technology will be used for other disease moving forward.  Could be a great silver lining behind all this.  We don't know yet if the immunity is lasting for this vaccine but I sure hope so.  I was on the trial and am tier 1a to get vaccinated so I personally can't wait for it (hopefully this week).
Because of the scale of the virus involved. HIV (f.e.) is a global health *issue*, but not a global health *emergency*. It probably caused more deaths than COVID, but not in the span of a single year.

The more footprint a problem has in the public, the more pressure is there for politics to release funding, and for medical companies to prioritize.

As well, it helps that SARS isn't a very complicated or entirely new strain of virus. In essence, COVID is 'just' a very complicated and dangerous flu.

High priority, easy (compared to, let's say, HIV) to develope. That's why we get a vaccine relatively quick.
Besides the great answers that some problems are harder to solve than others, if you have 1000 labs working on something and billions invested into it, you tend to obtain results much quicker than when 2 labs are working on something with a million dollar.

There are lots of problems we could solve rather quickly if we put all of our human resources into it.
I can think of two reasons:

As viruses go, coronavirus turns out to be fairly easy to vaccinate against.  The spiky bits  are easy to target .  We started off already knowing that much because of past research done on other coronavirus strains.

Second, consider the sheer amount of resources that have been dedicated to finding a vaccine. There are *a lot* of really smart people working on this, and they are very well funded.  The outcome here was never in doubt.

Edit: apparently virii is not a word.
[removed]
There's a few really relevant things to add to the conversation:

Covid19 is a very standard virus. It works exactly the way other common viruses work.

It has a very convenient target for antibodies, through the spike protein. This has stayed constant through the minor mutations around the world.

Covid19 vaccines are also being rushed through the regulatory processes. This is because of the impact on the global economy.  

This is just largely by cutting red tape by governments running their assessment and regulations concurrently with the trial process rather than waiting for trials to end.

The mRNA vaccines are a little different. The process for developing these vaccines has been in progress for years, and with covid pharmaceutical companies have doubled,and doubled down again on development, rolling out a supply chain that will be used for other vaccines in the future
Don't underestimate the amount of resources thrown against this. Furthermore, we have been successfully developing vaccines against coronaviruses for a while, this is 'just' another iteration. Viruses that incorporate in the host DNA are a lot harder to tackle. For other viruses, there is no economical incentive to produce a vaccine and go over to wide vaccination.
By the nature of the technology, mRNA vaccines lend themselves to quick development. Much of the work for the mRNA COVID-19 vaccines is based on the vaccines for SARS and MERS that were already in development, they pivoted what they had and adapted it.
has nothing to do with money, other than the thought of profit and speeding up actual production. the vaccine was developed within hours of the released genome sequence. it just happened to be a very easy one to develop because of the lack of major mutations of the virus.
I haven’t seen this in the comments yet so forgive me if I’m repeating something another Redditor has added but a further part to this is the scale of manufacture.

Normally any new drug/vaccine/ATMP (advanced therapeutic medicinal product) would go through 1-3 years of product development to define and optimise the production process.

For these vaccines we have been doing product development alongside large scale manufacture. That doesn’t compromise the vaccine itself in that there’s substantial testing ensuring that it’s safe/sterile/pure/of a high enough efficacy. 
But, it does mean manufacturing are being run ragged with last minute changes, alterations to the process batch to batch, document errors and optimisations in real time.

So some of the speed for these vaccines comes from a lot more work in manufacturing to adapt, almost in real time, to changes while troubleshooting and optimising along the way.

Source, I work for one of the manufacturers for one of the bigger candidate vaccines.
Because we threw the entire microbiology effort of the planet at one thing and neglected everything else.
Progress on other viruses and diseases stagnated and in some cases samples were reportedly destroyed to make space for COVID-19 work.
So many resources were consumed in this effort and it took a huge redirection of money to do so.
[removed]
At least one of the vaccines only took 2 days to develop. Or more specifically to target at SARS-COV-2. It was previously under development for MERS, which is a coronavirus in the same family as SAR-COV-2. They just had to make some changes and then do the safety testing.
Not only are corona-type viruses already very well understood because of how common they are, but also demand. Every biotech company in the world is throwing money at research right now, and believe me, they've got plenty of money to throw. They all want to be the ones to say they made the vaccine.
Well we didn‘t. We had a foundation... But aside from that. Not every virus is the same. For example: Influenza mutates permanently so every year we need a new vaccine / combination of vaccines to stop the most common strains. For other viruses it‘s to hard to find a way to create a vaccine, the diseases aren‘t common enough to justify the cost to develop a vaccine etc. Also it takes a bit of luck to get a working vaccine. For example: We‘ve got a vaccine against SARS. But it made infections worse. 

However we continually improve vaccines and gain new knowledge about viruses in general and specific viruses as well. I recently heard that a HIV/AIDS vaccine has gotten great results so maybe we will be the first generation to be able to vaccinate against every STD (which could result in STDs being a thing of the past in a 100 years). Also you have to differentiate between dangerous viruses that could cause death and rather harmless viruses that only lead to minor symptoms. If no one buys your vaccine because they don‘t care if they get sick (younger adults and influenza...) you could spend your money on finding a new therapy for cancer etc.
There was a vaccine being developed for RSV in infants.  They got to human trials in RSV at-risk infants, and several in the vaccinated trial group developed a more severe RSV disease from immune system over-responding when they did become infected.  Several babies were hospitalized with severe illness and I think a couple may have died.  After that no company wanted to touch that with a 10 foot pole.  Dead babies are bad for business.
There are some really good and detailed biochemical answers here, the fundamental answer is a bit more simple, and based in economics:

It is cheaper to invest the billions of dollars needed to simultaneously develop, and fast-track 81 vaccines (NYT), than it is to suffer the economic distruption of the disease.
Vaccine development is supposed to take time. The efficacy and safety of the product has to be thoroughly vetted before it can be released to the public. Whether it works at a high enough protection rate across various populations. Whether it causes widespread allergic reactions. How it interacts with the hundreds of  thousands of other drugs, supplements, etc that people take. All that cannot be immediately determined. It takes time and subsequently money to do trials. 

Some viruses are just crafty. 

HIV is an ever-evolving retrovirus which makes it very tricky to defend against. It mutates faster than any known biological entity which makes a vaccine nearly impossible. And it is already finding ways around PREP. And what a retrovirus does is clamp onto the DNA of the host adding itself permanently to the genome. 

Other viruses can change the receptors on their outer shell often enough to defeat vaccines through processes called antigenic shift and drift. This makes multiple strains of viruses that need seasonally-configured vaccines like the flu shot. 

The Covid vaccine has been rushed out of necessity. There are numerous cases (good and bad) of what can happen when a vaccine is expedited. Right now we are about to find out with this one as the widespread human "trial" of the vaccine is happening as we speak. Be hopeful but realistic as the data rolls in about side effects, protection rates/lengths, mortality, etc.
There was also am unprecedented number of people who volunteered to be infected by covid-19 and participate in trials.  There is a really cool radiolab episode about it:

https://www.wnycstudios.org/podcasts/radiolab/articles/dispatch-13-challenge-trials
There are very good reasons and three of the biggest ones are: 

1. Money and resources - the global demand is unprecedented, money has been thrown at this. Also, because of the high rate of infection, drug trials were never short of resources- ie. patients - these take time to recruit usually. This is not the case with covid and meant that clinical trials could be conducted simultaneously (instead of one round after the other, each round needing to raise more funds and find volunteers) 

2. The mission. Without being too over the top about it, the scientists that developed these vaccines - this is their A-game. This is what they trained for. They’re on the frontline of a global battle. They’ve worked tirelessly around the world, 24/7, it hasn’t stopped. This is literally what happens when the best of the best come together for a common good 

3. The vaccine technology. For example the BioNTech vaccine uses mRNA tech which is just a lot more efficient at priming the immune system while producing little side effect. The tech bypasses the need to use live or attenuated virus like in past vaccines. Better drug, less side effects, just all round very good at what it does 

Also it based on an existing SARS vaccine for which there was a lot of data pre-covid

The protracted timeline is something not to be viewed with suspicion, but celebrated. This is literally science bossing it right now
[removed]
As Covid 19 is a coronavirus (COronaVIrus Disease 2019), and the coronavirus family has been around for literally hundreds of years, plus a lot of work was done with it for the SARS/MERS outbreaks earlier in the century, I think a hell of a lot of the groundwork had already been done and this whole “we made a vaccine in 8 months” is a common misconception.
So question.  Many vaccines have 5 or 10 years testing.  I am pro vaccine backed by vigorous testing and high standards.  My kids and I have every vaccine possible including optional vaccines.

This seems rushed I am hesitant to give my kids this vaccine.  Why is so little testing  acceptable here?  Problems have occured in the past where limited testing was carried out.
I'm gonna say funding.
Every vaccine option is based on 2009 research from the SARS-2 outbreak. In tact the corona vaccine has been researched for far over a decade.

There’s also an economic incentive to rush the vaccine. First company to develop it on a large scale is set to make trillions.
[removed]
1. COVID-19's economic impact is absolutely massive, so every country everywhere prioritized it and opened their wallets. Funding a vaccine isn't likely to be more expensive than what this disease already cost most countries. That means researchers didn't need to go through years of grant applications trying to prove that their vaccine development program deserves to get the limited funding over other diseases.
2. COVID-19 does not mutate a lot compared to some other viruses, so it is well-suited for vaccine development. We also already have a good understanding of coronaviruses in general.
3. The mode of transmission, and the difficulty/cost in preventing transmission, means that vaccines hold a higher importance and more urgency as a part of disease response compared to methods such as sanitation, prevention, etc.
4. Ironically, because COVID-19 is spreading more or less unchecked in some populations, it is easier for clinical trials to test vaccine efficacy. Basically you have to give some people the vaccine and others a placebo, then wait to see if they do or don't get sick. In most places it is no longer ethical to deliberately expose healthy people to diseases... you can't just inject people with HIV and see how many end up with AIDS, think about what that means especially for people in the placebo group. For rarer diseases, it takes longer because how do you know if the vaccine worked or if people in the test groups were just never exposed? With COVID-19 fortunately/unfortunately that's not a problem right now.
The genome for coronavirus was made readily available to many labs very quickly. This is largely due to the scale of the pandemic and the general interest in finding a vaccine. Other viruses that are less common do not have as much political/scientific interest which affects things like funding. Also sometimes it can take two years for genomes to reach a lab and there’s limited research and development that can be done without it.
Viruses come in many types and variations. Some viruses don't mutate a lot, others do. Influenza is always a nice example. We have vaccines, but they need to be newly created each year. And they're not just one vaccine, but a cocktail of vaccines against the the strain of influenza we expect to cause the largest problems. Sometimes we get this wrong and we get a larger outbreak. So we have a vaccine against influenza, but the one we use this year is probably useless next year.
Though it is not a vaccine by any means, we do have a very successful way of preventing HIV infection. It’s called pre-exposure prophylaxis (PrEP) and is a daily pill that reduces chances of infection by up to 99%. Brand names are Truvada and Descovy. You can find a prescriber here: https://www.pleaseprepme.org/

https://www.cdc.gov/hiv/basics/prep/prep-effectiveness.html
These latest vaccines are developed using mRNA techniques.  The science has been around for a while, but up to now there's been no motivation for changing vaccine development to using mRNA.

[https://www.usatoday.com/story/news/health/2020/11/23/success-pfizer-moderna-mrna-covid-19-vaccines-ushers-new-era/6311228002/](https://www.usatoday.com/story/news/health/2020/11/23/success-pfizer-moderna-mrna-covid-19-vaccines-ushers-new-era/6311228002/)
HIV is obviously a complicated one.

If you are exposed to it then there is a drug, PeP, that you take within 3 days of exposure once a day for 30 days to almost completely reduce the chances of infection.

To prevent HIV as a precaution during sex, you can take a drug called PreP, once a day, everyday for as long as you’re sexually active, to almost completely prevent infection. 

If you are infected with HIV, there are antiviral medications that reduce your viral count to the point of you being ‘undetectable.’ Meaning tests can’t find the HIV virus in your system enough so that you are healthy and virtually can’t transmit it. But are still considered HIV positive.
I don’t have an answer that is any better than what I’ve seen in the comments; however I can offer you a bit of advice on skepticism since you reveal a desire to genuinely know truth. It a a fallacy to apply, because 1 thing is true about 1 virus it means you can apply this to all viruses. This goes for more than just viruses and is an example of ‘Association fallacy.’ Because of social media and the concomitant accountability our peers give us it will become infinitely useful to know when others are exercising fallacy and is equally if not more important to not commit these fallacies yourself. 

I know this wasn’t what you were looking for. I’m sure you just thought up what appeared to be a tangential quagmire which appeals to others, but it did prove a level of intelligence that may be interested in a deeper understanding of truth via skepticism. 

Good luck. Have a good day!
Here’s a really good read on how the leading vaccines were developed so quickly.    https://www.stuff.co.nz/national/health/coronavirus/300176414/a-gamble-pays-off-in-spectacular-success-how-the-leading-coronavirus-vaccines-made-it-to-the-finish-line

TLDR:  (1) mRNA vaccine technology is a ‘new’ but ripe and ready to go solution.  (2) Governments around the world have invested billions, reducing the risk to pharmaceutical companies.  (3) Pre-approval production (4) COVID prevalence made it quicker to trial than less prevalent diseases.
Largely because viruses mutate. It’s what they do. They make copies of copies of copies of themselves and eventually they move so far from the version your vaccine was targeting that it no longer recognizes it as the same virus. It’s why you need a new Flu vaccine each year, and why it’s a crapshoot as to wether or not the one you got will actually work on your local strain. It’s why it’s inherently impossible to create a vaccine for a virus. To break it down Barney style, let’s just say the vaccine is teaching your bodies immune system to look for a virus with a green spherical center, long stalks coming off that and blue spheres on those stalks. Now, viruses reproduce by hijacking your own cells and then using your equipment to copy themselves over and over and over, and then those copies go out and do the same thing. Only they suck at. So Over time you get a virus that’s got a purple center sphere, with long stalks coming off it and blue spheres on those stalks. Now, you and I can recognize that that’s still the same damn virus, but your body is just going off what the vaccine trained it to look for. So we make a vaccine that trains your immune system to look for a virus with a purple central sphere, only the virus never stops mutating so eventually it ends up with a purple central Diamond shape instead of a sphere. Vaguely over simplistic, but I think I explained it rather well. There’s no cure all for a virus aside from to stomp it out and deprive it of any hosts to reproduce. There’s no magic bullet, there’s no cure all vaccine, it’s just like the flu. It’ll be there next year. and the year after that. And the year after that. It’ll likely outlast you, and me, and your grandkids, and your grandchildren’s great great grandchildren.
A big reason is motivation. 1% to 5% fatality rate and high communicability generates a high level of interest in developing a vaccine for a disease that is wide spread in the world. 

Second reason related to the first is that a vaccine would be extremely wide spread in use and therefor profitable. 

Third reason is scientific related to the virus itself. Ie it lent itself to being made into a vaccine, which is luck of the draw really. 

Third is politics, related to number 1. Other diseases are just as deadly or more so, but slower in spread or not as virulent. 


So luck, profit motive, and self preservation.
[removed]
Usually it’s a case of funding with many vaccines research projects on hold for long periods before further funding can be obtained to move to the next stage of a trial. That major obstacle was removed for COVID which helped to speed up the process.

Corona viruses aren’t new either so there was years of research following the SARS outbreak which helped support the creation of viable vaccines along with new technology.
Simple for HIV, it is very difficult to make a vaccine because the virus change constantly.

For the others, note that the effort on the COVID vaccine is gigantic, because the pandemic is costing trillions. And the safety authorities are fast tracking the examination of results. People are not ready (not able) to put so much effort for the other diseases.
1) Lives everywhere are on hold as we refrain from mixing socially or seeing our loved ones. Many, many people are sick of this, and a number of them have deep pockets.

2) The flu vaccine is remade annually, which can be done quickly because we already have a decent working knowledge of how the flu virus works. COVID-19 is similar to some other viruses that are better understood, which helps speed up vaccine research.

3) Unlike the influenza virus, COVID-19 is comparably much slower to mutate, which means that there are fewer strains to protect against so the process can move faster.

4) Some illnesses don’t have a reliable vaccine due to other hurdles. Many symptoms of Lyme disease, for example, are caused by the body attacking itself. When the immune system creates antibodies against the Lyme bacterium, often those antibodies cross-react with tissues in joints and damage those as well. Since the purpose of a vaccine is to stimulate an immune system response, such as antibody production, many of the vaccine tests just had a similar effect to actually becoming infected. One of the ways we lucked out with COVID-19 is that it doesn’t do this.

Also, COVID-19 is a respiratory virus that’s contagious in incubation and spread through droplets in the air. There’s not a lot of human contact you can have without spreading it - not like if it were mostly blood-borne  or sexually-transmitted. One reason so much fervor was put into the vaccine is because that’s one of our only options to control a virus like this. It’s not like we can just go out and not breathe.
Unprecedented amount of resources invested in discovering a covid vaccine explains the speed of development. Big big big money for the pharma companies who succeed. No other virus has cost world economy as much as covid, so the rewards aren't as generous.
As viruses mutate more they become harder to vaccinate.  So the less we control COVID, the less likely we will be able to vaccinate it away this time around.  We want to avoid that situation, one where we see COVID developing into a multiple-strain constant plague on society.  Many old viruses have in fact been vaccinated away, others were allowed to mutate and became too difficult to vaccinate away.  Others are viruses that mutate way too quickly / are way too complex of viruses too vaccinate away, like HIV, are also in this group.
It is pretty much impossible to melt wood. The reason is that as you start heading the wood up, its constituent building blocks tend to break up before the material can melt. This behavior is due to the fact that wood is made up of a strong network of cellulose fibers connected by a lignin mesh. You would need to add *a lot* of energy to allow the cellulose fibers to be able to easily slide past each other in order to create a molten state. On the other hand, there are plenty of other reactions that can kick in first as you transfer heat to the material.

If you have oxygen around you one key reactions is of course combustion. But even in the absence of oxygen there are plenty of reactions that will break up the material at the molecular level. The umbrella term for all of these messy reactions driven by heat is called [pyrolysis.](https://en.wikipedia.org/wiki/Pyrolysis)

**Reference:**

1. Schroeter, J., et al. Melting Cellulose. *Cellulose* 2005: 12, pg 159-165. ([link](https://link.springer.com/article/10.1007%2Fs10570-004-0344-3))
No. In fact the process you are describing is exactly [how you make charcoal](https://en.wikipedia.org/wiki/Charcoal).

"Charcoal is usually produced by slow [pyrolysis](https://en.wikipedia.org/wiki/Pyrolysis) — the heating of wood or other substances in the absence of oxygen"

Water and other volatile organic compounds (such as methanol) are basically boiled off and what remains is a large lump of carbon- a.k.a charcoal.

Can you melt carbon? No- not at [atmospheric pressure](https://en.wikipedia.org/wiki/Carbon#Characteristics)

"At atmospheric pressure it has no melting point as its triple point is at 10.8 ± 0.2 MPa and 4,600 ± 300 K (~4,330 °C or 7,820 °F), so it sublimes at about 3,900 K."
No, just as when you boil lemonade, you don't get lemonade gas but rather steam and lemon caramel. Phase transitions don't happen to all constituents simultaneously, and plenty of other chemical reaction occur along the way.
You can heat the wood enough to soften the lignin and then bend it.  This is maybe the closest you can get to anything similar to melting it.  Once it cools, it will stay as it was bent, this is how acoustic guitar makers bend the sides of a guitar.  It's a common technique in making furniture as well. 
In Ceramics / Pottery, wood-fired kilns utilize this effect to some extent, wood ash glazing was the primary method of finishing early Chinese and Japanese pottery. 

Ash from the burned wood is floating around in this hot (2400F/1300C) and oxygen-starved environment, it then melts and sticks to the pottery forming a clear glaze. I think the main component of it at this point is calcium carbonate as all of the carbon has been burned off. 
That's how you make charcoal.

You do that, then heat the charcoal up to a few thousand degrees, still with no oxygen, and it evaporates directly.  Carbon doesn't have a liquid phase at normal atmospheric pressure.

In any event, by the time you get it to the melting point of any of the customarily solid elements which compose it, it will no longer be wood.  It will have broken down chemically.
This process is called liquefaction, the large cellulose molecules undergo a lot of reactions and release gas, oil, water and lots of solids. If the heating is done in a high pressure hydrogen environment, more liquids are produced. So it is not melting as you would understand it but rather chemical decomposition. This technique is being explored with coal and biomass as an alternative source of petroleum. I even read an article about using supercritical water (water that has mixed properties of gases and liquids at high temperatures and pressures) to help the decomposition process.  

References: Pyrolysis of Wood/Biomass for Bio-oil:  A Critical Review
Dinesh Mohan,*,†,‡, Charles U. Pittman, Jr.,† and, and Philip H. Steele§
Energy & Fuels 2006 20 (3), 848-889
No, you'll just pyrolyse it. [Search 'melt wood'](https://www.reddit.com/r/askscience/search?q=melt+wood&restrict_sr=on) in this subreddit and you'll find plenty of details.

There are many chemicals or compounds for which the melting point is listed as "decomposes." This is to say, that some chemicals do not pass through a liquid phase before becoming gas, or in other cases, the process of heating them enough triggers other chemical changes: the molecules are broken apart into smaller bits which are naturally gas at a given temperature.

Dry ice is a good example of this: at standard pressure, applying heat to it causes it to turn straight to a gas. Some of the amino acids do something similar: they simply vaporize when heated to a certain temperature.

When you heat something to melting, you're causing bonds or crystal structures to weaken to the point that the molecules are capable of sliding around on each other. This holds true for most of the things we're used to: simple chemicals like water, salt, metals. But, if there are bonds inside the molecule that are even weaker than the forces keeping it a solid, then adding vibration (heat) is going to break those apart before the substance can get hot enough to liquify.

Wood is complicated, because it's got a ton of different substances inside it. When we make charcoal, wood is heated in an oxygen free environment. As the temperature is increased, a bunch of gases are released as water and oils evaporate, and sections of proteins decompose. Some of these gases can be condensed back into liquids, but I wouldn't call these "melted" wood: more that the wood released these chemicals as it was heated.

Heat the wood even more, and you'll get charcoal, basically pure carbon. You could continue heating this for quite a while, up to 4000K, before it starts to change state. Unfortunately, carbon at atmospheric pressure does much the same as dry ice: it sublimates straight into a gas instead of melting. To get liquid carbon, we have to add one more factor: pressure.

According to the diagram here:
https://chemistry.stackexchange.com/questions/6068/what-is-known-about-liquid-carbon

There is a combination of temperatures and pressures capable of creating liquid carbon. This occurs above approx. 5000K, and 3 kBar (about 43000 PSI). As a comparison, tungsten melts at very near this combination, so you'd be long in search of an appropriate container for this reaction.

Tl;dr: Yes, heating wood in an oxygen free environment to ~5000K, 3kBar pressure will lead to liquid carbon.
Yes, for ***extreme*** values of "very hot" and "possible."

Under normal conditions on Earth carbon can't melt; it sublimates directly from solid to gas.  However.  If you pressurize your wood to about 15 gigapascals (this is a lot) and heat it to about 10,000 C you get a supercritical fluid-ish substance which, once you cool it down, is basically crude oil.

A similar (but lower temperature thus much slower) process is where oil actually comes from.
As a man who worked in a job where we did just that, no, the wood does not melt. Instead, as others have mentioned it gets broken down into carbon (charcoal) and methanol, among other things which all depend on the type of wood itself because they all have slightly different compositions. So to sum it up, no you cannot melt wood because it breaks down into other materials before this occurs.

Source: A little over a year of work at a plant that produced tons of charcoal.
So, way less scientific here but this is exactly what you do to make charcoal for fire starting. You heat up any natural material in a nearly airless, small container with a Vent to off-gas the wood alcohol and other impurities. It's really cool because you end up with a carbon "skeleton" of whatever you put In the tin. Highly recommend trying it. 

Edit: spellcheck got me. 
No, you can not do that, for two reasons: 

a) Wood mostly consists of long fiber molecules. Their shape simply does not allow the behavior typical to a liquid, because they are too long to move about each other mostly unimpeded. It would theoretically be possible to turn it into a gas. However, here, the next point comes into play:

b) C-C and C-H-bonds aren't that strong. They will break apart before there is any chance the molecule could move into the gas phase. 

And that's what's happening. It's called pyrolysis or dry distillation, and it's used to turn wood into charcoal and "wood gas", mostly short-chain hydrocarbons and alcohols. 
Destructive distillation happens first. If you keep it contained without air, the volatile chemicals are produced and it ends up leaving charcoal behind. I actually remember seeing it on an episode of Mr. Wizard as a kid. 
Did someone see a picture of a sculpture on reddit and then become inspired to ask if wood melts???
You can burn wood, and melt the wood ash. It's an ancient Japanese technique for creating pottery, using an Anagama (wood burning kiln). It's not exactly the same but pretty close to simply melting wood.
You do precisely what you asked - deprive the kiln of oxygen while continually supplying it with fuel (wood). 
I was a ceramics major in college and we did this regularly. 

Yes you can melt wood, but not all at once. It's made up of many different molecules which will melt at many different temperatures, but at a certain point it will all resemble a liquid. It will take unbelievably high pressure and high temperature to achieve. Once it is all liquid, if it were somehow exposed to oxygen, it would burn faster than gunpowder.
Fundamental part of polymer science -networks don't melt. A network is an interconnected polymer (think spider web but more random and in 3 dimensions). Wood is a network connected together by a complex mix of linear and branched polymers crosslinked by hydrogen bonds.

Melting is a phase transition where the material absorbs energy but doesn't change it's temperature. This is impossible in wood because the components cannot slide past each other. So, upon heating you end up breaking chemical bonds that constitute the wood. Usually this takes the form of water elimination followed by atomization of the carbon. This is of course under strict oxygen free conditions.
Wood is made up of a network of cellulose. So all the molecules are in some way tethered to eachother. In order for the free movement required of a liquid, the chemical bonds must be broken. We call these types of polymers thermoset beause once they are crosslinked, they cannot be remoulded. Everything becomes tethered in a local area.
Well... If you heat wood in an oxygen free environment many the organic compounds will vaporize into a gas (similar to LP gas) which can then be compressed and condensed into a liquid.

The remaining material could potentially reach a liquid state, but you'd need a very hot environment. However, the result wouldn't be "liquid wood". If you allowed it to cool and resolidify it wouldn't be anything resembling wood; it would just be slag.
The hydrocarbonic composition of wood make it impossible to reach its melting point. It degrades before getting there(turn into another substance) however it is possible to speculate it's melting point, it would be around 800°C, however it degrades with around 400°C
[removed]
This post has attracted a large number of low quality answers. The moderation team would like to remind you that answers on r/askscience are expected to be accurate, in-depth explanations, including peer-reviewed sources where possible. If you are not an expert in the domain please refrain from speculating.
Its not actually circulation of BLOOD (if it was, those limbs or body parts would die off) 
, its nerves being compressed and cut off . When ever you sit in a position or are in a position for a while that can COMPRESS nerves, they will stop transmitting. The nerve cells themselves will TRY to transmit at first, but when they don't receive feedback, they stop. This is the "numbness" or "this body part fell asleep" that we all experience. When the compression ends, the nerves can sense this and all at once begin a chain of "testing" aka sending out signals to all cells affected down the line, to make sure they are still there. Its like the electrical grid firing back up. It feels painful because our nerves use pain a response. The sensory organs in our brain after about 30 seconds to 120 seconds will shut this test firing pattern off and resume normal operations. But that time in between is pretty painful. It feels like Pins and needles because each sensation is a nerve " thread ending " being tested. (source [USC Berkeley's neuroscience center ](http://neuroscience.berkeley.edu/research/)            
Edit well this blew up overnight! Thanks for the gold, kind stranger! 
[removed]
Hey! This was actually one of the first /askscience questions I answered. Copy of [this post](https://www.reddit.com/r/askscience/comments/30d61w/when_a_part_of_your_body_falls_asleep_can_it/cprhn58/?st=jjt35mqo&sh=a680b675) below.
*****
The tingly feeling is usually related more to nerve impingement/entrapment. It's actually pretty hard to completely cut off blood flow to your limbs by compressing/ligating vulnerable vessels. Using the arm as an example, here is the [blood supply](http://imgur.com/CMtLdm4), and the [nerve supply](http://imgur.com/CgykZRR) to the upper limb. (Source: Netter's *Atlas of Human Anatomy*). In the first image, note all the vessels that branch off of the subclavian, sections labelled 1 & 2. The vessels that wrap around the back of the scapula (shoulder blade) and connect with the others around the glenohumeral joint (where the ball of the humerus meets the socket of the shoulder blade) provides *collateral circulation* even if the subclavian/brachial artery is pinched shut in the axilla (armpit), the most vulnerable point.

Now look at the nerves. With the exception of the musculocutaneous nerve, all the nerves that innervate the upper limb pass through the 'pit' of the axilla that is vulnerable to compression (The MC nerve is also vulnerable, just less so).

The same applies to compression at other vulnerable points, there's almost always better collateral blood supply than there is innervation, since taking out nerve supply upstream cuts off the downstream more effectively than with blood flow.

****

I was a a second (or 1st?) year med student at the time so I'll expand a little - I focused on macro scale blood flow in that answer & waved the rest away as "nerve compression", but what actually happens with nerve compression? It's likely related to ischemia/impaired flow, but at the *microvascular* level supplying the nerve. The nerves have tiny blood vessels (vasa ~~vasorum~~ *nervorum*) that supply them, and compressing the nerve will compress these and can compromise the blood supply to the nerve.

[This abstract](https://www.ncbi.nlm.nih.gov/m/pubmed/10679707/) has a decent overview, relevant part quoted below:

>while the membrane depolarization produced by ischemia affects both transient and persistent Na(+) channels. Postischemic and posttetanic paresthesias occur when hyperpolarization by the Na(+)/K(+) pump is transiently prevented by raised extracellular K(+). The electrochemical gradient for K(+) is reversed, and inward K(+) currents trigger regenerative depolarization. These mechanisms of paresthesia generation can account for paresthesias in normal subjects and may be relevant in some peripheral nerve disorders.

The gist of which is that 1) cuteneous & superficial sensory nerves are more likely to have "false" firing (see section above where I quoted) not related to actual stimuli; 2) ischemia (lack of blood flow) causes tissue pH to lower, this is balanced by exchange of Hydrogen ions outside cells for potassium ions inside such that Potassium in the fluid is higher. Then when this hits a tipping point, potassium flows back into the cells - including the nerve. This triggers depolarization (aka firing) of the nerve.

Typing this out between patients, please let me know if you have any questions or want more explanation of something! Hope this answered your question.

Edit: vasa nervorum, not vasa vasorum
[removed]
As people have already correctly mentioned, the pins and needle feeling is due to external pressure on a nerve.  I wanted to expand on it a little bit.  There are a couple of reasons why the pressure affects the nerves.  The most common reason for it is that the compression on the nerve temporarily cuts off the blood supply to that part of the nerve.  This results in a reversible injury to the nerve.  The numbness as well as the pins and needle sensation is called a paresthesia and the nerve injury is called a neurapraxia. 

Now for the reason why you feel pins and needles.  What it actually is is your body's perception of pain from the nerve.  Nerves themselves are actually composed of a group of axons and a bunch of supporting cells.  An axon is an offshoot of the nerve cell (the neuron) and this axon is what brings the signal from one neuron to another.  These axons all send different signals.  There are certain axons that come from neurons that control movement.  There are other axons that come from neurons that control normal sensation and other axons that conduct the signal for pain.  When injured, these axons all recover at different rates.  The rate of recovery is dependent on a number of factors including the number of supporting cells and the axon's size.  It just so turns out that the axons that recover from the compression the fastest are the pain fibers and this is why you get the sensation of pins and needles first before you get any other sensation back.

Most of the time, the limb that fell asleep is temporary and harmless but if the pressure is on for an extended period of time, it can cause a long lasting or even permanent injury.  This is why, when a person undergoes surgery, the people working in the operating room take a lot of care in ensuring that no part of the body is being compressed too much.
Fine. Low quality answers... Ha!  

The medical term for pins and needles is "paresthesias". They usually occur when there is pressure on a nerve. Pressure on a nerve blocks conduction of neural activity in large diameter axons (nerve fibers) selectively, which leaves only conduction in small diameter fibers.  

The large diameter fibers in a sensory nerve are responsible for light touch and perceptions of length and force in muscles (proprioception). The small diameter fibers are responsible for crude touch, thermal sensation and nociception (responses to stimuli that cause pain in a conscious person).  

So, when there is pressure on a nerve, you lose light touch and proprioception, and you retain crude touch, thermal sensation, and pain. This results in the paresthesias feeling people report as numbness and tingling, or pins and needles. It usually means a nerve is partially blocked. 

If a nerve is actually partially blocked, a doctor is instructed to map out the body region containing the paresthesias, and use their anatomical training to assign it to the location of the pressure block. Nerve root blocks in the back are common paresthesias (like sciaticia), but so is the paresthesia leading to Tommy John surgery (ulnar at the elbow), and Guion's canal (pinkie numb). 

If you haven't fallen asleep yet in my lecture, the next segment will cover the Gate Control theory of Pain Perception, and how spinal cord stimulators achieve their analgesic effects. 
I see this Q has been answered but I’m too excited to not put in my two sense. This is actually my field! I work in neurophysiology interpreting nervous signals during surgeries that put the nervous system at risk. Mostly, they are spine procedures. Depending upon the position you’ll be laying in for surgery (particularly the prone “Superman” position) a system of nerves near your armpit is susceptible to discomfort of your arms are positioned awkwardly. In every day situations, if you are laying uncomfortably, you can feel the pins and needle sensation and adjust your position to relieve it. But for surgery, you won’t be able to wake up and adjust yourself. 

What I do is attach electrodes to different parts of the body and interpret neurophysiological signals from your body that I detect. I can see the effects of mal-position and adjust your position for you so you don’t wake up with a paresthesia (which can last for quite a while). 

Imagine going in for a back surgery and losing strength and sensation in your arms! That’s just one of the many reasons I might be on your OR team- to reduce the chance of you experiencing this post-op :) 

Edit: mobile formatting 
Neurologist in training here, the pins/needles sensation is not due to circulation, but due to something going on with your nerves. Nerves can be compressed, sheared, torn, damaged (by high sugar in diabetes or even by your own immune system). In any case, the most common reason to feel pins and needles is from compression. Your nerve can get irritated and send impulses to the brain in this form (ie, feeling like pins and needles, or even burning sensation). Prolonged compression can cause permanent damage. Eventually, the area with pins and needles will feel numb and become weak (depending on the nerve). SO if you sit on the toilet for too long and your legs get numb, no you won't have permanent damage.
Its your peripheral nerves (peripheral=think nerves to arms and legs) telling your brain they arent getting enough bloodflow. The nerves notify your brain first, usually before there is damage to the muscles, ligaments, tendons, cartilage in your arms and legs. This is a simplification. But hopefully useful. 

MD
[removed]
[removed]
Neurosurgical RN here. I work with neurosurgical patients who exhibit parasthesia (the pins & needles) as a common symptom. While the origin may differ the reasons this happens is the same; a nerve or group of nerves are unable to properly send signals to the brain. Most commonly it happens when a sensory nerve is compressed from being in an awkward or cramped position such as sitting on a foot fpr too long. The tingling sensation is caused by the nerve trying to resume sensory function by sending pain signals to the brain. More chronic conditions such as pvd or poor circulation can cause this sensation because without adequate blood supply nerve cells can no longer properly send signals to the brain. Hope this answer helps! 

Interestingly enough, it’s not a blood vessel that gets blocked or cut off when you get “pins and needles.” It’s a nerve. Nerves generally come in 3 types. Motor, sensory and autonomic (present in internal organs usually). The former two are the most common types and some of our nerves actually are a combination of the those two. The ulnar nerve, for example, innervates the flexor carpi ulnaris (a muscle that helps bend your wrist) and also innervates the area of your hand adjacent to your (and on the) pinky and the pinky-side of your ring finger, front and back. If one is to cut off the supply or block the ulnar nerve, one would feel “pins and needles” or paresthesia on the area aforementioned, and weakness of the innervated muscles. Usually when it’s not a serious injury to the nerve though, only sensation is compromised and it’s more or less temporary. Serious lesions, however, tend to affect the nerves motor innervation causing the muscles it supplies to weaken and waste away (atrophy), and takes more time to recover. Both sensory and motor compromises can be recovered through medical treatment and therapy.
The nerves that transmit sharp pain, proprioception (where your joints are in space), and fine discrimination (fine sense of touch) are big, expensive neurons that use a lot of energy.

The nerves that transmit dull touch, heat, and cold are smaller and simpler, so they need less energy.

Moreover, each kind of nerve pathway cross-inhibits the other, which is why a hot pad or cold pack helps relieve pain (temperature and full touch neurons shut off the pain neurons).

So when you cut off circulation to a limb, the sharp pain/fine discrimination/proprioception neurons are the first to stop functioning.  The “slow pain”/heat/cold nerve cells keep working longer.  So you get this unopposed sense of dull pain and simultaneous heat and cold that we call “pins and needles.”

An actual needle stick is transmitted by the fast nerve fibers, though.
Just a gentle reminder that /r/AskScience aims to provide in-depth answers that are accurate, up to date, and on topic. You should only answer questions if you have expertise in the topic and can provide sources for your answer if asked. For more details please refer to our [guidelines.](https://www.reddit.com/r/askscience/wiki/index#wiki_askscience_user_help_page)

In particular anecdotes are not permitted, especially as a top level comment. This is not the right subreddit to discuss your special technique of stacking shampoo bottles or using magnets to block your shower curtain. So far we have had to remove about 60% of the comments in this thread. Please refrain from speculations, personal theories and joke comments.
I recall reading an analysis of this phenomenon many years ago in scientific american. The shower head does generate airflow, but also in a hot shower the warm air goes out over the top, sucking in cold air at floor level, causing the curtain to blow in. 

I tried to find it again, but failed. Instead, [here](https://www.scientificamerican.com/article/why-does-the-shower-curta/) is a modern equivalent analysis of the airflow using a finite element simulation - it seems comprehensive enough.

As people have pointed out it's a combination of the [Bernoulli effect and (when available) thermal dynamics](http://1.bp.blogspot.com/-kybBoxBAOkE/U18FXdCTlJI/AAAAAAAABV0/p1y6rQr24hs/s1600/the+shower+curtain+effect+explained.jpg) of air being warmed by the shower droplets. But the reason for my comment is your second point: *how to prevent it*.

One thing is to get weights or in the case of a metallic bathtub magnets and place them on the sides of the tub with the curtain between the surface and the magnet. This will reduce the effect, especially if you can get them at both corners and the middle rim of the tub as well.

Some shower curtains have these weights/magnets [built in](http://4.bp.blogspot.com/-CkxSQJRO1AY/U7qDB9CTG7I/AAAAAAAABnU/sm6bhCGZVZ8/s1600/magnetic+shower+curtain+has+magnets+embedded.jpg), while others have [suction cups](http://4.bp.blogspot.com/-9cfUx4t_Zt4/VLHE4bXr3wI/AAAAAAAACJI/Y8GTzf0naAU/s1600/heavy-weights-for-shower-curtain-bottom.jpg) at the bottom. Alternatively you can also buy kits that allow you alter your current curtain with [hook and latch](https://ourhumbleabodeblog.files.wordpress.com/2014/03/master-bathroom-shower-curtain-hooks.jpg?w=399&h=600) anchors.
[removed]
https://en.m.wikipedia.org/wiki/Shower-curtain_effect

Hopefully Wikipedia is an allowed source (I'm too lazy to check the sidebar on mobile) but it's not necessarily a determined cause yet.

https://www.scientificamerican.com/article/why-does-the-shower-curta/

This article though does provide one likely cause, backed up by fluid simulations and a pretty graphic.  Basically the flow of water forms a vortex of low pressure that sucks the curtain in.
Hot water heats the air in the shower, it rises over the top of the curtain. Cold air comes in from below and pushes the curtain in to replace it. Everyone has told you this already, but no one said to leave a small gap between the curtain and the wall to allow the cold air in. This will prevent the phenomenon from happening. 
[removed]
There is more than one correct answer to this question. In short, the main reason is [Bernoulli's principle](https://en.wikipedia.org/wiki/Bernoulli's_principle), which states that an increase in flow rate of a fluid corresponds with a decrease in pressure. The water flowing on the inside of the shower causes the air to flow, as both are fluids. This decreases the pressure inside the shower curtain relative to the outside, which contains still air. Thus, the pressure is greater outside the curtain than inside, causing it to move inward. This works for water at any temperature, but the effect is amplified at higher temperatures because the hot air moves upward allowing cold are to enter from the bottom, where the curtain is able to move freely.
Hot steam is escaping at the top of the shower and colder air from the floor is trying to get in to fill the presure difference. The steamy hot air is just creating a looping pressure difference. I have the same issue because my bathroom is Basicly a closet and I shower at temperatures akin to the belly of a dragon.
You can get a shower curtain liner with magnets at the bottom.  They're made for cast iron tubs, but you can improvise by using some waterproof adhesive to attach small magnets at the bottom of your shower to match up with the ones in the curtain.
[removed]
Try this!

Does it gravitate to you when you take a cold shower?

Try it! I'll wait...


Oh! You're back! Well what happened?

It didn't gravitate to you with cold water. Weird, right? With the comparison of these two observations we can conclude that the shower coming at you has something to do with heat. 

What are some things we know about heat? 

It transfers through conduction, convection, and convection. Huh.

Convection is when warm air lifts. Well it sounds like we might be on to something...

Let's think it through: the warm air lifts but it won't leave behind a vacuum. Air has to replace the air at the bottom of the shower but the whole volume of air inside the shower is being heated. Where can it get cooler air?

You got it! Great job! The shower sucks in air from the outside where the air is cooler and drags the curtain in with it. We figured it out!

Now to apply our understanding to the problem. How can we keep the slimey curtain from getting dragged in? It gets dragged in when the air pushes it. But the air has to a strong enough force to left the curtain. If it could come in somewhere more easily it wouldn't push the curtain.

You got it! Give the air some room to come in! Open the curtain on the side away from the shower head to let the cool air in. No more curtain sticking to your leg! Great work! 
I've noticed this especially in clawfoot style tubs where all sides are exposed.

FIX: get some alligator clips and tie a string and a little weight. Then you clip onto the shower curtain near the bottom, the curtain sits inside and the weight hangs over the outside. Worked great for me!
I believe this was actually what the 2001 Ig Nobel was awarded for. Though somewhat unfortunately, no paper came out of this, so there are only [news articles about this](https://www.umass.edu/newsoffice/article/umass-amherst-researcher-solves-mystery-billowing-shower-curtain).
[removed]
Bernoulli's principle. Faster air has less pressure. It's the same phenomenon that causes lift. The air inside the shower is moving faster due to the spray of the water, decreasing the pressure, and the air outside the curtain moves inward due to having more pressure.

edit: some phrasing
Several other solutions:

1. Wet the edge of the other side of the curtain, then press it against the shower wall to create a weak bond, strong enough to stop getting hugged by the shower curtain. This can work in a dorm shower where you might be less comfortable leaving the curtain open. 

2. Buy a shower curtain that is weighted on the bottom or is simply heavier, and thus harder to be pushed inward by the air flow. In my dorm, I put a binder clip on each curtain corner at the bottom to weight it.

3. Years later, in my house, I solved the issue with two curtains... one fabric for show that hangs on the outside of the tub, and a vinyl one that hangs on the inside. The outer curtain cannot be pushed in because it hangs outside the tub.
It's not gravitating toward you. It's being blown toward you.

The heat from the hot water heats the air and we all remember from school that heat rises (unless we're talking computers, because then heatsinks :P ). As the hot air rises, it goes out the top of the curtain. Cold air gets pull in from the bottom/sides and pulls the curtain toward you.

Try pulling your curtain open an inch or two on either side and see if it stops doing that.
Physics. My professor demonstrated this and you can try it too. Hold two pieces of paper so that they are dangling side by side with about an inch of space between them. Blow air in the space between them. You might think this will cause the papers to fly away from each other, but, if you did it correctly, the papers will come together. 

Basically, when air is suddenly pushed out of the way, it creates a vacuum. Which is why the papers come together and also why the shower curtains fly in towards you when you turn on the shower. 
[removed]
It's because turning on the shower causes a pressure differential between the inside and the outside of your stand in shower enclosure. Why? In short, Bernoulli principle, that we'll state, for the purposes of this post as: for a fluid in a closed system the sum of the potential energy (topographic and pressure) and kinetic energy (flow velocity) is constant. 

Your bathroom is a closed system, for all practical purposes. Before you turn on the shower, there's no air movement so the air only has potential energy; it being at the same elevation means that the pressure is the same everywhere, so no pressure differential. When you turn the shower on you have a couple of effects like 1) the air and water coming from the shower displacing (moving --> velocity) the air inside the stand in shower, 2) the water temperature and your own evapotranspiration changing the temperature of the immediately surrounding air, causing density flows (hot air to rise, cool air to sink --> movement -->velocity), 3) hot air expands, reducing its density and, as such, its potential energy (this is implicitly included in the previous point but I figured I'd spell it out). This means that because now there's a kinetic energy component at play inside the stand in shower, pressure must drop. Pressure outside the shower stays the same, which means a pressure gradient from outside to inside is created. This causes air flow into the shower, that is caught by the shower curtain like a sail. More specifically, it causes cool air at the bottom to try to enter as hot air leaves from the top. That's the effect you see. 

How to prevent that? I've had good results with fabric curtains that let some ir through, become heavy as they soak up the water and dry quickly afterwards. Generally speaking, your two options are to let air in and to make the curtain stiff and heavy. Weights and magnets help, as well as keeping the sides of the curtains open and doubling up on the curtains with one outside and one inside (and it looks nicer IMO). 

Anyway, good luck. 
Bernoulli's principle! Fast moving air causing a lower pressure. Air moves from high pressure regions of bathroom to lower pressure regions (essentially a "draft" which causes the curtains to shift)

Not sure if the temperature effects it much. 
I'll preface with I'm no scientist but i always understood it to be temperature differential.  The cold air outside the shower is more dense and sits lower while the warm or hot air inside the shower push the air up and out. The key to this is to stick your foot under the curtain as it opens up and you'll feel a cold breeze.

Since the curtain is inside the shower and presumably fully drawn across this allows air exchange in only one allowable direction. As the cold air rushes in to equalize the temperature and pressure differential it pushes the lightweight shower curtain towards you.

To solve this either get a secondary curtain for the outside or allow part of the curtain to be left open to create a different flow.

Source: used to own one. 
Isn't this due to Bernoullis Principle? 

Water coming out of shower head decreases the air pressure (the faster it flows the more pressure decreases). 

That would mean the pressure inside the shower is different than the pressure outside the shower, with the outside pressure being greater. 

The increased pressure on the outside of the curtain is what forces it inward. 
One thing I didn't see here is, because the top of the shower, where the hot air is leaving, has more square footage than the edges and bottom where the cold is being pulled in. This would create a slight negative atmosphere. That is what draws the curtain sideways towards you.
Hot water is heating up the air around it causing it to rush upwards which causes a negative pressure at the bottom of your shower, which in turn sucks in the shower curtain. You can test this by using cold and warm water and see for yourself. 

You can prevent it by giving the air a path to rush in and equalize pressure, like make a few holes in your curtain at the bottom. But that would defeat the purpose of a curtain :)
Already commented this here, but posting it as a parent comment now. Everyone is mentioning Bernoulli's principle, but I didn't see any comments explaining exactly why does increase in velocity of air particles leads to decrease in pressure.   

To answer that first let's understand what is pressure. Before the shower is on, air particles are moving randomly in all the directions and hitting the shower curtain. This is what creates the feeling of pressure. Air molecules moving around randomly in all directions, but the net speed is zero because velocities cancel each other out. On the other side of shower curtain, air particles are hitting the shower curtain too at the same rate and thus there is no movement.  

When you switch on the shower, the momentum imparted by water molecules to the air particles, making them move in a particular direction(the direction of water stream). Thus, on the shower side there is net velocity of air particles in the direction of water stream. They aren't hitting the curtain as frequently or with as much momentum as before. But on the other side of the curtain, nothing has changed. The air particles are hitting the curtain same as before on the side where there is no shower. Hence this creates a difference in force on the curtain. Outer side has more pressure, and thus more force. This makes the curtain move towards the person taking the shower.


As other posters have elucidated, the buoyant force of the warm air creates a thermal plume, travelling upwards.
The thermal plume entrains cooler ambient air from underneath the curtain. This pressure difference caused the curtain to be pulled in at the bottom. 

Edit: [Source](http://vbn.aau.dk/files/197603023/Thermal_Plumes_in_Ventilated_Rooms_vertical_volume_flux_influenced_by_enclosing_walls.pdf)
I recall reading an analysis of this phenomenon many years ago in scientific american. The shower head does generate airflow, but also in a hot shower the warm air goes out over the top, sucking in cold air at floor level, causing the curtain to blow in.
I tried to find it again, but failed. Instead, here is a modern equivalent analysis of the airflow using a finite element simulation - it seems comprehensive enough.
Pretty much with the same phenomenon that keeps planes in the air. When air is flowing it creates a kind of suction.  A great quick way to see this is put an index card on a table, hold a paper towel tube over the card, then have a fan blow across the top. The card should then get "stuck" to the tube.
Mechanical engineer here.

The pressure of all fluids drop as their velocity increases. (This is a result of the first law of thermodynamics called conservation of energy.) In the case of your shower, the flowing water molecules from the shower head move the air molecules next to them, giving them a positive velocity. This creates an area of low pressure. The other side of the shower curtain is at "normal" (higher relative) pressure. The curtain sees higher pressure on the outside and lower pressure on the inside. This causes the curtain to move toward the area of low pressure.

It's called the Bernoulli effect. It's the same reason airplane wings create lift. In the case of wings, the top surface of the wing is curved which causes the air to move faster over it than along the bottom. So low pressure above the wing, higher pressure below it. Therefore, it creates lift.

Same thing happens when you open a window in a moving car. If you have balloons inside the car, they are drawn to the open window because the velocity of the outside air creates low pressure outside the window relative to the higher pressure inside the car.
It is Bernoulli's Principle in action!
(https://en.wikipedia.org/wiki/Bernoulli%27s_principle)

Basically, a stream of moving particles (in this case, the flowing water and the air it pushes as it leaves the shower head) creates a low pressure zone which causes the shower curtain to move inward. 

The same principle creates lift on the wings of airplanes, because the upper side of the wing is more curved than the lower, causing air particles to move more quickly over the top of the wing, thus creating a low pressure zone above the wing.
> and is there a way to **easily prevent that?**

Sorry if this was already mentioned, but I highly suggest using a shower curtain liner in addition to the curtain itself ([example](https://smile.amazon.com/gp/product/B013PQANXO/ref=oh_aui_search_detailpage?ie=UTF8&psc=1)).  This will accomplish several things:

1. The heavier vinyl is more resistant to the effect of being "sucked in" while showering

2.  They generally have built-in magnets that also help with this

3.  They prevent your curtain from getting wet and coming in contact with shower spray and residue -- primary shower curtain will stay cleaner and last longer

4.  Are generally mold/mildew resistant, but are meant to be disposable and can easily be tossed and replaced once they start to get "grimy".  (mine generally last ~6 months)
The answer hasbeen given, but no one has named it, it's the [Ventury Effect] (<https://en.m.wikipedia.org/wiki/Venturi_effect>).  It is the reduction of fluid preassure thats happens when the fluid accelerates through a choke in a pipe. In the shower case, you are the choke. 

Source: cardiac surgeon. There is a heart problem called [obstructive myocardial hypertrophy] (<http://emedicine.medscape.com/article/152913-overview>) that is caused partly due to the ventury effect. One of our tutors used the example of the resident's bathroom dirty shower curtain sticking on your ass when you where on call, to explain the [systolic anterior motion] (<http://journals.lww.com/thoracicimaging/Fulltext/2012/07000/Systolic_Anterior_Motion_of_the_Mitral_Valve.15.aspx>) of the mitral valve observed in that pathology. Great guy!
Anyone who is saying this is a temperature effect and not a Bernoulli effect is wrong. Go do it with cold water. It will still happen.

This effect was and still frequently is used by devices https://en.wikipedia.org/wiki/Aspirator_(pump) for lab-scale chemistry to produce a vacuum.

It is a Venturi based suction as described by the Bernoulli equation.
[removed]
Man, this drove me crazy as a kid. I would put shampoo bottles on the tub's sides to hold the curtain back. As an adult I have only ever showered in an actual shower stall, and hadn't thought about that old peeve in close to two decades.
Easy fix:  Get a cloth shower curtain, and a *second* one that you hang on the outside.  For example, a simple solid color one inside the rub, and then a nice looking one outside.  The extra weight prevents the effect compared to plastic curtains, and the second curtain helps block airflow.

You can also get a curved shower curtain rod for even more space.  You'll never go back if you do that though ...
I've thought about this in the shower and I have come to the following conclusion: The air in the shower is warmer and less dense than air outside the shower. That makes the warm air in the shower rise and pull in cooler denser air from below.

I find that to help alleviate this leave one of the sides open a crack. Gives the air an easier route into the shower.
While I see a lot of people mentioning the air temperature difference, this is not the reason for the upwards flow and reduced pressure inside the shower. The waterdroplets partly evaporate while traveling down inside the shower (high surface area since there are a lot of small droplets helps the evaporation which is why it also works with cold water). That watervapor is significantly lighter than hot or cold air and creates an upwards flow. Because of the bigger difference in density (compared to just hot and cold air) the flow of that speed is higher than if you just had hot or cold air. That speed reduces the pressure inside the shower (see bernoulli) which causes the shower curtain to move inwards. Since warm water has more energy more water will evaporate and the flow will be faster, that is the reason why warm water works better.

source: engineer with specialisation in thermofluiddynamics.
I have observed this also! From my understanding it is to do with actually how an aeroplane wing or "aerofoil" works. Essentially one side has a faster flow of air over the top of the wing (curtain) and therefore lower pressure, compared to the other side which is normal standing air low speed, higher pressure. 

Therefore the curtain will movie toward the faster moving air just like a wing creates lift, in this case the lift is toward you in the shower. 

[Diagram](http://www.skybrary.aero/images/Aerofoil1.jpg)
Pressure differential! The moving water from your shower makes the pressure on your side of the shower curtain low pressure. Air/fluids move high --> low to reach equilibrium. Air from outside your shower is trying to do just that with the air inside your shower. 

I have no idea how to fix it though.  

EDIT: I *think* it's the same principle as lift, where the lower pressure on the curved top of an airplane wing creates lower pressure, generating lift. In this case, movement is horizontal.
Since no one seems to have directly answered your second question: "How to easily prevent that", pull back some of the curtain farthest from the shower head. It will let the air circulate and the curtain should cease to billow.
There was a paper in the Journal of Irreproducible Research, about 15 years back, that explained that the hot water created currents that pushed the curtain back in.

The website took the article down, and the researcher (after using the company's simulation station for said research) took that article down from his resume. Perhaps archive.org has a copy of that.
Air inside shower stall gets hotter and exits stall at the top.  Colder air tries to enter at the bottom, pushing curtain inward.  Pull the curtain an inch or two away from the edge of the stall on the side opposite the shower head.
You can also move the shower curtain rod away from the wall of the tub a fraction to reduce or eliminate the effect.  I cant tell you why this works, but it absolutely does.  As little as 3/4 of an inch may do the trick.  If you have a cheapo tension rod like mine, you will be fine, otherwise you may need a carpenter/handyperson to drill holes in the tile. You will want the curtain to lean away from the shower head very slightly.
As far an easy solution goes, we have clipped a number (maybe about 6) [plastic clothes pegs](http://i.ebayimg.com/00/s/NTY1WDg1MA==/z/6RUAAOSwR0JUM6VE/$_32.JPG?set_id=880000500F) to the bottom of the shower curtain. This is easier and cheaper than buying a new heavier shower curtain or getting magnets and other things people have suggested.
Edit: This also helps fix the problem when the windows in the bathroom is open and having the curtain drawn back a bit at one end lets in a nasty cool breeze or even blows the curtain about more.
[removed]
Use a second, decorative (heavier), shower curtain on the outside. It will keep that air flow from pushing against the inner curtain. I used to think those second shower curtains were nothing but marg's trying to sell you more stuff. But they actually have a useful function.
Since the science side of things has mostly been answered, as to your last question, maybe get some stronger magnets for your shower curtain liner to keep it taut? Or weights if your tub is nonmagnetic?

Also they make curved shower curtain rods that are easy to install and can really make things roomier, amazon has several for ~$20. 
I have the same issue with my small shower stall. It seems to be an airflow thing, the cause of which is apparently up for debate. 

I have found that if you can leave a gap at the end of the shower that is furthest from the head (so that water splashing out is minimized), then the air has an easier time escaping without moving the curtain. Magnets or a few weights at the bottom of the curtain help too if the gap alone is not enough. 

Also, the first thing I do is spray a bunch of water onto the curtain, which seems to help weigh it down a bit.
In the Book "An Abundance of Katherines", by John Green, the protagonist answers this question. 

It reads: what happens is the water spray creates a vortex, kind of like a hurricane. And the center of the vortex-the eye of the hurricane-is a low pressure area, which sucks the shower curtain in and up."
If it's like my bathroom, it's the heat inside the shower rising drawing in the colder air from the outside. 

Heat inside shower makes air less dense, it rises. Denser colder air at bottom flows in to fill that gap. 

I prevent it by leaving a crack on either end of the shower curtain so the air can flow around either side. It also helped to have my baseboard heating on in the bathroom. 
I have the exact same problem with my shower (I live in a studio). What i do is take my shampoo bottle and put it directly in the center of the curtain on the shower floor. That does the job for the most part and keeps the curtain from going inside the shower alot. 
Hope this helps... :)
You guys, buy a fabric shower curtain. It doesn't billow toward you in the shower, and you can throw it in a bleach load of laundry from time to time to freshen it up. Plus, you're not throwing away plastic shower curtains every couple of years (months?). Tell me any downside to using fabric shower curtains. I can't think of one.
Dude, just lift the bottom of the shower curtain a little, wipe some water against the tub, and lay the curtain back flat against the tub. The surface tension of the water effectively pastes the curtain to the tub. Almost playful cat-proof too.

Edit: a word
Warm air moves up, escaping through the top. Due to the pressure dropping inside and the air flowing upwards, air can't enter through the top and tries to enter from the bottom where there is less space to get through so the curtain gets sucked inwards.

Using cold water would prevent this process.
Seems like a lot of good science for your "why" question below, but as to the "how-do-I-prevent"?...

...somewhere back in my early apt days I got in the habit of "spraying-down" the curtain with the shower head when I first started my shower.  The added water weight running off the inside of the curtain always seemed heavy enough to resist the push from the air flow.
heat rises, so when it goes over the top of the curtain it pushes the cooler air down. since the shower is most certainly against a wall the hot air pushes up and out in one direction creating a "circular motion" of hot air on top and cooler air down on the bottom. cooler air stays low but wants to keep moving so it scuttles underneath the shower curtain pushing it up against you and it repeats the cycle. 

you can prevent this by leaving your shower curtain open slightly on one end leaving room for the hot air to escape from, disrupting the previous flow of air that made the shower curtain push up against you.
>...is there a way to easily prevent that?

Many people have mentioned magnets which will work but it may not help you right now. The easiest, quick fix is to get a washcloth or two, wet them down then strategically place them so as to anchor the shower curtain. Depending on your shower configuration you may need to use a shampoo bottle also.
> Why is that, and is there a way to easily prevent that?

I've actually seen a number of explanations, all of which seemed acceptable to me.  But the best way to prevent it is to buy shower curtain liners that have magnets along the bottom.  :-)

The answer that I liked best was Bernoulli's principle.  Even cold water is moving, moving the air around it, and therefore creating an area of low pressure (relative to the air on the other side of the curtain).  [Here's a link](https://en.wikipedia.org/wiki/Shower-curtain_effect) to the debate on Wikipedia.

Shower ghosts 
Bernoulli effect and a chimney draft have already been explained, as well as the vortex generated even in a cold shower.

There is another solution that I haven't seen posted yet. Besides weights or magnets you can get a shower curtain rod that is curved outward. You've probably seen it in some hotels.

It offers a lot more elbow room even from the start, and the outward bend seems to reduce the inward bow of the curtain. I don't know if it's relative, or whether curving the curtain outward helps resist bending in the middle, a la bending a slice of pizza so it doesn't dip downward. (Gauss and Numberphile ftw.)
It's not the heat, it's the way the water is being sprayed onto your bathtub.  I'll bet you have it set on a spray type pattern.  If you change it to a more directed or concentrated flow pattern, you'll find the problem will go away.
As the shower warms, it heats the overlying air, which gradually becomes less dense than the surrounding air and begins to rise. creating a negative pressure zone at the bottom.  so it need to be filled with bathroom ambient air.

you can avoid this by taking cold shower, using a weight filled tick curtain or having a glass door.
There are multiple factors that can play into this effect but the most influencing one in terms of the curtain moving is pressure. The fluid (air and water and water vapor) in the area contained in the shower are moving at a higher avg. velocity then the fluids outside the shower curtain. This velocity causes a lower pressure gradient inside the curtain thus creating a force on the outside from the fluids wanting to move to the lower pressure area. 
Acid rain was caused by SO2 emissions from coal plants, **which have been cut by >90% since 1990**. 

The 1990 Clean Air Act Amendments kicked off a cap-and-trade scheme that incentivized coal plants to install scrubbers and/or switch to low-sulfur coal, then low-cost natural gas took ~50% of coal's market share since 2008.

Bottom line: coal is somewhat cleaner than it used to be, and we're burning far less of it.

[SOURCE](https://www3.epa.gov/airmarkets/progress/reports/index.html)
[removed]
[removed]
[removed]
The reduction in the prevalence of acid rain in the US is largely been attributed to the success of the EPAs Acid Rain Program.

All rain is somewhat acidic from rainwater forming carbonic acid from rainwater reacting with carbon dioxide, but acid rain is particularly lower in pH. This is due to the reaction of rainwater with nitrogen and sulfur oxides to form the much stronger nitric and sulfuric acids, respectively. The primary source of these nitrogen and sulfur oxides is power plant emissions, particularly those burning coal. The Ohio River Valley contains a large concentration of these power plants, and acid rain issues in the US were largely concentrated around this region and points downwind (the Atlantic Coast).

The Acid Rain Program was begun in 1990 based under the Clean Air Act. It established a market-based (cap and trade) system of regulation upon which emitters of sulfur and nitrogen oxides were granted pollution allowances. Polluters were incentivized to voluntary undertake measures to reduce the volume of their emissions as they could sell unused allowances for profit.

By most estimates, the Acid Rain Program has been largely successful. The Pacific Research Institute has estimated that this program has reduced total acid rain levels by 65% from 1976 levels while the EPA estimates the program cost businesses only a quarter of what was originally estimated. Savings in property damage and human health costs, such as lowered incidences of heart and lung problems exacerbated by acid rain, most likely have resulted in the Acid Rain Program actually saving money, overall.
[removed]
[removed]
[removed]
It is still there, albeit most data shows a decline in the prevalence due to consistent progress in reducing emissions.  The pH ~~acidity~~ of rain is decreased (made more acidic) primarily through the action of sulfur dioxide and nitrogen oxides.  One of the biggest changes has been the result of a significant decrease in sulfur dioxide generation by China.  Parts of the US has seen some change, however other regions showed almost no improvement, if any at all regarding the pH of its rainwater in this timeframe.

The other aspect of this is that acid rain isn't nearly as harmful as people thought it was in the 90's, at least in terms of the public perception.  Acid rain is typically in the 5.0-5.5 pH range, with parts of the US seeing as low as 4.5 pH.  On one hand, when compared to something like a can of Coca-Cola which has a pH of ~2.3 this seems like it is nothing, but on the other hand, the effects of acid rain cover such a large surface area that it is not appropriate to compare it simply in terms of acidity.  There is a little bit of controversy as to just how harmful acid rain actually is, but most experts agree the most affected part of the environment are the various bodies of water, which are more susceptible to pH changes.  The amount and acidity of acid rain isn't enough to kill off wildlife outright in most cases, however it does cause the water to leech aluminum from the soil. 

Most of this information is pop science that is easily searchable.  I do not mean this in a pedantic tone, but rather as encouragement for others to search for themselves because it is a topic that has a lot of data but also quite a bit of emotions and opinions that are not based on facts.

Edit: pH not acidity in the second sentence.  Very few things increase when it also decreases :D
Thanks everyone for the insight and your time to answer the question! It's what I figured but learned a lot along the way too. 
[removed]
As pointed out below, when we take issues seriously we can solve them and even create products which are profitable(Sulfuric Acid), but companies don't do these thing without some sort of imposed government regulation. Another example is the Ozone layer...which has also been addressed...Air Pollution and Pollution of many of our rivers and lakes(check out Lake Erie). The problem is when we can't see the problem/pollution and science naysayers deny, (groundwater pollution, air born toxins), then we need intelligent world leaders to rely on science to recognize and address problems.
The facilities contributing the most moved to developing countries with less regulation or they implemented controls if financially possible.  The acid rain in developing countries has increased over the same period. An analysis of the industrial raw material production location is a good exercise to see what happened.
[removed]
[removed]
[removed]
[removed]
Because of regulatories being placed to plants, and also because the after-profuction cycles at plants usually nowadays have "side-product" factory along with them and they recycle acids, gas and other harmful products and re-use them inside the plant either by making them into products or using them in creating other products.

all in all, they literally make money out of all plant waste that included acid.
[removed]
Prior to 1990, various command and control regulations were ineffective, leading to the crisis you refer to in your post. The EPA then introduced a cap and trade system, fixing a maximum pollutant output for the US and establishing a market to trade the permits that enforced the cap. This was successful because it internalized the costs of the negative externalities of acid rain without dismantling the underlying energy markets.
You're on the right track, but as always, it's more complicated than that.

For starters, COVID may not have necessarily started from a patient X, it could have been a group of people.  Not sure if we know that for certain, but that's besides the point.  You're right in thinking that as long as someone has it, they can still transmit it to other people, but then we get into ideas like herd immunity and how COVID-19 exists in the environment.

For starters, herd immunity is the idea that so many people have had an immune response to a specific virus that if it were to become prevalent again in a specific community, it would not lead to an epidemic, because only a few people would likely show symptoms when contracting it, if any.  As others here have said, we are no where near that yet, that would likely take a few years to reach, especially here in the US.  Even so, this does not mean it can't infect people, there are always those who cannot receive vaccines due to a weakened immune system, hence the idea of herd immunity and actually being smart enough to get your vaccines to protect those who can't.

Secondly, we aren't sure how long our antibodies will last for this strain of COVID, much less if COVID has, or could, mutate enough to where the antibodies wouldn't be effective in fighting it off.  Certain diseases, like tetanus, we receive a vaccine for over certain intervals of time, this is could be due to a number of factors, one of which is that some antibodies are not forever, they vary on their length of effectiveness, or memory, within the body.  Another possible factor for other diseases is that the disease is so potent that we are only able to use dead forms of the microbe (or various other methods of making vaccines) in the vaccine which doesn't elicit as strong as an immune response as a weakened form would cause.  The strongest response your body will have in fighting off a disease in the future is to actually become infected, and sick.  This is obviously not what we want, but a similar magnitude of response often occurs because of many vaccines.  As mentioned above, mutations could also become an issue.  The reason why there is a new flu vaccine every year is because it mutates so rapidly.  In fact, the vaccine you get is an "estimate" of what scientists believe the flu may look like that year, so it could be entirely ineffective, or pretty spot-on.  Even so, sometimes the antibodies we have work against infectious organisms that aren't exactly what they were made for, but still work to some degree.  Effectiveness of this topic is somewhat controversial.

Finally, sometimes it's not possible to eradicate something entirely, because it still exists in the environment.  COVID-19 supposedly started in bats, then mutated to be able to infect humans, that means that even though we could potentially reach a point where humans aren't being effected by it, it could still cause problems in other animals.  There are serious consequences that could result from this as well, not even considering the fact that transmitting from 1 species to another indicates that it does have the ability to mutate into a new strain, and COVID-21 or something could eventually become a result of that.

TL;DR - Yes
Possibly, but the difference between the hypothetical that you're suggesting and how this started is knowledge of it happening. When the first person became infectious, no one had any idea, it spread, those people spread it, and no one even knew there was a disease going around until symptoms started appearing possibly up to two weeks later, and even then it wasn't clear it was anything more than a cold until people started needing hospitalization.

Now, we know that this is going on, if we managed to contain and treat and contact trace people until we got to a point where only one person was left who had it, things would be different. Just to get to that point our behavior would have had to change to limit the spread, and while we may have relaxed from that at that point, enough would probably still be in place for this person to avoid spreading it. In fact, contact tracing would likely be necessary to get to this point, so even before a positive test result was received, they likely would know this person had likely caught it, and with the number of people infected being very low at this hypothetical point, we'd have the resources available to quarantine them until they were deemed no longer infectious.

So basically, now that we have a better idea of what we're dealing with and how seriously we need to take it, if we get our numbers low enough we can focus on isolating the few people who still have it until they've fought it off. The reason we can't do that right now is because the number of infected and possibly infected people is so high that it would be impossible to complete isolate them all, especially since many will require treatment to survive.
Thank you to everyone that answered my previous comment. I now can confirm what I read was probably misunderstood it's source of it had one, or I misunderstood. Thank you, and that's one of the reasons I love reddit, as it helps me see what is wrong with an idea, or confirms that one is likely a good thought backed with evidence.
Please take into consideration how long the virus is shedding and contagious and how many hosts the carrier is exposed to on a given day.

Pre pandemic I wouldn't think twice about going to a concert or stadium. I wouldn't have worn a mask. I might have made 1000s of connections a day linking the diseased individual to healthy hosts.

A virus can only multiply if it can infect new hosts faster than the hosts can fight them off. Covid is a very infectious virus that stays contagious got a long time. But it's still possible to get the rate of infection below 1. 

New Zealand has no cases. If they test or quarantine at their border forever they can keep that going. But if the countries that they deal with regularly also show progress they can likely open borders without large repercussions. And hopefully we can all become like that.
It depends on how long immunity lasts. Under the assumption that a previously infected person is always immune, eventually it will go away, or mutate to allow people to be reinfected. 

Even with this assumption, it's technically possible for it to remain in the population by infecting young people who have not yet gotten the immunity, and then cause another pandemic when the percentage of susceptible people is high enough. (Nobody born after this pandemic will have the same natural immunity). 

Allowing enough people to get infected for herd immunity to have enough impact would mean millions more deaths and long term health complications, which will over time be much more expensive than temporarily closing some businesses. 

If the immunity is not permanent, there's no guarantee that it would ever go away naturally, and it could remain endemic throughout the population for a long time, frequently spiking and starting other epidemics.
Possibly yes, but half the issue was nobody took it seriously enough. Even the countries that were eventually able to start flattening the curve acted too late, but now are levelling out more. 

So if we are careful, have a plan, and everyone is in on it, that's a HUGE start. And this time we already know how serious it is because we have followed the science after getting caught with our pants down the first time. 

Basically, just don't be the United States right now and I think eventually we will all be okay, call me an optimist.
Yes, they can in the absence of herd immunity. But, you can manage it.

Let's say that you identify that person, and force them to go into a hotel room for 2 weeks. You then find everyone who they came into contact with, and do the same. The virus peters out because everyone is quarantined and cannot spread it. That's contact tracing. But, you have to act fast, have rapid and available testing, and good contact tracing.

Let's say that didn't entirely work. Someone slipped through the cracks. But you have everyone stay 6 feet away from each other and wear a mask. As long as that person doesn't infect anyone else, the virus peters out. That's social distancing.

In essence, you work to put out fires here and there using these methods. But you need to have the infrastructure to do this, and the US has done a horrible job of doing this.

This strategy has been quite effective at controlling infections such as tuberculosis, another very contagious infection. Anyone who gets TB gets reported to the state, and testing happens to their contacts. I've personally been contacted and told that someone I interacted with had TB, and I had to get tested. Then people are quarantined and treated, many of whom who literally have to take their pills in front of a healthcare worker to prove they took them.
Several countries have demonstrated that, with action on the part of society, the outbreak can be contained.  We see this in the case curves of places like Italy, NYC, etc.

Without any changes in behavior though, you are absolutely correct.
We have a far bigger awareness of it now. Contact tracing isn't possible in the US right now because it's so widespread. If fewer people had it, it would be easier to track down and quarantine. 

&#x200B;

Additionally, it is possible for someone to be infected and not know it due to not showing symptoms. That complicates things exponentially.
[removed]
Good news however. Even if you lose your antibodies for Covid 19 after a few months, it appears as though T cels are also stepping up and fighting, so getting sick a 2nd time may actually be more rare than previously thought.
Considering this, can anyone explain why some euro countries who’ve “beaten” it are able to carry on with normal life now?? I see my European friends posing under the Eiffel Tower with hundreds of maskless people hanging around, going to full indoor restaurants with no masks in Italy etc. I get that their hospitalization rates are down, but considering they have not reached herd immunity yet why have hot spots not come roaring back in every area that has reopened ?
Plausible yes.  At one point only one person was infected.  Likely?  Not if people are wearing PPE and taking general precautions and isolating when sick.  In either case we now know that many countries can identify and contain single clusters through tracking and quarantine.  A single person with smallpox would be rapidly identified and quarantined.  Those they had contact with would be quarantined.  Smallpox would be contained unless massively spread throughout a country to many many people before people could react (assuming we had responsible people instituting epidemiological control.  China has 1.5 billion people and are essentially covid free.  There are still outbreaks but they are swiftly quarantined and a recent 100 person outbreak near Beijing caused Beijing to shut down for a few weeks and 500k people were tested as a a canvassing measure.  Everyone wears masks and the Covid-19 virus is demonstrably controllable.  The US just shit the bed because we have weak leadership, selfish anti-expert self important and self educated people using misinformation to make decisions they aren’t qualified to make and a government to weak to enforce the necessary changes to deal with a pandemic.  I can go into a lot more detail and give primary sources to back up points you don’t agree with (I’m a bio PhD candidate and have close contact with epidemiologists high in the world, country, and state levels...  I listened to them, their predictions made in January were all right, and have continued to be accurate).  Most that has occurred could be foreseen 6 months ago.  But the people with the power to influence the US outbreak have not done what’s necessary in most states... and none have done it as long as necessary...
[removed]
Absolutely. The key though is keeping the numbers small enough that it is possible to properly contact trace and quarantine, thus keeping the case count low and the death count much lower as well as allowing the economy to safely recover.
They could be if there are no countermeasures like social distancing.  In places where people are careful and policy is good the disease can be eliminated. Under reasonable conditions spread is limited.  If everyone is only concerned about themselves the disease can take off again.
Look at common sicknesses ... the flu .. the flu has always been around .. it’s covid 19s turn to always be around. We are gonna adapt, live with it.. come out with a vaccine that hardly works like the flu shot and live life forever with a few deaths from it every year. There will be no avoiding it. I just find it odd that no other sicknesses have been treated like this.. like ticks and mosquitos.. it’s odd we haven’t had quarantined zones for limes disease and other things like that. I honestly like wearing the mask to cut down on just getting normal everyday germs from people, people are disgusting..the whole covid thing is an eye opener to a lot not just covid
[removed]
I haven’t read all the responses so I’m sorry if this was already said. Yes. However, no one was trying to avoid it when it started. We can avoid it now if we all follow simple guidelines. If we all isolate (like the independent wolverine) we will bring the number of cases down and make it easier to avoid getting COVID-19. Humans are social creatures, but the virus is even more dependent on our social interactions than we are. If we take strict precautions and stay home as much as possible and wear masks and social distance in public we could stop the virus from spreading. This seems impossible, but even if we can’t get everyone to take these precautions seriously we could stop the exponential spread of the disease and protect the most vulnerable people in society by limiting our interactions as much as possible.
No. As more and more people get it then more and more of the population will be immune to the virus and it will not be able to spread as effectively. A vaccine would have the same effect.  The only way that it could be a catalyst for another epidemic would be if the immunity that people got form the virus only lasted a very short amount of time.
It could come back of course and it is pretty much inevitable. The thing that needs to be said is that there needs to be a two pronged approach. The virus poses a minimal risk of death to those who are healthy and under 50. Those people need to go back to normal life, back to work, and keep the world running. Those who are high risk should continue to isolate, if they choose to do so, and governments can look into ways to help them. This is basically the only way forward and really we already should have implemented this approach.
Let's say we have been contacted 1000 times in the history of the planet, so even one contact attempt every 4.5 Million years, how long were we listening for?  

How long would we have had the computing power to decode a message?  On top of that, are we even looking in the right place?  I saw a great cartoon once of a couple of ants saying they've scanned all known pheremone bands and can conclusively confirm they're alone in the universe.
This is a large part of the Fermi Paradox. The galaxy is only about 100,000 light years across, so even at 1% of the speed of light, it takes 10 million years to cross the galaxy. We evolved from small mammals to tool-using humans with space rockets over less than 100 million years. The invention of writing to the Apollo Program is maybe 10,000 years or less. All of these time-scales are much shorter than the age of the Earth, let alone the universe. This means that if life intelligent evolved anywhere else within the galaxy, it's unlikely that it appeared at the same time as us - it's almost certain that any intelligent life would be millions of years more advanced or millions of years less advanced.

This tells us that galaxy-colonising advanced life must be rare, as if there is intelligent life that has the capability and intent to colonise the galaxy, anywhere within the galaxy, anywhere in the past X million or billion years, they should have reached Earth a very long time ago.

Of course, there are multiple reasons why galaxy-colonising advanced life might be rare.

- they lack the intent, i.e. they could colonise the galaxy, but they choose not to leave their home planet, or they do explore the galaxy but leave us alone (basically the Zoo hypothesis)

- they lack the ability, i.e. even with millions of years of advancement it's not practical to leave a solar system in mass migrations, or a more advanced society generally becomes more at risk of destroying itself before it reaches that stage ("the great filter")

- *intelligent* life is rare. Life has thrived on Earth for billions of years before one species developed spaceflight. Evolution doesn't inevitably lead towards developing life that can invent advanced technology. There may be many planets out there full of animals and plants, or even just bacteria, but it's possible that humanity is a bit of a freak accident.

- life is rare in general. We don't really know how common life is. We know the ingredients seem to be fairly abundant, but how often do these combine to make something we would reasonably call "life"?

- the conditions for life are rare. However, as we discover more and more exoplanets, it looks like there are quite a few planets that seem like they would be hospitable to life, so this is less of a factor than we used to think.

So this isn't really a "paradox" in the common sense, because there are many ways to resolve it. But each of the resolutions involves stuff we just don't know - we don't know how frequently life evolves in the right conditions, we don't know how frequently life evolves to form intelligent space-faring species, and we don't know how often a space-faring space faring species would have the intent and capability to explore the galaxy. Any of these are plausible, and it could easily be a combination of everything.
In addition to what others have said, the "observable" part of the universe is the limit of what we will ever be able to receive light/information from. Since dark energy is pushing things further and further apart, the longer time goes on the more and more galaxies will be inaccessible to us forever and any intelligent life in it. Forbes did an [article](https://www.forbes.com/sites/ethansiegel/2015/06/08/dark-energy-renders-97-of-the-galaxies-in-our-observable-universe-permanently-unreachable/?sh=f80765983193) where they say 97% of the galaxies IN our observable universe are inaccessible to use even if we left today at the speed of light. This greatly reduces the amount of volume a potential intelligent civilization would be able to exist in and still be able to interact with. So for all we know there is life out there.. somewhere.. we'd just never be able to know.

So to recap, take the entire diameter of the universe estimated at 93 billion light-years. Then the observable universe 46.5 billion light-years. Then about 3% of that. That's what we're able to interact with.
Their signals may not have reached us yet. Our signals may not have reached them yet. They may not know how to detect our signals yet. We may not know how to detect their signals yet. They may not know how to send signals yet. They may not be sending signals anymore. They may not want to be found. They may be extinct. 

The variables in answering whether or not we're alone are astounding. There's no reason to expect to find anything even if it's out there. On the cosmological timeline we have only existed for like 5 minutes, and we've only been looking for 5 seconds. 

It would be like if there was one fire fly someone on earth that only lit up once for a quarter of a second once every three days. We glance out our window for half a second and don't see it. What are we supposed to conclude from that? 

Humans have a long ways to go before they can detect alien life. Hell, we're still discovering new species of life right here on earth.
You know how old the universe is, I don't think you know how big it is.

Our Galaxy, the Milky Way, is 100,000 light years in diameter. It would take at least 100,000 years to get a message from one side of the galaxy to the other.

How long have humans been building stuff capable of receiving messages that move at light speed, like radio waves? Well, the Radio was invented some 120 years ago. So in 100,000 years, the radio waves we give off from our civilizations will start to reach the far end of our Galaxy. That is just our galaxy of course. Andromeda, the nearest large Galaxy to ours, is some 2.5 million light years away.
Matters of scale are hard to intuit.

There are probably at least 2 trillion galaxies in the universe. What would it take for life to be considered "common"? One planet's worth of life per galaxy on average? That could be 2 trillion alien civilizations. Sounds like a lot. On a universal scale if that were the case I'd say life is pretty common.

Well the closest galaxy to us is 25,000 light years away. So if life in that galaxy took off headed for us 25,000 years ago and somehow traveled at the maximum speed physically possible (a speed we can't even get close to and won't for a very long time), they would just be arriving today.

This speed limits communication too. So to be more realistic, they would have had to pick up their intergalactic cell phone and dial our number 25,000 years ago, and we'd just hear it ring now. And they wouldn't get our response for another 25,000 years. And that's just the *nearest* galaxy.

The span of recorded history is 5,000 years total.

>25,000 years ago: a hamlet consisting of huts built of rocks and of mammoth bones is founded in what is now Dolní Věstonice in Moravia in the Czech Republic. This is the oldest human permanent settlement that has yet been found by archaeologists.

The universe is unfathomably huge, and chances are very good that even if it contained a ton of life (relative to its size) we wouldn't have a snowball's chance in hell of ever hearing about it.

Do you know what human life on Earth will look like in the year 27,000AD? The time it takes for humans to evolve from the oldest human permanent settlement ever found to 27,000AD (if we're even still around), is the same time it would take for a single round trip lightspeed communication with the nearest galaxy.
[removed]
~~Some people mention Fermi's Paradox, but that tends to waive most issues with space travel with an "assume technology is sufficient to overcome challenges" argument.~~ Correction, the below is technically still part of Fermis argument, still worth the breakdown to get the idea of scale we're talking about.

Watch [this video by Cody'sLab](https://youtu.be/dCSIXLIzhzk) to get the idea of just how incredibly mindbogglingly vast the interstellar expanse really is. That's just the nearest star to us, Proxima Centauri. There are approximately 100-400 billion stars in our galaxy.

You know how it takes you ~10-14 hours to fly from one end of the earth to the opposite side in a commercial jet airliner? If you flew to the sun at that speed, it would take you 21 years. Light, the upper limit of how fast things can go, takes ~8 minutes, and "running" around the earth at that speed would let you encircle our planet ~7.5 times **each second**. That's roughly how fast electromagnetic radiation (for example, radio waves) travel through vacuum of space.

[Here's a picture of how far human radio signals have traveled over the last 100 years.](https://i.dailymail.co.uk/i/pix/2012/02/27/article-0-11EF84AB000005DC-804_1024x615_large.jpg) From center of that dot, to the edge.

Taking this into consideration, think about the logistics of the space travel and communication:

- You need to know where to go. Mapping planets is extremely difficult, as they are very small and produce no light of their own. Identifying which planets can support life, possibly without terraforming (which is also a process that could take decades) is like trying to figure out whether an apple on a table 2 miles away is edible or rotten while you're wearing a blindfold. But let's assume the aliens are just so damn advanced, they mapped the whole galaxy for planets. They now have to...

- Produce a colony ship/an Ark that can travel for millions of years self sufficiently, effectively creating a space habitat, that can travel at, let's say, 1% the speed of light (that's ~2,990 km per SECOND) and doesn't get blasted to smithereens after any potential collisions with small, undetectable celestial bodies. We can assume that a sufficiently advanced AI will be able to permanently monitor the direction of travel and automatically slow down then adjust the course of the Ark if it notices a planetoid on a collision course. But an asteroid the size of a city or smaller? It likely wont obstruct enough stellar background for any AI to notice (and space is very dark, so you wont just conventionally see it), while slowing down from/adjusting course at 2,990 km/s in a vacuum of space is challenging to say the least. But let's say all the above are solved with the ingenuity of science and technology. We're finally hitting the last step of the problem...

- The people. Assuming the aliums are space elves and live 10,000 years each, assuming 1M years travel time, that's still 100 generations between the start of the journey and the end. Communications between the colony ark and the point of origin are pretty much out of the question - at a mid point in the journey, it takes 5,000 years for a message to travel one way from the ark to their planet alone. So you have people living and dying on this ark, for 100 generations, each of them living ten thousand years, all having to be educated, indoctrinated and somehow controlled to maintain the mission. Unless they're of a gestalt consciousness that transcends spacetime, or have zero capacity for rebellion and self expression, this likely wont end well long term. So the only option is cryogenics which allow you to freeze your colonists for one million years before they reach their destination.

I think recent human history, Hollywood and games seriously skewed people's idea of just how ridiculously hard the above would be in reality. It's not impossible, but that's a lot of technological hurdles and unknown variables to overcome. It's similar when talking about structures like Dyson Sphere. Talking about them requires taking a gigantic leap of faith that involves the logistics of the whole operation, but when you think about gathering and transporting materials alone, it starts feeling quite silly. 

Also, it all needs to pay off - any civilization advanced enough to produce this level of tech likely also has some degree of economy and whoever builds the ark needs both funds and a reason to undertake such colossal task. Even if you assume an idealistic space empire or an absurdly rich and powerful technocrat, that's yet another hurdle in this endeavor. 

Now, you could simply wave your hand and say "they probably would have warp/wormhole travel/instant teleportation tech by then", but at this stage it's just making things up and anything goes.

Edit: a typo and numbers correction
[removed]
My favorite solutions to the Fermi Paradox are:

1. Gamma Ray Bursts make most of that time period unavailable for life. We are among the first so there hasn't been time yet for anyone else to get here or signal. This is covered in "[Where is everybody](https://www.nature.com/scitable/blog/postcards-from-the-universe/a_possible_answer_to_fermi8217s/)" which has links to some more formal research papers at the end.

2. Life is rare and needs a ridiculous coincidence to happen. In our case it is the collision that apparently gave us liquid water and seasons and plate tectonics. Described [here](https://youtu.be/dQsa-pemRPA).

Both could be true.
Your life span is 0.0000005 % of the universe’s 
current age. 

It would only be in the last generation or two that a story would be recorded in a way that would identify aliens and not gods/demons. 

And even then, that assumes the goal of visitation would be to publicly contact one of our often violent tribes of nuclear armed apes decedents. Instead of say, survey a section of jungle, or pop down to talk with dolphins. 

Even if space traveling aliens are common, I think it will be quite some time before the average person can detect them. Quite in the same way that it is difficult for the average ant to detect people flying around in planes.
The universe is massive. Assuming inventing tech that allows speed-of-light travel is impossible, traveling that vast distance to find other life would take far too many resources and wouldn't be worth it.

Statistically, yes, other life exists in the universe. Perhaps outside of the observible universe, but somewhere, life exists. And it would NEVER come here. It simply isn't worth the time, resources and / or time.
Earth may have already been contacted, but we may have just been deemed not worthy at the universal scale. How often do you think of bacteria?  Likely not much if at all, because for you they don't really matter, sure there are a group of people that study them, but compared to how may people don't think about them those people are a tiny tiny amount.

Also who is to say that they are not contacting us right now, but we just lack the technology or skill to be able to understand them. How do you as a human communicate with an ant? 

Then you have time just being a  huge issue, Earth may have been around for a long time, but humans have only been around for a very small amount of time, especially humans smart enough communicate through space in some form. So we haven't been in the communication stage for long enough to contact anyone. Then add on top of that the idea that once a species get too smart, they will eventually cause their own destruction, which would leave us even less time to communicate.
There is an important thing to remember: the distances between planets are HUGE. Even if there are some advanced civilizations in out galaxy, chances are, they see Earth as a fairly unassuming exoplanet that is too far to be worth colonizing.
[removed]
It's more likely evidence that the speed of light and the vast distance between stars is a universally insurmountable obstacle. They're likely out there but travel and contact is, and always will be, impossible.
[removed]
You're assuming non-human intelligencies abide to our anthropomorphic view of life. For instance all living beings on Earth are descendants of a unique ancestor commonly referred to as the "last universal common ancestor" ([https://en.wikipedia.org/wiki/Common\_descent](https://en.wikipedia.org/wiki/Common_descent)), for instance [the genetic difference between humans and chimpanzees is less than 2%](https://en.wikipedia.org/wiki/Human_evolutionary_genetics)

This life you mention for all we know could be so vastly different from organisms on Earth that they would be incomprehensible to us. We assume these beings would have something akin to a biological body, with a "tip" for a head and means of movement like legs and so on. We assume their means of travel would abide to our CURRENT understanding of means of travel ie. propulsion and that their understanding of physics would be the same as ours, when ours changes constantly.

Other forms of life could take the form of [bionic](https://en.wikipedia.org/wiki/Bionics) systems, the symbiosis of digital and biological parts that would be indistinguishable from each other. Current human evolution seems to be taking a [transhumanist](https://en.wikipedia.org/wiki/Transhumanism) trend, sci-fi stuff like "enhancements" and so on, for example for a species studying us, our cellphones could very well be considered an extension of ourselves, giving us the vast knowledge of the internet in seconds. Now imagine a complete symbiosis with this knowledge database, via say [Brain-machine interfaces](https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface), giving this person immediate access to all human knowledge seamlessly.

This stuff sounds like sci-fi but it's all progress that could be adapted in less than a decade for humans.

Most of our search for life elsewhere assumes life conditions being the same as here on Earth (understandably, we can't search for complete random conditions for the sake of it). It is assumed other lifeforms should be carbon-based, but we know [biochemistry can theoretically take other forms](https://en.wikipedia.org/wiki/Hypothetical_types_of_biochemistry) and depend on other solvents besides water. On Earth there a few multicellular organisms that are completely non-dependant on oxygen ([https://phys.org/news/2010-04-scientists-multicellular-life-doesnt-oxygen.html](https://phys.org/news/2010-04-scientists-multicellular-life-doesnt-oxygen.html)).

At this point, the question arises: what could even be considered life? There are advancements in so-called [Strong AI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) and [artificial consciousness](https://en.wikipedia.org/wiki/Artificial_consciousness), so this "life" could even be non-biological (so called "[non-cellular](https://en.wikipedia.org/wiki/Non-cellular_life)") or non-corporeal for that matter. (By non-corporeal meaning not interacting with our percieved dimensions of spacetime, an old philosophical concept called "[Non-physical entity](https://en.wikipedia.org/wiki/Non-physical_entity)")

Going back to the original question, in my opinion it's a big naive assumption to think these lifeforms would transport their biological forms (if they have one) in conventional looking crafts via fuel-based propulsion and we could "interact" with them as if interacting with a smart animal.
Marconi sent the first trans Atlantic radio transmission in 1901. 

The Soviet EPR sent the 1st extraterrestrial radio broadcast in 1961, to Venus (no one was home). 

In 1974 the Arecibo message was sent to M13, 25,000ly away (I’m told a prompt response is expected sometime in the fall of 51,974). 

In 2003 a series of radio messages known as Cosmic Call 2 were broadcast from Crimea to Gliese 49b, a super earth orbiting the red dwarf Gliese 49, 32ly away. 
Assuming the Gliese 49bians are home, advanced enough to receive radio signals, are able to decode the binary message, and are in the mood for a chat, we can look forward to a letter sometime in the late 2060s. 

We are ants on an island in the pacific, tossing messages in a bottle into the ocean and any extraterrestrial species with similar capabilities will be limited by the same obstacle: the scale of the universe.
[deleted]
[removed]
This would be making the assumption that there is life anywhere remotely close to us, and that it is also technologically capable of reaching us for solely communicative purposes in a manner that is not a massive waste of energy and also doable within their organisms lifetime. Also that we’re worth contacting/able to be contacted safely in a way they deem fit.
Life's been around for a billion years.  Should radio be a million year old invention?  The universe is big in both space _and_ time.  Worth noting that a clone of earth around the nearest star would still be very difficult to detect, since things like radio communication are designed for short ranges, not light years.  Inverse square law turns even the strongest signal into a broken whisper at stellar scales.  It's like putting a paper cone over your ear and trying to hear a man shouting in Madagascar. 

  Even SETI limits itself to a narrow band that _might_ be useful to a civilization that really wanted to be heard.  Beyond that scenario, we haven't really looked; I think the most viable Fermi solution is that we're practically deaf.
The Fermi Paradox bums me out so much that I decided to think of it this way: picture an anthill near the shoulder of I-95. The ants have a complex society, they explore their surroundings. But they have no concept of what those rushing metal things going by are.
Most people get that the universe is so big that life probably does exist out there. However, what’s easy to miss is that life is *so* unlikely that even though it is so huge, there’s still not a great chance of life from other solar systems finding each other. 

Take the lottery for example. Most people realise they will probably never hit the jackpot, but it can’t be *that* unlikely, right? Especially if you get a group together and buy loads of tickets? Well you could have been playing every week since the dawn of mankind and you still probably wouldn’t have won yet. 

When probabilities get really small, it kind of doesn’t matter how big your sample size is. 

As a bonus point, you mention that the earth is 4.5 billion years old, and the universe is 14.5. In other words, it’s taken about a third of the entire life of the universe for us to get a single spacecraft to the edge of the solar system (Voyager). We’re still so very far from actually visiting other stars. So any other alien life out there is likely in the same boat. 

One final thing - the speed of light. This is a hard cap on how fast things can move. Unless there’s some kind of wacky science that we don’t yet know about to get around it (which might be literally impossible), it’s just not feasible or worthwhile to fling anything at other stars.
Something I wish I saw talked about more is that despite the age of the universe, relative to the lifespan of the universe we are hardly into the first moment of existence.

Our solar system is amount the first with the required heavy elements for life to be possible because it required sufficient generations of stars to reach late stage heavy fusion going supernova.

And life on our planet formed almost as soon as our planet cooled so while we have no frame of reference for how fast or slow we were to evolve we can say life began evolving on earth as early as it could.

This would indicate life forms quite readily when the conditions are met but the universe isn't old enough for significantly older life bearing solar systems to exists. We may be at the very early stages of a universe teeming with life
Not if what we know about physics to be true.  It takes an immense amount of energy to move any sort of mass.  Any planet / society would struggle even to make the decision to spend it on a trans system trip.  And then there's the time dilation - which is another issue added on top.  Visiting other solar systems for us will either be robotic only and centuries after launch, or using technology and mathematics and physics that we don't even have reason to believe really exists at this point.  And it may be the same for any other species out there.
With respect, there is a fundamental flaw in your question. You assume that Earth has **not** been contacted yet.  It is entirely possible that aliens have been sending us light-based signals once every million years for a month over the last billion years. That would mean they have sent us a message a thousand times and received no response.  Humans have arguably had the ability to detect and recognize such a signal for only the last 0.01% of the most recent million years.  And it is even arguable now whether we have that ability.  Right now we have the capability to monitor only a tiny percentage of stars for signals.  

In addition, though physical travel is nearly impossible over the insanely huge distances involved, it is important to think about the experience of such aliens had they dropped by Earth every million years for the last billion years.  Life remained mostly small and microscopic until about 580 million years ago, when complex multicellular life arose, developed over time, and culminated in the Cambrian Explosion about 541 million years ago.  But even if they last visited only one million years ago there would have been no signs of intelligent life and no technology whatsoever.  Humans only started farming around 13,000 years ago, and have had satellites in space for less than 100 years.  We are absolute infants.  

Something else to keep in mind, which others have mentioned, is that it is entirely possible that intelligent species of aliens long ago concluded that the smartest thing any alien civilization can do is **stay hidden**.

Are there war-like aliens out there who hunt emerging threats?  If so, one has to ask themselves whether sending out a bunch of messages is worth the risk of being discovered and potentially exterminated.
For us to be contacted, it is not sufficient for life to be common in the galaxy. Rarity of some other things might also leave us alone.

* First, life itself could be rare. And beginning of life is so rare that it doesn't happen elsewhere. (it is challenged because emergence of life on earth happen quickly after earth being born)

* Life could be common, but multicellular life could be rare. In close to 4 billion year history of life, there were only two events that gave rise to multicellular life. And one of those events gave rise to plants.

* It is also possible that even multicellular life is easy to come by, but it is so fragile that it got extinct before doing much. (We've faced several mass extinction events. And we live in a rather empty part of milkyway, on other planets asteroid collision or nearby supernova are much common)

* Maybe multicellular life survives easily, but evolution of intelligence is tough. It took multicellular life 540 Million years to evolve first intelligent species. Also note that other Human species like Neanderthals were also intelligent beings, but even they are extinct now. Upto 20,000 years ago, us Homo Sapiens were not doing exceptionally great either. (IMO this could be the case with life on other planets).

Now the next parts of "life being rare" would also apply to us, so let's see the ways in which humans don't visit other other stars.

* Maybe Intelligent life is self destructing. Imagine us humans being wiped out by some super bacteria or nuclear war or skynet.

* Maybe interstellar travel is tougher than we think. And we might never go beyond solar system (this point I don't believe personally)

* We might choose not to colonize whole galaxy. Next 500 Million years we can spend just by moving Earth away and close to sun to be in goldilock zone and after that we may go to some nearby red dwarf star to spend next trillions of year.

* We might colonize whole galaxy (or maybe even local galaxy group) but choose not to disturb other planetary bodies that already have life. We have taken care of moons of Jupiter, by crashing space probes into Jupiter, just to make sure that we don't infect or interfere with any life on those moons.

Some of these hypotheses are called _Great Filters_.

EDIT: Or maybe we have already been contacted. All those UFO videos are true and we are all doomed ;)
To add to the Great Filter explanation for why we see no other intelligent life, I wonder too whether it's just as likely that advanced civilizations merely age and slowly die gracefully I'm a way similar to most organisms (as opposed to obliterating themselves with powerful forces).  With world population set to peak and then decline by 2050, I wonder if humanity's future as an advanced society is one not too different to that of a quiet retiree's, looking back fondly on good times and accepting an end that's not too far in the future.

If that's the case, then we'd have only had a few short hundred years where we were space capable or able to transmit messages across space.  If this were true, civilizations would ignite around the universe, enjoy a few hundred years of high technology, then blink out before having much time to contact one another.

There's something philosophically interesting about this possibility.  We always think of life as--- by definition--- something that is always trying to grow and expand and fill every crack.  But it's interesting to think of an elder intelligent society sitting back and saying to itself "we've had a good life, now it's time to give someone else a chance."  It may just be that we humans have spent most of our history so far in our impetuous youth, and so we expect to go out and "conquer the stars" with that same youthful vigor.  But no one stays young forever.
Considering that we've only had the ability to even do wireless communications for less than 100 years of that 4.5 billion years, it's a relatively small window.  In addition, based on our conflicts, number of people that don't believe in science, and various other factors, we might not even be considered worthy of being contacted...
Whose to say during Earth's 4.5 billion years that we already haven't contacted/ visited ? 
I know it's unlikely, but it is something I do ponder about time to time. What if something happened during the age of the Dinosaurs ? The likelihood of us knowing about it is almost 0. What if we were visited during the early days of man and they witnessed first hand something landing from space. Sounds ludicrous, but again I don't think we'd ever know about it. 

But then I can't help but also think what if the life outside of earth is something like bacteria. Sure it'd be fascinating for science and whatnot, but for the average folk, they'd be like "meh".
After reading a few books about cellular evolution, I understood the following. Single cellular life is not very special. It can quite easily emerge on any "alive" planet - any that has volcanic activity and a set of chemicals, that are able to produce organic molecules.

Primitive life on Earth had emerged quite early, and didn't change much for billions of years. Until a very rare event happened. Once a cell got into another cell, but wasn't eaten, but became a partner. This endosymbiosis event had solved a huge threshold (related to energy consumption) that was limiting the cells from becoming more complex.

So, this very rare event took billions of years, and then during about 0.5 billion years the life has exploded in diversity and complexity, which led to conscious organisms that are able to contact other planets.

So, if what happened on Earth is universal (and the books I've read convinced me that it is), almost any planet that has liquid water and volcanic activity, has primitive life forms. But the absolute majority of those planets have only primitive life forms. And only a tiny fraction of them have life that managed to solve the "rare event" problem and started consuming more energy and evolving further.

But as soon as that "rare event" happens, there's not much that can stop life from producing intelligent species like humans. But the event is so rare, that makes intelligent life is also rare.

But another thing here us that as soon as life gets intelligent, the evolution gets so rapid, so it just gets almost instantly teleported into some final form, that doesn't care about contacting others at all. They are just too wise and have things to do.

So, to simplify: first life takes billions years to get to multicellular life. Then about 1/2 billion years to get to the conscious life. Then conscious life takes maybe 100K-1M years to get so much beyond, to become something that we won't even consider as a life. And this tiny time period of 1M years per planet would happen for any planet that has life potential. And these moments have a tiny chance to coincide on the timeline of the universe while being close to each other in space. That's why we don't have many contacts.
No, because of the speed of light. It may indeed be as much as a speed limit as we think it is. Although we have theories on how we might be able to get around it, we could be wrong and there are no ways to get around it. If that's the case, life could be all over the universe, but we are all too far away from each other to make contact.

Or life is exceedingly rare. Or we have made contact and no one is telling us, perhaps due to religious reasons. Or aliens are intentionally trying to keep a low profile. All of these are possibilities.
We probably can't detect a signal unless it is a directed signal.  Picking up TV from a distant planet would require a large antenna.  We existed as an intelligent, completely radio quiet species for so long that the percent of time we have had radio is nearly zero.  

Also, we are made out of meat.  And we put meat sounds on the radio.  Aliens are not interested in talking to meat.
We can already say that silence is an indication of the rarity of intelligent life.  The key is that "rare" is relative.

*How* rare?

So, we know we don't live in a universe where every single planet has advanced civilizations on it because we live near many planets already and only one of them comes even close to showing signs of intelligent life (ha ha little joke there).

Beyond that we have to start making some assumptions.  One is that intelligent civilizations will at some point in their history use radio signals to communicate.  That's not nearly as certain an assumption as what we know about our own star system but it's still probably a pretty good assumption.

As long as we accept that assumption, then the scope of our definition of "rare" can expand greatly, for the simple reason that if intelligent civilizations use radio, as you say, we should have at least heard something.

By the way, the chance of someone contacting us, specifically, as an intentional effort is actually pretty low ... the outward signs of our existence have not propagated more than 100 light years or so at the absolute most, which means 50 light years is our best round-trip case for return communication, and we know of only a thousand or so star systems in that range.

So, from that we can conclude that there aren't radio-using intelligent civilizations right now on every star system, just as we already know that there aren't on every single planet.  But beyond that we can't conclude much.  What if they existed in the past but are gone?  And there's always that other question -- what if they don't use radio?

But beyond that, a thousand-something stars isn't much in the grand scheme of things.  The whole rest of the vast universe hasn't had a chance to hear from us yet, let alone reply.  So now things get tricky.  Way out in the rest of our galaxy, in the 1000s or 10,000s light year range, what is going on right now?

We have no idea.

There could be intelligent life popping up and starting to use radio all over our galaxy right now and we would have no idea.   There could be a new civilization sending its first radio broadcasts every year.  We won't have any idea for many millennia to come.  Effectively, those civilizations may as well not exist.  We will never come into contact with them.

So that gives an idea of how rare is "rare" when it comes to intelligent life -- we can actually say with certainty that it is pretty damned rare, but not necessarily unique in all the universe since our ability to estimate with any degree of precision only goes out so far.

(We can try to make more guesses, based on stuff like the fact that we aren't today hearing from any ancient civilizations from 10,000 years ago, but then we really start running into the limits of our assumptions about radio and the age of civilizations and so on.)

TL; DR  We already know that intelligent life is super rare.  Heck some days on Twitter it seems like there might not be any at all.
You need to remember that many of our scientific achievements which we consider to be signs of intelligent life have only been around for about 100 years. Specifically radio waves and satellites.  
  
And any intelligent species that might detect us could be hundreds or thousands of light years away and either unable to make the trip or unable to see us. When we look at a star in the night sky, we are looking at a past version of that star. It would be the same for any alien species looking toward Earth from their point of view.
I read an interesting theory recently, which said that we might be one of the earliest species to evolve as intelligent life. 

To sort of sum up the theory, although the universe is about 14-15 billion years old, life itself (as we currently understand it) has only been possible for about 10 billion. So the Earth is actually early in that. 

Also, looking at the possible lifetime of the universe gets you a conservative estimate of several trillion years, so the universe is barely out of infancy - peak sentient life might be a long time in the future. 

Some more reading below:

https://www.npr.org/sections/13.7/2016/08/25/491307739/are-we-the-earliest-intelligent-life-in-the-universe
A slightly strange, but nonetheless important question in this space is whether we would even know what contact looks/sounds like. 

In the context of intelligent life, it could be a totally different setup in terms of consciousness that still follows the physics of our universe. Not to mention other universes.
We may have been contacted at some point in the past and there's just no record of it. But assuming we haven't, one hypothesis is they want to leave us alone kinda like how nature documentary filmmakers have an unwritten rule of observing without getting involved. I'm sure it's not easy seeing an animal get killed by a predator, but nature should have its course. Aliens might see it as unethical to involve themselves, and want us to evolve "naturally".
[removed]
I would recomend checking out these videos, [https://www.youtube.com/watch?v=sNhhvQGsMEc](https://www.youtube.com/watch?v=sNhhvQGsMEc) [https://www.youtube.com/watch?v=UjtOGPJ0URM](https://www.youtube.com/watch?v=UjtOGPJ0URM) the only thing I will say about these videos is that they are a good starting point. There is a lot of stuff that is brushed over or condensed so I would recomend further reading around the subject if you find this interesting but they are still a good starting point to establish a basic understanding. As an aside, you may find this interesting too if you are interested in the rest [https://www.youtube.com/watch?v=uzkD5SeuwzM&t](https://www.youtube.com/watch?v=uzkD5SeuwzM&t)
[removed]
The distance between stars is VAST. Life may be very rare, but it also simply be that we any signal can only get here at the speed of light, and might be super weak when it gets here, and might be unrecognizable for the brief moment of time we are listening in any specific direction, or extra terrestrial life may simple be less developed. The universe may be teeming with bacterial life, but they won't be talking to us.
Humans have only been able to receive radio signals for a couple hundred of the millions of years life has existed on earth, and our civilization has been on the brink of collapse multiple times in that period.

Even if there are millions of inhabited worlds in out universe, the odds that any of them have a space faring civilization, able to communicate in a way we can understand, is close enough that travel between the two is possible, and exists at the same time as us is very low.
I wonder if it is even doable for a civilization to visit other stars. By this I don't just mean the practicalities of traveling these vast distances, because even that challenge is massive and we are only theorizing that it is possible under ideal conditions.

But there is also the societal point of view. We only have Humanity as a model, and we can see that the big space-faring nations of Earth are not particularly interested in exploring how to travel to distant stars. Moreso, space-travel is about economic gain to a lesser degree, but mainly about politics/national defense. There is little incentive for any nation to push for the colonization of even Mars for example. It's a huge undertaking and if you pour too much money in it the population back on Earth starts to complain because societal problems like inequality are also expensive issues to solve. So why pour money into the research at all? By the time that incentive exists to leave Earth, for example due to the planet becoming too inhospitable or overpopulated, the resources and knowledge might not even be accessible anymore. It might be too late. That seems like more of a Great Filter to me than "nuking one another into the stone age"
Perhaps it’s just impossibly far to everywhere. Let’s say the fastest anything can ever be accelerated on purpose is 1% the speed of light. Then to reach the nearest star takes nearly 500 years. So it’s limited by the speed one can travel. Best we can hope for is radio communication with the lag time of the speed of light. You speak now and hear a reply ten years later.
We really haven't done very much searching, so it's hard to claim that we haven't been contacted yet. Jill Tarter from the SETI Institute likes to compare the search for extraterrestrial intelligence to searching through an ocean, a thimble at a time. Using that analogy, we have only sifted through a hot tub's worth of water out of an entire ocean. 

[Paper](https://arxiv.org/abs/1809.07252)
One thing people haven't really talked about is the fact for all 14.5 billion years of the Universe's life it wasn't in a place to make complex life.  After the Big Bang the only elements that would have formed from the initial high energy plasma is Hydrogen, some Helium and tiny tiny amounts of Lithium.  For anything heavier up till iron you need the first generation stars to make.  For anything heavier than Iron you need Nova.  So the deaths of the first gen stars.  Our sun is a Third gen star so it may have taken the deaths of the second gen stars as well to build up heavier elements in sufficient quantity to for complex life.  It's entirely possible that humanity is one of the first civilizations.
In the grand scheme of time we are at the very start. There far, far, far more years years ahead of us and only a fraction of the hydrogen in the UNIVERSE has been used. So if you ever wanted to know where humanity sits we are probably in the group of first ones if not the first.
Something to think about is life used to exsist on earth before oxygen. In fact oxygen caused an extinction event.

So if a species mapped the galaxy 200 million years ago and saw a planet with a toxic atmosphere filled with literal dinosaurs the planet would be marked as unsafe and they would move on.

Or they live at what to us would be freezing or boiling temperatures so the planet was skipped for the same reason. We have snails that live near magma with iron shells on this planet.

Or any number of other reasons why their life would be totally Incompatible with any single factor of our planet.
Earth is an outlier. 

Check this chart of known exoplanets. Were' the far right dot in the yellow region. 

[http://cdn.sci-news.com/images/enlarge3/image\_4968\_3e-Kepler-Catalog.jpg](http://cdn.sci-news.com/images/enlarge3/image_4968_3e-Kepler-Catalog.jpg)

Pretty unique...
We only first sent out messages in the 1930s and 40s with radio waves. Those waves take a long time to get where they're going. Then however long that was, assuming those aliens left that same day, would take that same time to get to us. 

Space is huge. We realistically won't be found for likely thousands of years. Space is just too big. I firmly believe aliens are out there somewhere, but they probably haven't noticed were alive. Even if they did, it takes thousands of years to travel here.
Your question makes several assumptions: 1) Time remains linear for those with technology advanced enough to travel the universe, which I doubt.  2)  you seem certain our planet hasn't been visited at least once in a billion years, which I doubt.  They may have stopped by long before humans evolved to a point where we could comprehend their presence.
Simple life is probably common in the universe, complex life probably not, and intelligent life probably rare. Also, when you consider that life arose on earth pretty much as soon as it was physically possible (the planet was no longer a big UV irradiated lava ball), and that earth formed pretty much as soon as it was physically possible for planets to form - well, we might be the first intelligent species.

There’s a really good episode of radio lab where they discuss the mitochondria as the key enabler of complex life. Mitochondria used to be free living organisms that were taken up by another cell and learned to live symbiotically. In the entire history of the earth, that uptake only happened once. One time. That’s crazy, and is a pretty good candidate for a “great filter” event.
One thing to consider is heavy eliments can take a long time to form in sufficient amounts. Not saying this is wrong bit just pointing out the raw age of the universe isn't the best starting point. Also their are a lot of other factors to a galaxy, solar system, planet being potentially habitable. But still we ought to have a lot of potential life forming planets, but we don't think we do. 🤷🏻‍♀️
The difficulty is in communication over the \*VAST\* distances involved.   The Universe could be absolutely \*TEEMING\* with life, but non-technological life would be largely undetectable to us except for the very closest star systems.

Similarly, even technological species would have a problem contacting us.  Right now, our "radio sphere" is only about 75 light years wide.  Most people quote it as being over 100 years, since the actual invention of radio, but almost all of the high powered radio transmissions until the invention of microwave radar were of a low enough frequency that they were reflected back down to Earth by the ionosphere.   

What that means is that we can't expect to "hear" from some intelligent, technological species that is greater than about 75 / 2 = 37.5 light years away, which is almost \*NOTHING\*.   As time goes on, that number will increase of course.   Some extraterrestrials with radio telescopes sensitive enough to detect us, but are 150 light years away, won't be able to hear us for another 75 years, and we can't expect a communication from them for 225 years.   

And of course, actual visiting is *probably* out of the question, at least for biological entities.  It's certainly possible to built machines that will last hundreds of years, but you're limited to traveling at a fraction of the speed of light.   

A lot of people really don't fundamentally understand the very real limitations on what is actually possible, given the mind-blowingly \*VAST\* distances involved.   So far as we know, the only methods of theoretically going fast enough where time dilation becomes significant require "unobtanium".   There are methods we know of that could get us to 5 or 10% the speed of light using current technology and a bit of engineering, but that's basically only useful for unmanned probes.  At that speed, it would take 42.5 years to get to Proxima Centauri, the closest star to the Sun, and that's for a fast flyby.  If you want to slow down and study the system, it will take you 85 years to get there.

And no, you're not going to go faster than light, not if you have mass, unless you actually break the Universe.
[removed]
[removed]
You just described the Fermi Paradox. 

No one can give you a straight answer. We have no idea why the universe appears so *empty.*

We simply don’t. There are lots of hypotheses. None of them can be tested. 

They range from life being super-duper rare to an alpha civilization destroying all competition once it’s detected.
Maybe they're more intelligent than us. Maybe we're like ants or apes to them.

Also we've only been listening for 100ish years. That's a blink in the whole of 4.5 billion years of Earth's history, let alone humanity's history.
13.8 billion years, you're quite off on 14.5 (where'd you get this number?).

And the answer to your question is quite simple. Space and time are too vast for any two civilizations to typically meet before they self destruct for one reason or another. They always do, eventually. And even if they last thousands of years, physics has limits.

The best we'll likely ever do is listen in on another civilization's past radio signals, if we're lucky.

It's really that simple, most likely. Sad, but that's the prevailing base idea.
[removed]
we’ve been alive for barely a fraction of that time the possibility we’d been contacted especially in the past maybe 500 years where it could have been recorded without it being misinterpreted. is so small it’s very improbable
You would think...  though if you were a species that mastered interstellar travel you'd likely be a relatively peaceful species.   (Or you'd have destroyed yourself with wars and such)   if you came across our planet in your travels would you want to contact us considering we have thousands of nukes pointed at each other?  And even assuming you did want to,  who do you contact?
As an addendum to the information already provided here:

Its not just visible light that'll produce a tangible thrust.  Any wavelength of light will.   This becomes a problem for space probes, because electronics and power supplies turning on and off create heat in the infrared spectrum, and these infrared photons cause a small thrust which over long periods of time will cause the probe to veer off course.

There's actual computer simulation and modeling done at NASA to account for this infrared-thrust effect when setting probe trajectories and course corrections.
Yes, very slowly.

Light has momentum, even though it is massless, so if you shoot a beam of light in one direction, conservation of momentum will push you in the opposite direction.

A reasonably powerful LED flashlight will use about 1-3 Watt, lets say 3 W. The efficiency of a LED is somewhere between 25% and 40%, so for sake of ease of computation lets make that 33% and we get a net amount of light output of 1 W. 

The ratio between the momentum and energy of light is 299,792,458 (Which is also the speed of light). So in 1 second, the flashlight produces 1 J worth of light, which is equal to 0.33 * 10^-8 kg m/s. If the flashlight is not too heavy, say 100 gram or 0.1 kg, that means that 1 second of light would propel the flashlight to a velocity of 10^-7 m/s. This assumes that all light is directed in straight line. The more cone-shaped the bundle of light is, the lower the momentum transfer is.

Leaving the light on for one day would propel the flashlight to about 0.009 m/s or almost 1 cm per second. Unfortunately, operating a 3 W LED for a day uses about 260 kJ of energy. Regular AA batteries have somewhere around 10 kJ of energy (depending on the type). And at a weight of 20-30 grams per battery, you can't carry put more than 2-3 in the device without violating our original assumption of a 100 gram device.
As other answers have said, Yes.

In terms of using this as actual spaceship propulsion (powered by solar panels), there is a slightly easier way:

https://en.wikipedia.org/wiki/Solar_sail

Which reflects the sun's light to generate (tiny amounts of) thrust!
There's a classic physics trick problem where you ask students to figure out the best way to gain momentum using only a flashlight or laser pointer while in space. The trick of the question is that it's better to simply throw the flashlight/laser pointer than to bother turning it on!
Aerospace Engineer here. In college we were presented a question during class on this very topic. Essentially, if you're an astronaut and get separated from your space craft and all you have is a flashlight - can you get back to the ship?

Long answer - If you turn on the flash light and point it in the direct-opposite direction of the space craft. Yes. But it is going to take a long time.

Short answer - If you throw the flashlight in the direct-opposite direction of the space craft. Yes. But you're a lot more likely to mess up your trajectory and miss the ship.
[deleted]
[deleted]
Can someone explain to me why the light would need to continue being on in order for the flashlight to keep moving? I thought when something gets propelled in space, because space is pretty much a vacuum, that it just keeps moving? Like if a rocket ship was up there, fired its engines, then turned it off, it would now move a constant velocity.

I know space isn't a perfect vacuum, but surely those few hydrogen atoms colliding with the flashlight would make negligible difference, unless there's another reason why the flashlight needs to keep its light on to keep moving?
Followup question. Given that sources of artificial light are not evenly distributed on the Earth (see: http://eoimages.gsfc.nasa.gov/images/imagerecords/55000/55167/earth_lights_lrg.jpg), are we propelling the Earth in some direction? Granted, the Earth is huge, and I imagine atmospheric scattering of light would dampen this effect, but is this theoretically happening? I would imagine that since most artificial light is used at night, if this were to be happening, we would be propelling ourself in the direction of the sun.
It has been shown that light can impart a momentum on an electron that absorbs it, but it has not been shown that an electron has a recoil when it changes state and emits a photon. 

So the object the flashlight would move if you exposed it to external light, but if it is just generating its own light it will not create a thrust.
Don't know much about physics, but why would the weight  of an object matter in space? Wouldn't any object be weightless? I would of thought that the surface area of the reflective/light producing object would be what mattered? Can a physicist please explain?
How do you measure movement in space? I was think about how you would test OPs question. How would you set a flashlight in space without already being in motion? Or because it is all theoretical anyway... it doesn't matter? 
[removed]
There are number systems which do just as you describe. Here are two (I don't know of others) such examples of this:

* https://en.wikipedia.org/wiki/Projectively_extended_real_line

* https://en.wikipedia.org/wiki/Riemann_sphere

The latter is the extension by defining z/0 in the complex plane.

A lot of the math rules are the same as you're used to, but there are important differences. For example in the projectively extended reals statements such as

* a > b

* a set of all numbers between -4 and 7 is [-4...-1...0...7]

are no longer meaningful without extra context. I can always pass through infinity to just as easily write

* a < b

* a set of all numbers between -4 and 7 is [-4...-10...infinity...7]

With some added assumptions of what "a" and "b" are and where infinity is on your interval if it's included, you can restore the idea of order and intervals.
There are many good answers already, but I think there's a simpler one that has to do with inverses.

An inverse is a sort of mathematical undo, it reverses the action of some function.

Instead of thinking of division and subtraction as operations, think of them as inverse multiplication and addition.

When you see `5 + x = 7` we can solve this using an inverse: `x = 5 + x + (-5) = 7 + (-5) = 2`. We can construct the integers from the naturals by *closing* the addition operation through the extension of negative numbers.

When we try the same thing with multiplication, we get the rational numbers. Given `x * 3 = 6` we can use the multiplicative inverse of 3, one third, to solve the equation: `x * 3 * (1/3) = x = 6 * (1/3) = 2`.

We cannot actually close the rationals under multiplication, because of zero. The closest thing to do is take the set of rational numbers except for zero and treat this as a multiplicative group.

Since zero times anything is zero, we have `0*x = 0`. Since `x = 5` is just as valid a solution as `x = -1`. Since no unique solution for `x` gets determined by the equation, there's no way to assign a consistent value to the multiplicative inverse of zero.

TL;DR: zero times anything is zero, so it's impossible to undo that multiplication and figure out what you started with.
When you define i such as i²=-1, you can just use it the same way as any real constant, and keep doing math.

If you define a constant to be equal to 1/0 and try to keep doing math with it, you will find contradictions at every corner. First quick example to come in mind: if we call it b (why not), then 1/b is, obviously 0, but what is 1/(b+1)? If it is zero, then b+1=b so 1=0. If it is not, then you just found a finite value for b.
The problem is that x*0=0 for all x so if you invert multiplication then x=0/0 also for all x. This contains no information. If you allow this you can "prove" things like 1=2 (All those wrong proofs contain a hidden division by zero)
Try it.  Let's say any number divided by 0 is some constant, c:

1 / 0 = c

now let's multiply both sides by some number, say 5...

5 * (1/0) = 5 * c
5 / 0 = 5 * c

since any number divided by zero = c, we have:

c = 5 * c

So c must equal zero.

But if we regard dividing by zero as a valid operation, we end up with things like this:

3 < 5
3/0 < 5/0
0 < 0

Basically it breaks all the other rules that we have declared and derived that form algebra as we know it, so we must specifically disallow it to make everything else work.
There are many good explanations here, my favorite most simple about why you can’t divide by 0 is this: 

What is 20/4? 5. What you’re really doing here is glorified subtraction.  20-4 is 16, then minus 4 again to get 12, and again for 8, and again to get 4, and one more time to get 0. You subtracted four 5 times until you get to 0. 

So then how about 20/0? Well.. 20 minus 0 is 20, so you do it again, and again, and never get anywhere. You can’t do it. 
There’s one case where mathematicians did exactly as you describe, and that’s inversion (geometry). 

Consider a space mapped in the unit circle (circle with radius r = 1) and then invert it by taking every point in the circle and placing it outside the circle at the same angle from the center, but at 1/r (since r < 1 for every point inside the circle). 

They defined the center of the circle’s inversion (1/0) as being equal to infinity (infinitely far away from the circle once inverted) for the sake of continuity.

Defining such an operation in general requires either a purpose or a logical justification for doing it, and as others have mentioned, there are math operations that you can use on the square root of -1 to get real-world practical results, unlike the dividing by zero case so far.
Firstly, I make the assumption that you want things to be consistent with Algebaric fields - that is, 1/0 obeys the rule of an albebaric field - e.g. rational, reals, complexes.

https://en.m.wikipedia.org/wiki/Field_(mathematics)

 0 is normally the additive identity of a field. We show that 0x = 0 by that the distributive property,

0x = (0+0)x = 0x + 0x

Subtract 0x from both sides and we get that 0x = 0. Now if there is a multiplicative inverse of 0 - let's denote this as "Z". (I.e. Z = 1/0).

This means that 0Z = 1. But we just shown that 0Z = 0 by the previous result above. Hence Z cannot be in our field and we have to break the closure rule of fields (adding and multiplying elements in a field returns a result in the same field).

Note that this also applied to Algebaric rings as well. But, if we are going to sacrifice the field property we could extend the real or complex numbers to include infinity. 

https://en.m.wikipedia.org/wiki/Extended_real_number_line
TLDR:

1.  "i" is a number like any other. You can add, subtract, multiply, divide it etc. and always gives a single internally consistent answer.

2.  "i" represents real values in nature when you're doing certain physics calculations so it's necessary and important that we have it.

3.  5/0 has no answer because nothing, not even Infinity, can be multiplied by 0 to get it to equal anything but 0.

4.  We can rewrite math as we see fit, but there is nothing in nature to my knowledge that would require changing the rules to give 5 / 0 a proper answer.


Long Version:
There is a subtle but meaningful difference here between "i" and diving by 0.

On The Nature of "i":

"i" is just a number like any other. We use the moniker "imaginary" to describe it, but really "i" is not meaningfully different than 2 or 5 or 7.  This bakes our brains a little bit because we can't see "i" anywhere on the number line, nor can we hold up "i" numbers of fingers and toes.

But there are real actual things in nature that have a value of "i".  In physics, the equations / calculations in electricity and magnetism and / or signal processing often reveal physical quantities that contain "i".

Furthermore, because "i" is a number like any other, you can perform any and all mathematical operations on it ( addition, subtraction, multiplication, etc.) and you will always get a single answer which is internally consistent with the rest of mathematics.


Dividing By Zero:

5 / 0 is basically the mathematical equivalent of asking the question,

"What number must 0 be multiplied by in order to equal 5"

But this question has no meaningful answer.  No number, not even the big boy himself, infinity, can budge Zero from its position even a little (infinity X 0 = 0).  So asking how many zeros it would take to equal the number 5 is just a nonsense question.  It would be like asking,

"How many Rocky Road ice cream cones does your uncle Charles have to eat in order to grow Santa Claus out of a moon rock."

There simply is no answer.  The equation (5/0) itself  contains the false premise that this particular denominator (0)  could ever be made to equal 5 through multiplication alone.  So I guess the super duper snarky answer to this equation might be, 

"5 / 0 = You made a flawed question.  Try making a better question."

Can't We Re-write Math?:

Yes actually we can. We just made up all the rules to math anyway.  We could technically write them to say anything we want (5 / 0 = Thanos is Tony Stark's son).

There is actually an entire field of mathematics called "non-euclidean geometry" that is basically based on one person's desire to create a new form of geometry from scratch where two parallel lines would eventually cross one another instead of staying the same distance apart forever.

But the reason why non-euclidean geometry has value / staying power is that it turns out that on a curved surface like a globe, two parallel lines do actually cross each other if you take them out far enough (If you and your buddy are both standing on the equator facing north and you start walking, your paths are 100% perfectly parallel to start with, but you will bump into each other at the North Pole).

Similarly, as we discussed previously, in certain equations like the ones in E&M physics and signal processing there are actual real physical quantites whose calculation requires the existence of "i".

And I'm a physicist not a mathematician, so I definitely can't speak on behalf of the math world.  But as far as I'm aware there are not any real calculations that by their nature would require there to be an answer to the question 5 / 0.

So I would think it unlikely that a brand new branch of mathematics would be created for the purpose of giving the question 5 / 0 an answer.
[removed]
In math, anything can hold true if you assume it. So let's just assume that for two real numbers, a and b, there exists an equality a/0 = b. Using simple algebra, we can then see that a*b = 0 for any two real numbers, thus making all real numbers indistinguishable from each other. So you can divide by zero, in a system where every number equals every other number. Needless to say, this kind of mathematical system is not very useful and indeed not widely used in the mathematical community. 
[removed]
Because doing so doesn't give you a particularly useful structure. When extending the real numbers to the complex numbers by adjoining an element i, most of the properties of real numbers continue to hold because the complex numbers are still a field. However, if you try to define 1/0 as you describe, you lose several useful properties, for example that 0*x = 0 for all x.

There is a structure that's basically what you describe called a [Wheel](https://en.wikipedia.org/wiki/Wheel_theory), but the length of the article should give you an idea of how little it's used.

Mathematicians don't just define structures because they're possible, they define structures in order to talk about what useful and interesting properties and connections to other structures they have, but in the case of wheels that's not very many.
You can! But things break, and ultimately cause more trouble. In particular, let's look at what happens if we assume that we can add division by zero while still keeping our ability to add, subtract, and multiply.

Let's say we define 1/0=£. What this should mean is that £ is the value that you can multiply by 0 to get 1.

Consider 0\*£=(0+0)\*£=0\*£+0\*£; subtracting 0\*£, we get 0\*£=0.

Thus, 1=0\*£=0. And so in a world where we can add, subtract, and multiply, and also divide by 0, all numbers are equal to 0. Which, sure! In that world, all operations have the result 0.

Okay, so we ran into trouble because the normal rules of math would apply multiple values to 0\*£; let's look at what rules we lose if we specifically choose the value of 0\*£.

0\*£ has no value: this is a cop out! If the whole point was to stop having a thing (0) that we couldn't divide by, the solution shouldn't be having a thing (£) that we can't always multiply by. That said, this is the answer you get if you call 1/0 "infinity." You don't get to multiply 0 times infinity, or subtract infinity from infinity, or divide infinity by infinity. Which means we went from a world where a/0 wasn't defined, but now we have both 0/0 and infinity/infinity undefined.

0\*£=1: this means that our little proof above of 0\*£=0 can't work any longer. Now, it's entirely meaningless to have 0 such that 0+0 isn't zero, so that part of the proof is fine. And, if we're saying 0\*£=1, we should be able to subtract 1 from an equation that basically says 1=1+1. So, what broke? Distributivity. We no longer have (a+b)\*c=a\*c+b\*c. But it's even worse than that. 0=0\*1=0\*(0\*£)=(0\*0)\*£=0\*£=1, so we need to also ditch associativity of multiplication! At which point, without both associativity and distributivity, we no longer have an operation that really deserves to be called multiplication. Note that all of this also holds if we set 0\*£ equal to any other already-existing nonzero number.

0\*£=0: this means we aren't using the typical understanding of what dividing by a number should give you. But consider 1=0+1=0\*£+1=0/0+1/0=(0\*1+1\*0)/(0\*1)=0/0=0\*£=0. So again, we lose both distributivity and associativity.

0\*£=something new: at this point, if you're careful, you can define what I'm sure others have mentioned: a wheel. In a wheel, division is divorced from multiplication; we lose the fact that x/x=1. Further, subtraction is divorced from addition; x-x is no longer necessarily 0. So you get something, and it's not forced to have all elements equal to zero, and you still have associativity, but you do lose distributivity.
Math is a lot less about theories and crafting as opposed to discovery. Let's start with primes - they aren't considered primes because a mathematician decided they were - prime is an inherent trait of the number.

If you were herding 7 sheep and you wanted to split them into even groups you would only have 2 options - 1 group of 7 or 7 groups of 1.  The same holds true in all bases (binary - 111 groups of 1 or 1 group of 111). It also holds true for any alien-farmer who is discovering alien math on an alien planet - 7 is prime. 

The imaginary number i is more abstract and less grounded in the real world (hence why it's called an imaginary number), but it solves a basic problem of maths - namely, how do we take the square root of a negative number? i works both ways sqrt (-25)=5i because i^2 = -1

Let's try that with dividing by 0. Let's say 10/0=¥. Also,  15/0=¥, and 48/0=¥.

That means the reverse is true as well. ¥×0 = 10, and 15, and 48.  Suddenly we have ¥×0= (every conceivable number ever). That doesn't really help us solve anything, and it breaks the rule that anything×0=0 at the same time.
Let us think about what it means to divide a number. Let' say you have a,b,c defined as:

a/b=c

What else is that formula saying? It also says:

a=b*c

Right? So now replace b with 0 and n for a. Can you think of any number, besides 0, that can reached multiplying 0 times another number? That is to say, for any n besides 0, 

n=0*x

Has no solutions. Now you may wonder why I exclude 0 from n. Well thats because 0 times *anything* is 0. So for any number besides 0, there are no solutions to the formula, and for 0 there are infinite solutions.

Thats why dividing by 0 makes no sense
You can, and you can do it without "breaking any rules," it is just not interesting to do so.

To see what I mean, think about a number system where dividing by zero makes sense. Let x be any  number in this system, and set y = x/0. Then, by multiplication, x = 0*y, and since 0 times any number is 0, we see that x = 0. So, **the only number in our system is zero**, which is not a very interesting or useful system.

People shouldn't say thing like "dividing by zero is forbidden!", but should say instead something like, "since we want things to be interesting/useful, let's work on a system where zero has no inverse."
You certainly can make this change, but it requires some other changes as well.

Specifically, you need to resolve some challenges concerning the multiplicative identity property of R (real numbers.)

If you define 1/0, then what should a times a^-1 be when a is 0? The inverse property of R suggests that should be 1 if 1/0 is defined. However, the multiplication property of zero suggests it should be 0. In concrete terms, does 0 times 1/0 = 1 as the inverse property suggests, or should it be 0 as the multiplication of zero property suggests?

As other posters have mentioned, it's totally possible to build a number system in which division by zero is defined, but it changes the properties of R substantially. If we allowed division by 0 in R, R would behave in counter intuitive ways (with regards to how we typically use R.)

In more technical terms, while addition can be closed under R, multiplication--and by extension division--cannot be closed under R without significant modifications to some of the basic properties of R (because zero.) With regards to defining sqrt(-1) to be i, we can do so without changing the basic properties of R, only extending them. Defining division by zero is fundamentally different, as we've seen that doing so would change the basic behavior of R (would lose field properties.)
[removed]
Probably because dividing any number by 0 will not give the same "Undefined" for all real numbers. 0 \* x/0 should be x. So you cannot define x/0 as one constant where x is all real numbers. What could in theory be done is defining 1/0 as some number\(q\), and so 0 \* \(q \* x/0\) =  x.
The problem is that the limits as you approach dividing by zero are different depending on what limit you take. 1/x goes to plus infinity or minus infinity is x approaches 0 for the right or the left (and with complex numbers there are still other possibilities). So you can't just define it as one constant which plugs into another equations in certain ways, since +inf and -inf behave differently. 

And the square root of one actually has two values, notated as +i and -i, so it is also not actually uniquely defined.  
That's like declaring 1+1=3.

X/Y means "How many times can I subtract Y from X before getting to zero?

Thus 2/0 is asking how many times you can subtract 0 from 2.  You can subtract 0 from 2 and infinite number of times and never reach zero.

So declaring that dividing by zero results in a constant would be pretending your cat is a dog because you would have preferred a dog.  You are just pretending while ignoring reality.
Division by zero is undefined for two reasons, neither of which can be addressed the way sqrt(-1) can.

First, infinity is not a number. It is a limit. It just means "arbitrarily large".

Second, [the graph of C/x as x tends toward 0 diverges](https://s3-us-west-2.amazonaws.com/courses-images-archive-read-only/wp-content/uploads/sites/1227/2015/04/03010751/CNX_Precalc_Figure_03_07_0042.jpg). It's not a matter of a missing point.
Well you could. But from a math research perspective, the correct question to ask is usually not "Why don't we define this?" but rather "Why should we define this?". In math you are free to define as many new concepts as you want, but not all of them are equally interesting.  
  
Take as an example the complex numbers. We define i as the square root of -1, sure. But the reason the complex numbers are so well-known even to some people outside mathematics is not because people *really* wanted to solve the equation x^2 = -1 but because complex numbers allow us to do all kinds of nifty stuff like represent planar isometries as a combination of multiplication and addition by complex numbers.  
  
Of course, one can just answer the question "Why should we define this?" with "Let's just do it and see what happens". Well, as some other people pointed out in the comments if you do define "1/0" or "infinity" as an extension to the real numbers you don't get a system that is very interesting. Algebraically your extended system doesn't work out because as people liked to point out the normal rules of multiplication and division don't work anymore in a system with "1/0" added. Topologically if you add an infinity point to the real number line you get a circle (as another guy also mentioned), and we understand circles pretty well already from other areas of maths. So my answer to your question is "Yes you can, but it doesn't lead to any new interesting theory".
Are you asking about something like a Dirac Delta function?

https://en.wikipedia.org/wiki/Dirac_delta_function

It has enough information to capture the original division by zero - the direction of the function gives you the sign and the 'weight' gives you the numerator. 
Its not that dividing by zero is undefined and therefore all we need to do is “define” it. The problem is that the division operation does not allow division by zero. 

If a/0 = b
B * 0 = a

This is impossible because anything multiplied by 0 is 0. 

Besides the undefined nature of zero is quite useful. We already know that the limit as x approaches 0 of a/x is infinity.
Simply because that is not how math works. You can’t make up a constant and expect it to have a real use. You have to have a proof that it will hold true

For example, any integer to the power of 0 = 1. No one just made that up and started using it in calculations. They had to make a proof to know that any integer to the power of 0 does in fact = 1. 

Same thing goes for square root of -1 = i.
The problem is because division is a compressed way of subtracting a number from another number multiple times.  


So, when you take 100 divided by ten, the arithmetic function you are performing is to see how many times you can subtract 10 from 100. So, 100\-10 = 90\-10 = 80.... 10\-10=0. Arithmetically speaking, you would have to subtract 10 from 100 10 times in order to get to zero.  


Now, lets try and divide by zero using that mathematical sequence. 100 divided by zero. Now, you get 100\-0 = 100\-0=100\-0=100..... At no point would you reach zero, so it is therefore undefined.  


The next question you might posit is why can't it be stated that anything divided by zero is infinity. At first, it might seem to make sense to identify it as such, but you face another problem if you do so; do you assign "any number divided by zero" as "positive infinity" or "negative infinity"? It would be non\-sensible for it to be either one, so it cannot be anything but an undefined and non\-arithmetic construct and therefore unable to remain in any arithmetic formula.
This is because 0 can fit into any greater number buy an uncountable infinity therefore it is placed with “undefined” as there is no answer but it is technically already a constant. I do agree there should be constant for that but it would be a lot of hassle and kind off useless 
It's a result of the axioms of the real numbers. The reals are a field, and the 0 element of this field is the one such that a + 0 = a for any a. The idea of "division" is simply the multiplicative inverse. Basically, it means that c is the multiplicative inverse of d if cd=1 (and we take this to mean c = 1/d). So, we can essentially combine these two results to find the multiplicative inverse of 0. If we call this element "e", then we have. 0e = 1. In the context of a field, there is no such e that will make that statement true. So, we cannot divide by 0.
So, there are a lot of answers here covering the mathematics, but I thought I'd weigh in on the computer side since you mentioned dividing by zero being a"problem," as it's often memed to be in software. And it kinda is.

I say *kinda* because the error getting memed is actually when an *integer* is divided by zero. Remember that integers are counting numbers - 1, 2, 3, etc - and computers have a simplified means of storing and acting on them as binary numbers. This is good for speed and expressing exact (integer) values, but it doesn't allow for *special* values.

I said it was *kind* of an issue because floating point types (decimal numbers like 1.333 or 2.25) do allow for special constant values like you're describing. In most programming languages..

1 / 0 = Positive Infinity
-1 / 0 = Negative Infinity
0 / 0 = NaN (short for Not a Number)

...but only for decimal numbers.

HTH
I had a computational theory professor tell my class a joke about mathematicians being able to explain their way out of anything.

When a mathematician was asked to capture a rhinoceros, the mathematician took a piece of chalk and drew a circle around his feet and said “everything outside this circle is a cage”

So yeah, it’s kinda strange there isn’t some clever/weird explanation for dividing by zero...
There is no value that you can assign to dividing by zero that is consistent. Suppose we say that dividing by zero equals X. Then any value, say like 3, generates an equation.

3/0 = X 

Then multiply both sides by 0.

3 = 0, if we assume X is finite. We really don’t want 0 and 3 to have the same value, and this weirdness holds for say 4,5,6, pi, i... just replace the 3 with a different value and you get something that we don’t want. So that is why we call it undefined.
the problem most people don't realize about dividing by zero is the grammar of math. you must understand that to say i divided by 0 is to say i took no actions. no math processes were performed when dividing a number or object a zero number of times. does this make sense? there is no theory to develop a math expression that means you did nothing and took no action
It's undefined because it's not possible. It's like a typo, it's just a mistake. Division has to do with splitting something into smaller groups. You can't split something into zero groups. Try it in the physical world. Try dividing your wardrobe into zero groups. It's not possible so just writing out 10/0 doesn't mean it's solvable. It's just something you wrote that can't be solved. It's the equivalent of trying to destroy matter.
[removed]
It's been a minute, but doesn't calculus allow you to divide by zero? I swear I remember my calc two class going over limits and finding of you divide by zero you get positive infinity, negative infinity, 0, 1, or DNE. Can anyone back this up, or am I crazy?
[removed]
As an engineering student, we just use limits to work around this. 

The limit of 1/x as x approaches 0 is infinity. Also, the limit of 1/x as x approaches infinity is 0. Very nice when trying to find the derivative of a point in an equation when using the definition of the derivative (Rather than using any shortcut rules).
[removed]
[removed]
[removed]
The real numbers form something called a field where we have, among other properties such as commutativity, associativity, and distribution, we have the operations of addition and multiplication.

As it turns out, if we introduce a formal element called x and allow it to operate with the rest of the real numbers, we also get a field that extends these real numbers. This is the field of rational expressions (e.g. (x^(3)+1)/2x) which include the real numbers (expressions not involving x).

If we include the relation x^(2)=-1, i.e. whenever we see x^(2) in an expression, we replace it with -1, it turns out the result is *also* a field. Now, if we call this x by *i*, the resulting field is the same as the familiar complex numbers.

You might then ask if we can extend the real numbers similarly into a field where division by zero makes sense, maybe using x=1/0. Well, no we can't. Such an x will not behave in the new structure as *i* did in the resulting field.

This is not to say that you *can't* define division by zero. In fact, in some applications, doing so will be useful. However, when you do so in many contexts, such as with fields, you will have to give up some property. You can maintain these properties for most of the field elements, but adding the division by zero will give you something that acts as an exception to many of your rules, which may or may not be desirable and often enough it isn't.
Yes! There's a lot of research going into tinnitus. The DOD funds a good portion of it (turns out loud explosions and artillery are destroying the hearing of our active troops and vets.)

I'm sure there are many groups working on this, but I know the Tzounopoulos group at the University of Pittsburgh is actively working on a drug to cure/prevent tinnitus. Not my area of expertise so I don't know the specifics of the project, but here is a link to his website. Some of his papers may be interesting to OP and anyone else looking for info. 



http://phrc.pitt.edu/people/thanos-tzounopoulos
[removed]
Tinnitus is often associated with noise-induced hearing loss, which is in turn caused by damage and death of hair cells in the cochlea. These cells are responsible for sensing vibrations and translating them to nerve signals to be sent to the brain. Once damaged or destroyed they cannot regenerate naturally. However several methods of inducing hair cell regeneration are currently under investigation, see [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5534485/).

EDIT: As you can tell from my flair I am not an expert on this topic and I've only done some cursory reading because I have mild tinnitus myself, I won't be able to answer any questions that I'm not familiar with.
Yes! Gene therapy is being worked on by multiple research firms across the globe. The most notable being Frequency Therapeutics who have a drug currently in phase 1/2 of testing. It's called FX-322 and it's an injection into the eardrum. It works by regenerating the hair cells responsible for the conditions in the first place. If things go well, the drug could possibly be 5-6 years away from release. Research for hearing loss and tinnitus has picked up a lot momentum in the past couple of years due to increased funding from multiple companies and governments. The reason that it is so late is because until now, the mechanisms behind the ear and the brain were not understood very well. Now that research has caught up, multiple solutions are being worked on. 
I am not a doctor or scientist but have done extensive research, participated in Tinnitus groups, and had multiple well educated Audiologists and Otolaryngologists and was very fortunate to have them. These were people who attended conferences and were always staying ahead of the curve in understanding the research and trials available, in the pipeline, and being discussed.

Many doctors have also said the same thing and it seems to be consensus - without having tinnitus it is difficult to assess from the outside, and there have been many audiologists I have seen in other cities who I have had the opportunity to educate based on my experience. There is a limited number of people worldwide who are truly invested in this condition.

​

**What is it?**

Tinnitus (tin-ih-tus) is a catchall used to describe hearing noise when there is no external source present. Common understanding always brings this discussion to two of the most basic causes - trauma or long term degradation.

Trauma can be a single external event, such as a gun firing, explosion, a crash, but could even be smaller things too. There is some consensus (but it is not a silver bullet) that a shot of prednisone into the ear OR a regimen of oral prednisone taken within 48 hours (24 hours or earlier preferably) can limit the damage of a severe auditory trauma.

Long term degradation is similar to trauma, but is the result of noises less than severe auditory trauma over the course of a lifetime.

​

These are the best understood because they affect the most people - those who have had military service, and the elderly. These cases affect the vast majority of tinnitus users, but the severity of the tinnitus in a good portion of these is manageable and most likely monotone in nature. The movies you may have seen depict this kind of tinnitus very well, and you may have even experienced it yourself after a night out at a concert, bar etc only for it to slowly die down over the next few days.

​

As others here have stated, there is work here that focuses on the idea that hair cell regeneration may be a positive factor in repairing the damage.

\---------

**What is it** ***really?***

Now to the more extensive understanding. Tinnitus is really a neurological issue, despite the cause. I know many are hopeful for the ear hair cell regeneration but this is not as promising to tinnitus in general as it seems.

​

There are many other causes for Tinnitus - a side effect of medication, infection (not just ear, but viral infections in the brain), acoustic neuroma, Meniere's disease, blood vessel issues, muscular issues, TMJ, and others!

Due to tinnitus having many causes there are some treatments which affect others and which do not work for most. There is even a type of tinnitus which is objective and the doctor could actually hear the noise, this is from blood vessels and is called pulsating tinnitus.

​

Since tinnitus is so broad and there are many underlying factors, it is incredibly difficult to 'cure' as you would only be treating some specific types of tinnitus. The type of noise people hear, the frequency, severity, number of tones/pitches, the nature of the noise, etc are all very very different although the name tinnitus is used to describe all of these. This means that it is very hard to create an accurate sample of people to run tests on, as the description and type of tinnitus may be very unique for people, and is also dependent on subjective evaluation by the subjects themselves.

The nature of tinnitus is normally that it is not constant on every day and can change in severity, volume, frequency etc on a daily or hourly basis. Tracking progress on treatment can be hard due to this, and there are a lot of factors that *could* impact tinnitus such as stress (#1), diet, sleep, external factors like noise, and others.

​

**The good news**

The good news is there are promising steps being made to treat the neurological causes and factors of tinnitus. To understand research both past and present, a tremendous resource is [https://www.tinnitustalk.com/forums/research-news.4/](https://www.tinnitustalk.com/forums/research-news.4/) which is a collection of tinnitus sufferers who have had it for years or decades and stay very active in the community and do a ton of research as well as share experiences if they participate in trials.

​

There have been many medications developed, all of which have failed at the second trial round usually, which attempt to address potassium channels that can lead to the reduction or elimination of tinnitus. There is some promise here but the majority of these medications treat another primary cause like seizures and the added effect on tinnitus is secondary which makes trials for those with tinnitus ineligible. There have also been many trials with different medications which you can clearly see on that TinnitusTalk research board.

​

Other attempts at solutions have involved treatments to the brain with magnetic or electrical stimulation to address the neuroplasticity of the brain. Many of these are new and again will never be a silver bullet but could be a general use to assist those who are eligible due to the potential for reduction or elimination. rTMS or repetitive transcranial magnetic stimulation was a hot button for a while, and there are other treatments that are always in the pipeline but require much time and are essentially unattainable for the near future.

​

**The even better news**

​

Research here is growing exponentially, and we understand more every day.

​

**The bad news**

There is a massive lack of funding for research here though and also a very low potential for ROI for pharmaceutical companies due to potential medications not having enough people that the cost of research and manufacturing would far outweigh those who could be positively affected and need prescriptions. A good few promising companies have gone under and hold on to all their research and it has not been shared with the scientific community, halting a lot of progress in a potential search for treatment.

​

**Resources**

​

If you're interested, [tinnitustalk.com](https://tinnitustalk.com) is a fantastic resource which contains a lot of very knowledgeable individuals. Be careful as there are tons of differing opinions and take no one person's words as fact, **INCLUDING MINE!** Also, us tinnitians (as a lovely facebook group member called us) can be a very very angry bunch due to living in eternal hell day in and day out so there is some petty bickering that happens here.

The American Tinnitus Association [www.ata.org](https://www.ata.org) is a GREAT resource, and **PLEASE if you are shopping Amazon for Christmas try using the Smile program to donate to them!** The British Tinnitus Association is also at the front of the global community too.

​

​

**TL;DR**

Nope we're in trouble. There are tons of programs in existence and being developed that work to manage tinnitus, but there is no 'cure' in sight, and there may never actually be one! Too many causes and difficulty in getting reliable data along with poor ROI for companies discourages research.
[removed]
[removed]
[removed]
[removed]
There is currently a [pilot study](https://clinicaltrials.ucsf.edu/trial/NCT01988688)  being done at UCSF using deep brain stimulation through implanted electronics to modulate tinnitus.  This is based on a few [case studies](https://www.ncbi.nlm.nih.gov/pubmed/20541595/)  of Parkinsons patients reporting changes in their tinnitus during DBS implantation.


Currently available technology includes hearing aids and similar devices which play various types of broadband noise into the ears, either to [mask](https://www.oticon.global/hearing-aid-users/hearing-loss/tinnitus/tinnitus-treatment)  the tinnitus or to [habituate](http://neuromonics.com/)  the central nervous system and reduce neural firing along the auditory pathway. As you might expect, YMMV with any of these devices and they’re not cheap. 
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
Well the reason the temperature in the past was so much higher is that the high CO2 levels persisted for many thousands of years allowing for the progressive accumulation of water vapor in the atmosphere which accelerated the greenhouse effect. CO2 is actually the second most important greenhouse gas behind water vapor, though it is often the trigger for climatic change because it's concentration in the atmosphere is controlled by a variety of complex processes where as water vapor is effectively controlled by temperature and pressure. Over time the increased temperature has a positive feedback with water in the atmosphere allowing for very elevated temperatures. It's worth noting that when we say the "the last time" we are taking a snapshot of a world that had been experiencing those conditions for millenia.

We currently believe we can keep the warming around 2C because we are projecting mitigation and emission reduction strategies that will eventually slow the warming trend. In the short term (geologically speaking) that means a temperature rise of around 2C.
I think a lot of the concern about methane stems from the potential positive feedback loop that exists in polar regions. There's a ton of methane locked up in the 10's of millions of square kilometres of polar permafrost regions, and if these regions melt under climate change this adds to methane emissions, which adds to GHG emissions, which leads to more warming, which leads to more permafrost melting, etc. 
This is a good question. First, a little background. So the last time atmospheric CO2 levels were as high as they are today (~ 408 ppm) was about 15 mya (some say more, some say less). Regardless of the exact date, Earth's climate system was considerably different back then than it is today, and so were all of its components - climactic feedback mechanisms, ocean and atmospheric circulation patterns, vegetation coverage, ozone levels, ice cover, concentrations of other GHGs, etc. All of these components contribute (either directly or indirectly) to global temperature. Therefore, although it is important to understand paleotemperature fluctuations and the climactic factors that caused them, comparing our current climactic situation with the past in regards to CO2 concentrations alone isn't necessarily relevant to your question; you have to consider current state of all the system's components. Scientists believe (with high confidence) that the temperature change resulting from GHG emissions can be kept to less than 2°C relative to pre-industrial levels IF atmospheric concentrations do not exceed 450 ppm CO2eq by 2100. These numbers take into account all the knowledge we have about the current state of the Earth's climate system's components. 
One thing that I've not seen mentioned so far is the importance of the oceans in regulating temperature and CO2. 

The oceans are one of the most important components of the carbon cycle since they interact with the atmosphere and can store large amounts of carbon at great depths where it doesn't influence planetary temperature. 

Our oceans are also vital in terms of temperature regulation. Alongside the atmosphere, the oceans redistribute heat from the equator to higher latitudes. At these high latitudes waters become cold and dense and so they sink, forming what we call deep waters. The 'ocean conveyor belt' (or meridional overturning circulation) is really important in maintaining a climate that is in equilibrium. If we stopped emitting greenhouse gases today then in a very simplistic model we might expect that over a few thousands years deep water would carry a large amount of carbon dioxide down into the deep ocean and atmospheric CO2 would reduce to pre-indutrial levels.

On the other hand, if the formation of deep water is reduced, via factors like less dense waters in the North Atlantic (as Greenland melts) and changed in wind patterns in the Southern Ocean, then it means less carbon can be taken from the atmosphere in the long term and temperatures could increase dramatically. Right now we think there were times in the past where overall deepwater formation was slow and that is associated with periods of long-term warm temperatures.
Well we pretty much can't. The best estimates show that there is less than 5% chance of us keeping the warming to below 2C variance. There is still a slight chance, but because there are so many people who aren't bothered by it, we will not be able to do it.

The short answer is that we don't, at least not all of us.  I certainly don't. 2C is incredibly ambitious at this point, so I'm actually with you.  

However, the reasoning behind it is that CO2 to temperature isn't as simple as a 1 to 1 relationship where a CO2 level of X leads to a temperature of Y.  There are many factors in play, of which CO2 is just one.  More of it will always lead to a temperature increase, but the end value depends on a lot more, stuff like how much ice exists near the poles, solar energy, all kinds of stuff.  

So in a previous "configuration", this CO2 level will have led to warmer temperatures at times, and probably colder temperatures at times. But in this configuration, this is what we get at this level of CO2. 
Baked into some of the assumptions is that we're going to achieve negative emissions in some way. [The Economist just ran an article about it a month ago or so.](https://www.economist.com/news/leaders/21731397-stopping-flow-carbon-dioxide-atmosphere-not-enough-it-has-be-sucked-out?zid=313&ah=fe2aac0b11adef572d67aed9273b6e55)
As far as I know nobody seriously expects it to stay under 2C. Many researchers said that point of no return passed years ago. 

Politicians do not live in reality however. They have decided to stay under 2C. The fact that they aren’t doing much in that effort and the fact that it is already impossible doesn’t seem to bother them. After all, they have decided and so it shall be. 
Global Warming is ultimately a rate of change problem.

The earth has been on an existing warming trend since the little ice age ended in the 1600s (a period of significant cooling from ~1200 AD).

The issue with anthropomorphic climate change is that we're accelerating this warming trend into what could be dangerous territory.

What the ultimate equilibrium temperature is for a given amount of atmospheric C02 is an unknown, currently the IPCC estimate for climate sensitivity is 1-6 degrees C per doubling of atmospheric C02. That range hasn't gotten more precise since the 70s.

Behind the reductionist headlines what you're seeing is a projection through a date, IE keeping the warming **by 2100** to under 2 degrees C which does not mean that the rate of warmth will slow.

As to how accurate those projections are? This far they've overestimated the effect of C02 on the actual rate of change significantly, but referring back to that 1-6 degree range that may just be further down the road.
the 2 degrees celcius number is a goal to curb emissions worldwide. This actual likelihood that we achieve the goal is very low. Furthermore, the number wasn't chosen by scientists, it was chosen by policy makers, arbitrarily. 
The short version is that they don't, and that the there isnt a 2C goal it's instead "2 degrees of warming by the end of the Century."  
However the 2 C goal is very likely already broken.  
It's only in the IPCC's most optimistic modeling scenario that the 2 degrees of warming by the end of the Century is possible anyway.  
The realistic scenario is 4 degrees by end of century, and the pessimistic is 6 degrees, which is more in line with your napkin math.  
See: [IPCC report see page XXII figure 8](https://www.ipcc.ch/ipccreports/far/wg_I/ipcc_far_wg_I_spm.pdf)
Also, there are some who believe we can't - there is a tipping point from feedback carbon effects that we won't know is there until we are well past it. Even if we can't, we should still fight to reduce carbon emissions, because it'd take everything we had to survive larger changes in the climate.
There’s a lag time because the oceans prevent extremes from happening. When the rise is over thousands of years the oceans keep up. When it’s over a hundred years the oceans are still absorbing. Just like if we rapidly reduce the oceans will keep the temp higher for a time as well. 
[removed]
Also, why are they predicting only small amounts of sea rise, less than a meter, by the end of the century when we can see huge amounts of water coming off Greenland and Antarctica. Greenland alone could raise things by 7 meters, and although i'm not saying it's all going to melt this century, it's certainly not going to stay at the same ice levels. It's kinda confusing when we hear about how fast things are melting but then are told that sea levels will only rise a little
The goal can be actualized if the world’s annual CO2 emissions - which is about 40 GT of CO2 as of 2015 - are decreased sharply within 20-25 years. 

From the Global Carbon Project 
“ For a >66% chance to keep global average temperatures below 2 degrees Celsius above pre-industrial levels, society can emit 2900 billion tonnes of CO2 from 1870 or about 800 GT of CO2 from 2017”

Moving forward, there will have to be a 4% decrease in global CO2 emissions each year until 2037 which would then levelize around a cumulative CO2 emissions around 3,000 GT of CO2
 

This can be achieved by three primary means....

-Mitigation (e.g. switching from coal to solar/wind/geothermal)
-Adaptation (e.g. creating urban boundary zones, river management, etc.)
-Negative Emissions Technology aka Geo-engineering (e.g. solar radiation management, CO2 capture, afforestation, etc.)

Hope this helps!
Because CO2 concentrations are only one factor of an extremely complex system that includes our position relative to the Sun and it's activities, which have changed significantly in the last several thousand years. Weather patterns, vegetative cover levels, water salination levels, and thousands of other factors are significantly changed since the last time we believe CO2 concentrations were at this level (all of the data that indicates the temperature in the last period of high CO2 concentration has a significant error margin, plus or minus several degrees C). It's like asking "we have the same number of graduates from Harvard on our team as we did 30 years ago. Why isn't our production the same?"
There is not a direct correlation between CO2 and temps. Triassic period was around 10 degrees C higher than now and CO2 was 210 ppmV (similar CO2 density as today but earth temps are about 8 degrees C lower right now). Whereas at the end of the Ordovician period CO2 rose to 2,240 ppmV and earth fell into a major ice age. Just saying you can’t look at CO2 alone to predict temps. 
[removed]
[removed]
The 2C goals outlined by scientists is reflective of the scale at which CO2 production would have to be reduced in order to have any positive affect on the environment. Consider that, the 3-6C change that we can see in trends is a culmination of years and years of industrial pollution and lack of control technologies. The 2C goal acts as a reference point for the scientific community to understand the amount of CO2 reduction that would be needed to reach such a goal. And unfortunately, even this goal is considered way too optimistic every energy analysis (BP Energy Outlook, IEA annual reports). The 2011 IEA report stated that in 5 years time climate change would be irreversible, and we're starting to see the consequences of the negligence towards these statements now
The last time atmospheric CO2 was this high, a great many things were different, most of which we have no idea about. In fact, almost everything we do know about climate in that era is through measuring isotope ratios. Great science, but just think how many critical things about the current environment would not be picked up studying isotope ratios. 

More to the point is the recent science pertaining to CO2 sensitivity, which is really the main determinate of how much warming will happen. Current best estimates are around 1.1 to 1.6 degrees C, much lower than the 2.3 degrees implied in the IPCC's projections. On that basis, it is highly unlikely the 2 degree "threshold" will be under threat this century, or next.
I'm not a denier but that doesn't make sense to me can someone explain to me how we can be at the same levels of co2 but -6 degrees less if co2 is the sole contributer?
I don't understand how methane is a contributor to warming. It's a reactive gas with oxygen turning into C02 and H2O
it burns readily in a stove so it must react with 02 in the air even if slowly ....right?
There are quite a few potential reasons. Aerosol forcing (from coal plants in SE Asia primarily) have actually artificially reduced warming, which wouldn’t have been around during the Euocene. 

Warming would have taken much longer than we see today, allowing more time for feedbacks to take effect. We have yet to see feedbacks with “memory” catch up to the amount of GHG we’re pumping into the troposphere. 
Apes and old-world monkeys have ABO blood types like us. 

Other animals have blood types, but not the same ones as us. There are thirteen different dog blood types, but cats are categorised as A, B, or AB. Not the same as human A and B. 
[deleted]
Does anyone know - was there a time where the scientific community legitimately thought blood transfusion was impossible? I can't imagine it was easy figuring out blood type before certain technological advances.
The short answer is yes, as you can see. 
I want to address the broader question here that many commenters are sort of missing. How is blood in itself different between species. 
Most of the issues when you think about "typing" is a consequence of learning about the antibodies present on red blood cells. About every cell in your body has two little towers, MHC complexes. These function to let your immune system function, part of their job though involves with the cell nucleus, which rbc do not have. Blood does NOT have these complexes either! Instead there are a variety of very complex "factors" and other functional proteins, carbohydrates and such. This is really important because it is those factors that lead to coagulation upon transmission into a new individual. They clump to other blood cells and clog our veins/arteries interestingly enough for dogs and cats (maybe others I am not sure), most can get a transfusion the first time and be fine. Now this is certainly not best practice but it goes to show the difference in the coagulation cascade between mammals.  
Other animals have an incredible amount of diversity for example:

1. Bovines have 80 blood factors and 11 groups
2. Porcines have 16 groups
3. Sheep have 22 factors and 7 groups
4. Equines have 34 factors and 7 groups

In humans you really would not have a scenario were say an individual mates with another one and their baby dies, but in some animals this can happen and it is called neonatal isoerythrolysis and probably a result of domestication and inbreeding. It is more complicated than simple "typing" and has alot to do with the antigens present such as Mik antigen in cats, and unfortunately the prevalence for this is unknown. This is why vets should ALWAYS CROSSMATCH, because typing alone does not really tell the whole story. Unfortunately, most clinicians have limited experience or technical capabilities to do this and opt to type or Ive seen many times no typing be done and infusions given if that is the animals first one.   

Further than that it is important to remember we are talking about mammals. Avian and reptile species have nucleated blood cells which is another whole game. Mammals decided a long time ago to shoot out our nucleus and have increased oxygen carrying as a benefit. bird and reptile haematology is incredibly complicated and honestly I am not sure I have really heard of transfusions being common. Ejection of the nucleus is the rate limiting step for pre rbcs to become erythrocytes (rbcs) and this is not needed in reptiles and birds. So to an extent they have a faster maturation time for blood. Therefore, it has been less common to try transfusions and little research has been done.  We really do not know a lot about other (exotic) species in general and hematology is a complex field to study. Most of the work is going to happen in a zoo, university, or the field. Whereas the common pet/livestock species are more likely candidates  for this research. For every 100 research project at a school I'd say maybe 10 are exotic focused, but this depends upon the school. 



Edit: I shouldn't talk about humans I suppose, it is possible for humans to have a similar condition with their offspring but different mechanisms are involved. 
[removed]
Cat breeder here and experienced with non domestic species. Cats have blood groups A, B and more rarely AB. This is very important information for breeders as blood type B cats produce anti A antibodies in colostrum. This means that a blood type B queen (female) bred to a blood type A stud can result in roughly a 50 / 50 split in A and B blood types in the litter. If allowed to suckle in the first 18 to 25 hours of birth the blood type A kittens can lose toes, tips of the tail and eventually die. While some breeds such as the oriental are almost always A, other breeds like the British Shorthair have a very high number of B blood type cats. So testing all breeding cats for blood type is essential. Occasionally breeders may decide the mating of an A stud to B girl is worthwhile, however the breeder must hand rear the kittens for the first 24 hours to prevent ingestion of the high antibody colostrum. A type A queen can nurse both A and B kittens. Blood type AB is separate from the other groups entirely and can nurse either however it is the rarest blood group. 
Someone tagged my post about the lack of genetic diversity in cheetahs as not science.  So here's a bit more science about the lack of genetic diversity in cheetahs:

They have extremely poor genetic diversity and all share the same blood type.

http://www.nytimes.com/1983/07/24/us/rare-genetic-uniformity-found-in-cheetahs.html

https://cheetah.org/about-the-cheetah/genetic-diversity/

The rare benefit is, most cheetahs could donate blood or even organs (and really blood is just another organ) to most other cheetahs.  The genetic bottleneck occurred ~12,000 years ago and the cheetahs scraped by with a tiny sequestered population that beat the odds and repopulated.

Their poor genetic diversity makes them at greater risk to become extinct from a single viral event, since they all share the same immune system.
[removed]
Fun fact, when you say A+ or A-, we're (unknowingly) referring to the presence or absence of something called the ['Rhesus Factor' or 'Rh Factor'](https://en.wikipedia.org/wiki/Rh_blood_group_system#Rh_factor) on our Red Blood Cells. 

>An individual either has, or does not have, the "Rh factor" on the surface of their red blood cells. This term strictly refers only to the most immunogenic D antigen of the Rh blood group system, or the Rh− blood group system. The status is usually indicated by Rh positive (Rh+ does have the D antigen) or Rh negative (Rh− does not have the D antigen) suffix to the ABO blood type. However, other antigens of this blood group system are also clinically relevant. These antigens are listed separately (see below: Rh nomenclature). In contrast to the ABO blood group, immunization against Rh can generally only occur through blood transfusion or placental exposure during pregnancy in women.

Now to the cool part, if you haven't guessed already, the Rh factor was first observed in the blood of the Rhesus Monkey!

>The rhesus blood type named after the rhesus monkey was first discovered in 1937 by Karl Landsteiner and Alexander S. Wiener. The significance of the discovery was not immediately apparent and was only realized in 1940, after subsequent findings by Philip Levine and Rufus Stetson.[1] This serum that led to the discovery was produced by immunizing rabbits with red blood cells from a rhesus macaque. The antigen that induced this immunization was designated by them as Rh factor to indicate that rhesus blood had been used for the production of the serum.[2]

>In 1939, Phillip Levine and Rufus Stetson published in a first case report the clinical consequences of non-recognized Rh factor, hemolytic transfusion reaction and hemolytic disease of the newborn in its most severe form.[3] It was recognized that the serum of the reported woman agglutinated with red blood cells of about 80% of the people although the then known blood groups, in particular ABO were matched. No name was given to this agglutinin when described. In 1940, Karl Landsteiner and Alexander S. Wiener made the connection to their earlier discovery, reporting a serum that also reacted with about 85% of different human red blood cells.[4]

>Based on the serologic similarities Rh factor was later also used for antigens, and anti-Rh for antibodies, found in humans such as the previously described by Levine and Stetson. Although differences between these two sera were shown already in 1942 and clearly demonstrated in 1963, the already widely used term "Rh" was kept for the clinically described human antibodies which are different from the ones related to the rhesus monkey. This real factor found in rhesus macaque was classified in the Landsteiner-Wiener antigen system (antigen LW, antibody anti-LW) in honor of the discoverers. [5][6] It was recognized that the Rh factor was just one in a system of various antigens. Based on different models of genetic inheritance, two different terminologies were developed; both of them are still in use.

Just as others said, yes, primates do have the same blood types as us humans. A lot of research hinges on this, I believe. Just ask Mr. Rhesus over there!
[removed]
They do. Other primates have similar blood types, but not all are exactly the same. It continues to branch out like that as you move farther from humans. Other mammals are more similar to us than reptiles for example. 


99% of the time the answer to a question comparing humans to other animals is "yes". Humans *are* animals, so already we know that some animals have blood types because we are an example of that. And like with blood types, the closer related we are to other animals the more similar our traits will be (on average). The exception to this is traits that aren't directly a result of DNA. So a question like "Do other animals walk on two feet" is more difficult to answer because that isn't directly determined by DNA, rather it is a result of our unique bone structure (which *is* determined by DNA and is similar to other animals).
While dogs have universal donors, cats can be A (most common), B, or AB. Source: the hundreds of blood transfusions I've performed in veterinary medicine. Can only speak on those for certain, but evolutionarily, cats are distant enough for me to conclude yes, there would have to be others.
Dogs have 7+ blood types
DEA1.1. 1.2 1.3 --> 1.7 plus some breed specific red blood cell antigens that can cause issues in other breeds.. i think Dalmatian and whippet (but not all members of these breeds have them)..

Dogs as a generalisation are not born with red blood cell antibodies so do not need to be blood typed prior to their first blood transfusion but typing and or  crossmatching should be done later in life for future transfusions as they may have developed antibodies from the initial transfusion ..

Cats have A B and AB.. cats are born with blood cell antibodies so if given the wrong blood type as a transfusion they can have a fatal reaction ..
I'm curious, can dogs or cats 'donate' blood like humans?  if so, is this done voluntarily and stored like human blood or ?

Okay I know I'm super late to this, but does anyone know how all of this ties into organ transplant? Specifically the transfer of a pig's heart valve into a human. I haven't seen anyone mention how pig blood works, which made me curious as humans can recieve heart valves (and maybe other organs?) from a pig
Dogs have either DEA - or DEA + blood. Positive donors can only give to positive recipients, but negative blood can be given to either type. Cats can be either type A or B, though B is very rare in the US. Cats must be crossmatched before every transfusion, though most dogs only need to be crossmatched the first time.


Object | Size
---|---
size of observable universe  |  8.8×10^26 meters
average height of human | 1.7 meters
Plank length |  1.6×10^-35 meters

_____________________

Universe / Human ratio = 5.16×10^26

Human / Planck length ratio = 1.1×10^35


result : Humans are closer to the size of the universe than the Planck length is to the size of a human


Definitely the latter; the comoving radius of the observable Universe is on the order of 10^(26) m, while the Planck length sits at about 10^(-35) m. The metre is closer to the cosmological scale than the Planck scale.  
[removed]
To put the planck length into perspective, a proton has the width of about 10^(-15)m and the planck length is about 10^(-34)m. So logarithmically there is a greater difference, 4 orders of magnitude, between the planck length and a proton than a proton and a human. 

Edit: there's a "c" in planck  
[removed]
This animation lets you go through the various scales easily:

http://scaleofuniverse.com

From the startup screen, you realize that 1m is about in the middle, one or 2 orders of magnitude towards the size of the observable universe. Still, the plank length is much, much smaller than the smallest things which we have experimental evidence of (neutrinos). So, we are pretty much at the middle of the scale.
/u/MercilessScorpion already linked this, but i think it deserves its own top-level comment:

www.htwins.net/scale2/ 

Interesting that compared to the obvservable universe and the smallest particles we know of, neutrinos, we are much closer to the middle:

Neutrinos are (estimated) to be as small as 10^-24m, and as stated above, the observable universe is about 8.8×10^26m.

so we're near the middle by about one order of magnitude!


We are closer to the observable universe, but not by much. I forget where I heard this, but I'm pretty sure that the half way point between the universe and Plank length is the size of a period on a piece of paper. 
What is your criterion for "closer"?

It is tempting to conflate exponents with averages. We are tempted to think that, for example, on a scale of 10^1 to 10^12, the "middle" is somewhere near 10^6 or 10^7. That because 6 and 7  are halfway between 1 and 12, the same applies to powers of 10.

This is a fallacy, because on the scale of 1 to one trillion, "halfway" is 500 billion, or ~ 10^11.698970004336019.

Wolfram Alpha computes the diameter of the observable universe (OU) to be approximately 5.4e61 Planck lengths. Given 162 cm as the average human height, this is equivalent to about 1e35 Planck lengths. This means 5.4e25 humans equals one OU. 

If you are just wanting to compare absolute size the OU dwarfs everything by several orders of magnitude; The 'jump' from 10^60 to 10^61 is much, much larger than the jump from 10^34 to 10^35, because ten 10^60's is equal to 100,000,000,000,000,000 10^34's. From this point of view, humans are much closer to the 'yoctoscopic scale' than to the 'cosmological scale'. The 10^25 that stretches between human scale and the cosmos has more 'weight' than the 10^35 that lies between humans and the Planck length.

But if you are looking purely at orders of magnitude and the change of scale that comes with rising exponents, then humans are closer to the 'cosmological scale' being 10^-25 th of a universe, than to the 'yoctoscopic scale' where Planck lengths are 10^-35 th of a human.
Taking into consider that the average male human is ~180 CM (1,8 M), or 1,8 * 10^0 M for this equation. But to simplify just to show how insanely large the difference is we will round everything into their closest 10^x point, which in a human's case is 10^0 M.  


A planch length would then be 10^-35 M while the observable universum would then be 10^26 M on it's radius.  


You can clearly just look at the exponents, which concludes that the universe is much close in size than a planck length. 
While we're on this topic. I'd like to ask a related question. 

How long does it take for an electron to travel one Planck length? If it takes any measurable amount of time, can't we imagine half that time and therefore the electron would be halfway between the Planck length during that time?  Which would then be a smaller distance? I guess I'm asking, how is there possibly a smallest distance? 

Or is the Planck length so much smaller than an electron that electrons are just buzzing along inside of a grid-like, pixelated space, instantly moving from one point to the next with just an expulsion of energy preventing it from moving any distance instantly? 
But isn't it true that (per wikipedia): "the Planck length is so many orders of magnitude smaller than any current instrument could possibly measure, there is no way of examining it directly".  So, wouldn't it make more sense to compare the size of the observable universe to the smallest length that can be observed (i.e. measured)?
[removed]
That's actually a really good question.. I see the answer posted - and am somewhat surprised at the disparity between the two. From a purely speculative standpoint, it's an interesting revelation considering how much throughout history we struggle to break the bias of placing ourselves at the centre of our observations
The answer could be that we are in the middle, literally.  

While the observable universe is 10^26 in size, the non-observable universe is estimated around 10^31.  Everything we can see is 'the hubble volume', but we can model how much may have expanded beyond that.

We cannot really see/measure the plank-length, yet, so using the two un-observables is fair game for this gedanken, IMO.

It does make me think about the epistemology of measures, that we "are at the center of everything."  I.E. we might not be, but rather the tools that we use to model things might be 'shaped' so much like our sensory organs (body) that we see everything with a slight 'anthropic tint.' 

We study these things at the ICRL, and the topic was included in the anthology ["Filters"](http://newalexandria.org/archive/filters/)
I would say it depends how you look at it. 

Closer to the observable universe: As has already been noted, we are about 9 powers of 10 closer to the size of the observable universe than the Planck length. 

Closer to the Planck length: We are 1.7 m greater than the Planck length, however, the observable universe is 8.8×10^26 m larger than us (and the Planck length). 

These analyses makes sense in terms of ratios, but make no sense when it comes to actual distance. The Planck length is 10^-35, which is pretty much 0, while we are on average about 1.8 meters. Meanwhile the length of the observable universe is 10^26, which is, for layman purposes, more than a trillion squared. Now tell me, is 1.8 closer to a trillion or 0? Answer's pretty obvious when we look in terms of physical distance, rather than order of magnitude. 
I didn't see any mention of the scale of orders of magnitudes. They don't grow linearly, they grow exponentially. So to say that we are "about halfway" between Plank length and the size of the observable universe is not true. We are far closer to the observable universe's size than the Plank length.

Also, the observable universe is literally observed, Plank length is not- it is mathematical. So wouldn't it be better to see where we lie on what we can directly observe? 
I find it a little strange that people assume the logarithmic scale is the  correct scale for such comparison.  Logarithms are pretty much a human cultural invention for ease of representation.  Physics does not operate logarithmically.

In which case, we are much, much closer to the Planck length.

[edit: see comment for more discussion.] 
[removed]
“Niagara Falls” is actually three waterfalls, the “American”, the “Bridal veil”, and the “Horseshoe”  falls.  Never in recorded history have all three of them completely frozen, but the smaller two (American and Bridal Veil”) have frozen a few times.  When it happens, most of the water on the surfaces of Lake Erie and the Niagara river is already frozen so the volume of water going over the falls is reduced anyway. The rest of the moving water goes over the unfrozen parts of the Horseshoe falls.  Besides this, two big hydroelectric power plants, one American and one Canadian, divert water away from the falls to generate electricity.  This means the river and the falls have less water than they can handle anyway.  

Source: grew up in the city of Niagara Falls.  This stuff is local legend and drilled into our heads as kids.  
[removed]
As mentioned in previous posts, much of the water gets diverted to hydro electric plants. In the summer, during the day, there is an agreement that states the water cannot he diverted as much. This is so that tourists can see the falls “in all their glory”. At nighttime and during the winter, much more is diverted for energy production. 

As for your question. Historically speaking, ice bridges have formed, even to the point where people would play on the bridges, set up a market, and walk across to the other country’s side. This has been illegal since 1912, but even while this happened, water still flowed over the falls. Although the ice bridge was thick and sturdy, the fast flowing water runs beneath the ice, so it can look like the falls have frozen, but there is still water flowing over them, along and behind the ice. Think of it like an icy covering that’s hiding the real flow of water. 

There is only one instance of the falls completely freezing over, about 150 years ago, however all the other water had completely frozen as well, so the side effects were minimal. 

tl;dr
If Niagara Falls ever did completely freeze over, the rest of Lake Erie probably did to, so there would be no concern for flooding or anything of the sort. 
The falls are never truly frozen--as in frozen solid with no flow. Think about a river or lake that freezes over. The ice may be quite thick but there is still water underneath. The river is still flowing below that solid sheet of ice. So in the case of the falls, you may see ice, but there is still flow behind it. It is always flowing even in the coldest winters.
[removed]
Three ways:

1. The Niagara River flows down towards the brink, where the river splits in two to go around a large island (Goat Island) perched on the brink. The American half may freeze over, but the Canadian (Horseshoe) half rarely if ever does.

2. The top layer of the American Falls often freezes over while current continues to flow under.

3. Some water is channeled off into the hydroelectric generating stations on either side of the river, where it bypasses the falls entirely and runs through tunnels, canals, reservoirs, and generators as it drops to the level of the lower Niagara River. 
[removed]
[deleted]
Normally when a water fall freezes the water under the ice is still flowing. Unless the water freeses completely, which does happen, leads to lots of flooding when the spring melt comes along. Not a huge issue for some river systems, especially like niagra which has an upper river water control system that controls water flow over the falls. Also it flows from the north, which is normally colder, meaning if the northern section of the river is frozen solid the southern section will have a reduced water flow to deal with. 

Where i live in Alberta the river systems are completely solid until they flow down over the US boarder down south. This morning was -45°c and its been a solid -30°c for a few weeks now. Until the system reaches that -10°c and lower range it is just a big ice cube all the way up into the mountains where they start. We will likely have a fair ammount of flooding this spring due to chinook systems that warm the temperature to +10°c for 5-7 days allowing for thaw and then dropping back to -30°c after the system passes. Which causes that thawed water to freeze on top of the ice that hasnt melted. Just piles of ice stacking on top of eachother, waiting to melt. Interestingly enough the large ammounts of melted ice water isnt the danger in the spring thaw but rather the large chunks of ice that have yet to melt being carried downstream in the increased pace the spring melt flows with. Its been known to take out bridges and houses along the shore. Reshaping the bedrock even and altering the flow of the river in extreme cases.
It can only fully freeze if the water flow behind it is weak enough (or frozen). Therefore, there is no imbalance of water behind it spilling off. Remember also, that water runoff in winter is less than summer because ice-melt upstream decreases.
I can tell you with certainty that, in NY at least, water levels in the St. Lawrence Seaway rise to an insignificant degree. Same for water across the Great Lakes Region, in each of the lakes. Keep in mind the Great Lakes are absolutely massive, and can take the modest increase of water input from the frozen falls, especially in the winter. There's less water falling over the Falls (a lot of the water is frozen) so there is no problem.
If you've seen the [image circulating this week](http://beta.ems.ladbiblegroup.com/s3/content/808x455/7fe6cf315a4811b08856086d33c54699.png), you'll notice that only the cliff next to the falls is coated with ice. Look at the left side of the photo, that's where the waterfall is, and it's pouring thousands of gallons over the falls every second. 
BLUF: No. 

During night time and winter months, more water is diverted to the hydroelectric plant for power. This prevents it from overflowing to Goat Island, and the rest of the surrounding area. The mist is what actually freezes the surrounding area, and the water is moving too fast to freeze all the way (except in 1948). Wikipedia has all the stats on cubic feet per second if you’re interested. Interesting fact, the falls area mainly goat island) remains frozen till the summer months due to the mist keeping the area cool. I used to visit the falls every week for decades, till I moved from the area. Love the area, high crime rate or not, it’s beautiful year round. 
In my limited experience, without the diversion of water to create 'hydro', the water upstream also would freeze. Eventually there is a channel created by the moving water under the frozen relatively immobile surface.
As the water upstream freezes the channel under the ice empties and creates a temporary tunnel.
Barry Lopez wrote about this in a book called 'Arctic Dreams' i think.
To add some context about the power Vista, the water intake for the power plant is upstream along the river on the Robert Moses parkway. [Two intakes ](http://clui.org/ludb/site/niagara-power-station-intake)control the flow going over the falls and fills up the reservoir. This is usually done at night when grid power demand is low and cheap.
In the unlikely event that all three falls, plus the two hydro-electric plants, plus the locks that let ships move in and out of the great lakes, freezes solid, the water will have nowhere to go. Lake Ontario has to add about three feet before the flow from Lake Erie would be stopped. That alone would take about a year, possibly more. Once Lake Erie stops to flow Huron, and Michigan will stop flowing also (the three are roughly the same level). It would take a decade or more to fill all three up to the point that they threaten to pour overland into other smaller lakes. Huron alone has to raise up almost 6 feet before it can significantly flood any of the surrounding lands.

All that aside, Summer will come along before any serious damage can be caused by the falls freezing solid. Once summer temps take over, the ice will probably melt, freeing up all the water that was held back by the temporary dam.
Hi there. From Niagara! The last time it “froze” it was actually just clogged because of the giant formations of ice that traveled down from the mouth of Lake Erie into the upper river. Since the water flowing over Niagara Falls is controlled for various reasons. (I’m paraphrasing here; to fill the reservoirs for the various hydro electric stations there and to prevent erosion of the falls proper) it essentially can never freeze over. Some flooding of the upper river happens, but in all my years living there it happened maybe once? 
Because the river leading to the falls is so rapid all the time it never really freezes over all the way. We sometimes go to the falls in the winter to see everything around the river covered in the spray that freezes. Makes everything look like wild ice sculptures.
Wolves are noted for their genetic plasticity.  Canines in general seem to be very adaptable to a wide range of environmental pressures, including selective breeding from humans.  The famous domestication study with Russian foxes further exemplifies this.  Most animals are nowhere near as plastic, including domesticated cats.  


[https://en.wikipedia.org/wiki/Phenotypic\_plasticity](https://en.wikipedia.org/wiki/Phenotypic_plasticity)   
[https://www.nature.com/articles/s41559-018-0611-6](https://www.nature.com/articles/s41559-018-0611-6)
Darwin says it’s because of their nocturnal rambling habits. 

>	On the other hand, cats, from their nocturnal rambling habits, cannot be matched, and, although so much valued by women and children, we hardly ever see a distinct breed kept up; such breeds as we do sometimes see are almost always imported from some other country, often from islands.

—[ON THE ORIGIN OF SPECIES.](http://www.gutenberg.org/files/1228/1228-h/1228-h.htm)

In other words, because the main use for cats has been to let them roam free and hunt by themselves, people haven’t generally kept them for selective breeding - they’ve bred randomly by themselves when they’re out alone.  Dogs, in contrast, have generally been kept kenneled up and it’s been possible to breed them to specific standards. 

Also, “The Nocturnal Rambling Habits” would be a great name for a band.
In addition to others discussions of degree of human control over mating, there's also a fundamental biological difference in how much the bones/other structures of dogs and cats shift from when they're young to when they're mature. Puppies and dogs have very different cranial bone ratios (just think of the face of a puppy vs the face of a full grown dog). Cats, on the other hand, have pretty much the same ratios from when they're born to when they're fully grown. Thus, it's much more likely that you get a random mutation that effects phenotype in dogs as they have way more moving parts that you can easily see the changes of.  

[Source, Bones: A Treatise Volume 7](https://books.google.com/books?id=gi0j5sEIW8AC&pg=PA127&lpg=PA127&dq=puppy+to+dog+skull+change+vs+kitten+to+cat+skull+change&source=bl&ots=TeF1LnnZjD&sig=ACfU3U2yO-8yh84bhC_2xTGCWRsYvKulzw&hl=en&sa=X&ved=2ahUKEwjikZTLg5voAhWJr54KHYgJBFgQ6AEwE3oECAcQAQ#v=onepage&q=puppy%20to%20dog%20skull%20change%20vs%20kitten%20to%20cat%20skull%20change&f=false)
[removed]
[removed]
Dogs and wolves diverged anywhere between 15,000-40,000 years ago. Dogs today show a lineage of being domesticated some 14,000 years ago from Asia though there are other dead lines from Europe that are even older.

Cats lived on the out skirts of society and slowly became domesticated at there own volition. This was a processes of about 9,000 years. 

Dogs were needed for more dynamic and specific duties while cats were favored for their natural rodent hunting talents. Cats not only had less time to change, they also didn’t really need to.
[removed]
[removed]
Dogs were actively bred by humans to do a super wide variety of specific jobs, from pulling a big sled full of supplies across the tundra to following rats into holes to tear them to shreds.

Cats have just sort of hung out with humans and ate the mice.
[removed]
[removed]
in Victorian England in the 1800s it became fashionable to have bespoke dogs and they bred hundreds of different breeds. Before that there were only a few original breeds. It was called dog fancy and dog shows still use that term today. 

Cats are the same but to a lesser extent. They are literally a different animal.
Selective breeding. Dogs have been selectively bred by humans for tens of thousands of years in order to shape breeds for specific tasks. This is relatively easy to do with dogs because females only go into heat twice a year for 2-3 weeks.   

Cats, on the other hand, are in heat almost continuously most of the year and controlling their breeding is very difficult. You basically have to keep them indoors in isolation. Besides, the major functions of cats over the years have been vermin control and companionship, neither of which requires selective breeding.
Humans selectively breeding dogs for their attributes for looks, jobs*like rat hunting needs smaller dogs, duck hunting needs dogs that have coats immune to water, herding be able to fight off predators while smart enough to heard, working like slead and police dogs, weather conditions like in the desert to full winter like Alaska. Looking at the dog groups you will see common features between toy, hunting, herding, working, guarding the list goes on. So dogs were selective bread to suit someones needs weither for a job or just the look of it.

While for cats humans didnt go out of their way to breed them. And they are so small that catching small critters was the main thing they can do. Only recently last 80 some odd years have cats been more selectively breed but it is again manely for the look. Since there are not many jobs cats can do, beside attacking your feet under the covers.

 So cats are more of a recent diversity while dogs have been breed for a longer time period. I hope that helps! 

You might also find the russian fox breeding experiments interesting.
Cats aren't domesticated, for the most part little to no selective breading has taken part. As such cats genes are well diversified and most are similar in appearance simply due to Darwinism. They have evolved to be the way they are through natural selection, the traits of the standard moggy are perfectly suited for prowling streets and farms for vermin and being cute enough that we let them sleep in our beds
[removed]
Dogs' ability to take commands very well has resulted in their use over a wide variety of fields resulting in them being bred to improve their performances to the specific tasks. Later, it became fashionable to show off your breeding skills and a wide variety of dog breeds came to be.

Cats, on the other hand, are notorious for their inability to take commands. As such, selective breeding of cats has remained a luxury, thus not having as broad an appeal nor history as dog breeding.

In short, it was never a necessity to breed cats. As long as they ate mice, they were fine.
It’s because humans found it fashionable to have specific breeds of dogs as a status, and so they’ve intentionally bred dogs for looks. 

Yes, this has also happened For the intention of using the dog as a specific tool or assistant.

Either way it’s kind of weird to think about how we chose a species, then decided which ones were valuable based on our values and forced them to mate and have babies at our will.
Cats have one role: killing rodents and other pests.

Dogs have many roles: killing rodents, fighting, tracking, fetching shot birds and small game, and strength based tasks like pulling sleighs.

Size is a function of role.
Dogs have very [long and flexible implications when it comes to SNPs,](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000451) which is a single gene that can influence a lot of other genes downstream. As the acronym suggests, SNPs are snips of printed DNA (now RNA) that get edited before it gets real and makes the real stuff. SNPs' consequences are also kinda like a sentence instead of just a word. Dogs are very flexible with changing their sentences and the sentences can vary wildly, which is very unique to their species. Artificial selection had a hand because we like cute sentences, or we like functional sentences. So despite mostly being the same with a lot of features, breeding some minor mutations can change entire paragraphs of gene coding.
Because dogs where used for a lot of different tasks. For example some dogs are used for guarding houses others for hunting. Because of those different tasks there are a lot of different breeds. Cats are only used for comparison and to hunt little animals like mouses.
Keep in mind that cats were (mostly) only ever used as rat catchers, while dogs were bred for a variety of jobs and uses. Cocker Spaniels are bird hunters, Labs are water dogs, Greyhounds are runners, Chihuahuas are watchdogs, Bloodhounds for tracking, ect.
Because there simply hasn't been much selective pressure for other kinds of cats.

We (humans) bred dogs into all kinds of different rolls. 

Sled pulling, hunting (with a variation for bird, or big prey, or small prey), companionship, sheep herding, pest control etc etc.

Cats do the one thing (kill rodents) but that's more or less all humans have ever needed them to do.

We don't even bother breeding them, if we had a farm and there where too many mice, cats would show up and adopt us.

All we had to do was not chase them away, and perhaps give them a warm-safe corner of the barn/house to raise kittens.
Honest short answer, cats are more dangerous and harder to tame than dogs.  Which heavily restricts size and selective breeding.  You can't breed a hunting cat, or guard cat because they don't have the social hierchy of dogs and are way more likely to be dangerous.

We actually see alot of interesting cat breeds, but we keep em small and manageable for the most part.
I come upon this thread 6 hours into its inception. I'm not going to read all the reply threads (fatal flaw, right?) in detail.  Skimming through, I see lots of erudite, fact-based, apparently scientific, answers. What I don't see is that which is, to my untrained perception, the obvious answer. Limiting the discussion to domestic animals, in which dogs weigh from, what, a pound or so, on the microscopic side, to probably 250+ lbs in the case of some Mastiffs, I would imagine a domestic 250 lb house cat would have little compunction about consuming its "owner," should the owner fail to come home after work one evening, opting instead to pass a few hours in the neighborhood pub, delaying the kitty's evening meal, then coming home to a displeased, hungry, 250 lb cat.
1. Cats are not truly domesticated. We don’t actually control their breeding in most cases. Also: felines are less likely to form packs than canines. They tolerate each other, they don’t really follow one another (lions and cheetahs are the only big cats I can think of off the top of my head that regularly hunt in groups). 

This makes dogs easier to control and less likely to try to attack senior pack members. 

2. A 70 Kilo dog does not scare me. Their claws are not sharp enough to do real damage, and I am as strong and usually faster than dogs that large. 

I would be terrified to play with a 25 kilo cat. MURDER MITTENS!!! 

Maybe our ancestors were smart enough to realize large cats aren’t as great of a houseguest as a large dog.
Cats that live in houses are just a subset of, and interbreed with, a general cat population.  House cats are also genetically very similar to their wild ancestors. 

So, cats aren't really domesticated. They're more of a communserate (or mutal? IDK) species that's along for the ride with H. Sapians.
Partly because dogs were (IIRC) the first animals that humans domesticated, and we have been selectively breeding them to exaggerate different *desirable* traits. Cats were domesticated far more recently and have not had nearly as long to be bread into weird shapes and sizes.
selective breeding. Dogs are the result of thousands of years of mating the most passive/docile/low aggression pups. When the Russian experiment breeding foxes started 50 years ago, They noticed that all sorts of specific physical traits such as spots and furry tails started emerging. There’s a great documentary called dogs decoded that is probably still on Netflix that covers this stuff
Geology PhD here, I will add citations and extra info if this comment gets interest, I'm just about to get on a plane so that'll be in several hours but a lot happened 3 Ma, during that period the Earth transitioned from a "greenhouse" earth to an "ice house" Earth as the Northern hemisphere glaciated. This was due to the closure of the Isthmus of Panama which altered ocean currents and allowed the Atlantic meridional overturning circulation to form (it may have existed in a weakened state before however). This cooled the northern hemisphere allowing ice sheets to form. The ice sheets of Antarctica had already formed at this point however around 34 Ma. 

In short the CO2 levels were higher due to a number of reasons but one was the planet was still in a warm "phase" with high levels of atmospheric CO2 because there was less area of ice sheet to store CO2 and cool the planet allowing more CO2 to be stored in the oceans

Disclaimer: this is all I can remember off the top of my head, please correct me if any of it is wrong :-)

Edit: My flight is delayed! Good news for you all but bad news for me haha, anyway, promised citations from a presentation I did on the topic recently:

• Bartoli, G. et al. 2005: Final closure of Panama and the onset of Northern hemisphere glaciation. Earth and Planetary science letters. V. 237, 1-2, p. 33-44. http://dx.doi.org/10.1016/j.epsl.2005.06.020

• Filippelli, G. M. and Flores, J-A. 2009: From the warm Pliocene to the cold Pleistocene: A tale of two oceans. Geology. V. 37, no. 10, p. 959 – 960. doi: 10.1130/focus102009.1

• Grotzinger, J. P. and Jordan, T. H., 2014: Understanding the Earth. W.H. Freeman and Co. (seventh ed.)

• Haug, G. H., and Tiedemann, R. 1998: Effect of the formation of the Isthmus of Panama on Atlantic Ocean thermohaline circulation. Nature. V. 393, p. 673 – 676. doi: 10.1038/31447

• Schneider, B. and Schmittner A. 2006: Simulating the impact of the Panamanian seaway closure on ocean circulation, marine productivity and nutrient cycling. Earth and Planetary science letters. V. 246, p. 367 – 380. http://dx.doi.org/10.1016/j.epsl.2006.04.028

• Linthout, K., Helmers, H. and Sopaheluwakan, J. 1997: Late Miocene obduction and microplate migration around the Banda sea and the closure of the Indonesian Seaway. Tectonophysics. V. 281, no. 1, pp. 17 – 30. 

• Lisiecki, L. E., and Raymo, M. E., 2005: A Pliocene-Pleistocene stack of 57 globally distributed benthic 18O records. Palaeoceanography. V. 20. doi:10.1029/2004PA001071

Edit 2: If you thought the Pleistocene had high CO2 then google the paleocene eocene thermal maximum, I'm happy to add info about that after my flight (once I can access my computer)

Edit 3: thanks for the gold! My first :-D boarding now (delay was minimal) I'll be back in a few hours with more cool palaeo stuff.
[removed]
[removed]
[removed]
[removed]
This question has attracted a number of joke answers, speculation, and low quality responses by non-experts, as well as a number of questions about the removed comments, all of which have been removed. If you are unqualified to give a thorough response, please consider refraining.

Edit: Additionally, many responses are making arguments about CO2 levels over the past 500+ million years as well as general arguments for or against climate change. The question asks about the causes of levels 3 mya, please keep responses on topic.
I have a question, if someone feels like answering it.

If the dinosaurs thrived because of "Greenhouse" earth, due to the overly warm temperatures and the propagation of all the "tropical" flora, won't the current "Greenhouse" Earth just lead us back to the same?

Plant species that don't grow above a certain latitude will now grow in Canada? Stuff like that? I understand it would be a huge change for every single human on the planet but will it really be horrendous?
When it's said that the atmospheric levels were at 410 ppm 3 million years ago, we get that information from ice cores, correct?  My question is, how long was the atmosphere at that level for it to appear in the ice core?  Was it at 410 ppm for a week (obviously not likely) or was it there for a year?  100 years?
So 3 million years ago, the CO2 levels were on their way *down* passing the 410ppm mark?  
How many years ago was it that the CO2 levels were on their way *up* at 410ppm, and what happened to all the life on earth at that time?
[removed]
Those are called sunspots. They are areas of the sun's surface that are cooler than the rest of it. They're still very hot, just not hot enough for the light they're emitting to be enough to see through eclipse glasses. Basically, there are strong magnetic fields in those locations that are inhibiting the normal surface convection of those areas, thus keeping them somewhat cooler than the rest of the sun.
From r/astronomy, should answer your question :)

[If you still have your eclipse glasses, take a look at the Sun today - there are currently two enormous naked eye sunspot groups facing our planet.](https://www.reddit.com/r/Astronomy/comments/6y16bi/if_you_still_have_your_eclipse_glasses_take_a/)
Earth-facing sunspots AR2673 and AR2674.

I received the http://spaceweather.com/ alert for them just last night!

If you've taken on a more than passing interest in the sun as it sounds like you have, you might consider signing up for their alerts - they'll let you know about Coronal Mass Ejections as they happen so you can have some lead time to get to a dark area to see the aurora!
Sunspots.  They are bright, but they look dark because what they're next to is even brighter.

"If you have your eclipse glasses, go look."  
  
This made me go 'whoah dude' more than it probably should have.  I mean, it's obvious that we all share the sun, but I've never seen it spelled out so clearly like that.
One of the sunspot groups has grown tremendously in just the last day or so, here's a cool GIF of it:

http://spaceweather.com/images2017/04sep17/sunspots_anim.gif?PHPSESSID=q66lf7ihj64s5nvk96jco0g392

The larger spots within the groups are larger than the planet Earth.  The growing spot group, known as AR2673, is active, and being monitored for flares or other disturbances.  It is facing Earth directly, and if it blows, could spray a blast at us.  Go here, might have to scroll down a bit, great pics:

http://spaceweather.com/


I ran outside and took a photo.  Yep, sunspots.  Pretty large one in the center.  That may mean the sun is spitting stuff our way.
[Sunspots 09/04/2017]
(https://i.imgur.com/ut7Uziw.jpg)

And the view from NASA's [Solar Dynamic Observatory]
(https://sdo.gsfc.nasa.gov/assets/img/latest/latest_512_0171.jpg)
On eclipse day I used a 6" telescope (with appropriate filter) and my Canon Ti2 to take this picture when the sun was about 75% occluded.

https://imgur.com/m9T50DG

You can see the sunspots in the picture.  They actually change gradually over time and you can follow their movements and the rotation of the sun by observing regularly.

Just thought you might like to see a close up of the spots you saw through your glasses.

Wow, I'm pretty late to the party here -- but I feel compelled to post anyhow -- at least you'll see it, /u/supaiderman.

Just like /u/Arkalius and several others pointed out, those are naked-eye sunspots.  They're places where the Sun happens to be *very magnetized* right now.  The magnetic field in a sunspot is about 1-3 kiloGauss (about 2-6 thousand times more powerful than Earth's own field), over a region about the size of, well, Earth's entire surface area.  

The strong magnetic field prevents convection:  normally the surface of the Sun is heated by hot material rising up from below in bubbles about the size of Texas.  These bubbles are called "granules" because they look like granules of corn in a solar telescope.  Each granule rises to the surface; cools by radiating, well, sunlight; and then sinks again in about five minutes (!!!).  To do that the material has to move sideways to get away from the stuff coming up just under it.  In sunspots, the magnetic field prevents that sideways motion, stalling the convection and allowing the material to stagnate and cool.

Sunspots are themselves pretty damn hot -- about 4,000 Kelvin, which is plenty hot enough to be bright white.  But the surrounding solar surface is about 6,000 Kelvin, and glows even more brightly -- so the sunspots look dark by comparison.

If you go to the [Solar Dynamics Observatory's nowcast page](https://sdo.gsfc.nasa.gov/data/), you'll see that the corona over the sunspots is particularly bright and (if you pay attention to the spectral information in those UV images) much hotter than even the rest of the ultrahot corona.  That's because the strong magnetic field penetrates into the corona, heating and containing it.  Watch over the next few days and you might see a strong solar flare happen there.
Wait.. you can just stare at the sun whenever with eclipse glasses? 
[removed]
[removed]
http://www.skyandtelescope.com/astronomy-resources/astronomy-questions-answers/will-mercury-and-venus-ever-transit-the-sun-simultaneously/

If you want to see Mercury and Venus both cross the sun at the same time, you'll have to wait another 67000ish years.
Go here.  

It's neat.  

https://sohowww.nascom.nasa.gov/spaceweather/

SOHO satellite solar analysis.  Includes images of the sun in different wavelengths of light, and will also show you items that you can't see with your glasses, such as the sun's coronal activity.  
There's a little black spot on the sun, today.

It's the same old thing as yesterday

There's a black hat caught in a high tree top

There's a flag pole rag and the wind won't stop

[Sunspots](https://www.spaceweatherlive.com/en/solar-activity/sunspot-regions)
[removed]
[removed]
[removed]
[removed]
If you want to see planets pass in front of the sun, you just missed the transits of Venus in 2004 and 2012, the next one isn't until 2117.  

Transits of Mercury are more common, there was one in 2016, the next one is on November 11, 2019 - then the next after that isn't until 2032
If you go to spaceweather.com, top left there is always an image of the sun showing sunspots (the dark areas) and faculae (bright areas). The numbers are just how astronomers keep track of them (i.e. Sunspot group 2357).
Due to the time currently and the influx of comments that are still violating the rules this thread is locked.

~~If you are going to comment please read our [subreddit rules](/r/askscience/wiki/rules) because we have strict comment moderation. If you are starting off with a personal experience or a statement like "I think" without citations to back up your stance, your comment will be removed.~~
You mean "micro gravity" as "anti-gravity" is more of a science fiction term ([micro-gravity is NASA's term](https://www.nasa.gov/audience/forstudents/5-8/features/nasa-knows/what-is-microgravity-58.html) about living and working in/on spacecraft). Or more likely just "weightlessness".

Female astronauts have the option of carrying their preferred pads or tampons while in space ([Popular Science 2016](http://www.popsci.com/brief-history-menstruating-in-space)). If they choose to not get their periods they can medically induce amenorrhea, or stop their periods from happening ([The Atlantic 2016](https://www.theatlantic.com/health/archive/2016/04/menstruating-in-space/479229/)). There was [a scientific paper](https://www.nature.com/articles/npjmgrav20168) written on medically induced amenorrhea in female astronauts in 2016. 

There are several ways to induce amenorrhea, through hormonal pills, injections or intrauterine device (IUDs). While it is safe to have your period in space, it may not be all that convenient to switch pads or tampons and deal with disposal. The water treatment is also set up to recycle water from urine, but not menstrual blood so that is also an issue ([American Chemical Society 2014](https://www.acs.org/content/acs/en/pressroom/presspacs/2014/acs-presspac-april-9-2014/recycling-astronaut-urine-for-energy-and-drinking-water.html)).

According to National Geographic (link below) if a woman spent three years in space she would need about 1,100 pills, which adds some weight to a mission but is less unwieldy than many tampons. Alternatively, three years worth of tampons (or pads) still weighs more than three years worth of pills.

Female astronauts' periods have a long history. National Geographic has a great opening paragraph in one of [their articles](http://phenomena.nationalgeographic.com/2016/04/22/how-do-women-deal-with-having-a-period-in-space/) on the subject:

>Sally Ride’s tampons might be the most-discussed tampons in the world. Before Ride became the first American woman in space, scientists pondered her tampons, weighed them, and NASA’s professional sniffer smelled them—better to take deodorized or non-deodorized?—to make sure they wouldn’t smell too strongly in a confined space capsule. Engineers considered exactly how many she might need for a week in space. (Is 100 the right number?, they famously asked her. No, Ride said. That is not the right number.)
I think what you were getting at is how does the blood come out without gravity? Because in that case it actually doesn't require gravity, the muscles in the uterus push it out sufficiently enough that it's unnecessary to worry about, but the comments about how you would deal with it are all correct and quite frankly hilarious on the scientists part. As a physicist who is a part of mission planning it's obvious that none of the men (who make of 90% of the field) would have any idea how to deal with such an issue. 

Edit: source! This article mentions how the uterus does the work. http://www.today.com/health/do-you-get-your-period-space-why-microgravity-menstruation-matters-t87711
Addendum: Now that I am at my computer, I can source/edit the post for those interested.

The three predominant modalities previously used were depot medroxyprogesterone (Depo-Provera), danazol, or oral contraceptive pills (OCPs) in suppression-dosing fashion (RT Jennings & ES Baker's chapter from Principles of Clinical Medicine for Space Flight by Barratt 1st Ed 2008). Suppression dosing means that they would skip the placebo week and restart a new pack every three weeks to cause amenorrhea. The most common choice was OCPs which would make since given the more abnormal bleeding patterns we may see initially with Depo and the fear of bone mineral density effects that both space and Depo could play (Knunitz 2009). Though it must be noted, that OCPs were not without risk either. Depot medroxyprogesterone is a progesterone-only contaceptive; the most common OCPs contain estrogen and progesterone. Exogenous estrogen may increase the risk for venous thromboembolism given female astronauts now meet Virchow's Triad of hypercoaguability caused by the estrogen, stasis in space, and trauma that occurs. Dr. Varsha Jain at Kings College London is an expert on this and has a freely-available lecture online [here](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20150001648.pdf). 

As others have mentioned, the ISS can recycle urine, but not blood and thus not having any bleeding was strongly desired. If women were done with childbearing, they sometimes would perform an endometrial ablation to prevent bleeding. This was preferred over a hysterectomy because they would prefer not to create adhesions in the abdomen and put astronauts at risk for bowel obstruction etc while in space. This is again discussed in the chapter by Jennings linked above. Currently, Mirena IUDs are preferred. This is more appropriate modality in that it can cause amenorrhea, decrease risk of endometrial cancer, act as highly-effective birth control, and decrease the risk of venous thromboembolism (VTE) by not giving exogenous estrogen.
Are astronauts required to have appendicectomies and cholecystectomies prior to their mission? If so, what about endometrial ablation? Given the radiation in space, have any female astronauts gone on to have kids? If so, have they turned out 'normal'?
[removed]
[removed]
[Dynamo Theory](http://abyss.uoregon.edu/~js/glossary/dynamo_effect.html). Quote:

_"In this dynamo mechanism, fluid motion in the Earth's outer core moves conducting material (liquid iron) across an already existing, weak magnetic field and generates an electric current. The electric current, in turn, produces a magnetic field that also interacts with the fluid motion to create a secondary magnetic field."_

So, to paraphrase, while the iron itself is not magnetic, it produces an electric current, which generates an electo-magnetic effect.
There are two different magnetic effects there.  

The inherent "magnetism" of ferrous magnets ("permanent magnets"), like you might find on your fridge, is due to a magnetic field being aligned in the metal (frozen, quite literally). As you heat the metal, the field "melts" and becomes misaligned.  

The magnetic field generated by the core is due to the material being in motion. Electric and magnetic fields are generated by charged particles in motion. A classic example of this is an electromagnet where you coil conductive wire and run electricity through it to generate a magnet (electricity is just electrons, charged particles, in motion). The liquid iron at the core of earth has sufficient charge that it acts in a similar manner, and generates a magnetic field. If the earth's core would cool fairly quickly, and we had an external magnetic field that was strong enough - the earth could become a permanent magnet as well.
The reason why the earth generates a magnetic field despite iron losing its magnetism at a temperature much lower than the core is because the molten core, which contains electric charges, rotates and thus produces an electric current which in turn produces the
magnetic field.
So magnetism is a bit weird in general. Electricity and magnetism are part of the same basic force, electromagnetism, and they're both caused by the movement of electrons.

Magnetism, in particular, arises as a consequence  of electron motion. Moving electrical charges generate a magnetic field. 

There's two very basic types of magnet then: permanent magnets and electromagnets. Permanent magnets are likely what you're thinking of when you talk about iron losing its magnetism. They're the result of an imbalance in electron spins throughout a material, resulting in a net motion that's moves in one direction. This net motion creates a magnetic field, and can be lost because heat disturbs the crystal structure of the material, allowing them to reorient and balance out the spins. However, at the pressures of the solid iron core, it takes much more heat to disturb the structure, meaning the core itself can remain magnetic at much higher temperatures--though this field should be fairly weak.

Electromagnets result from moving the electrons through a material in a certain pattern to induce a magnetic field. This works in reverse too, as moving a magnet through a coiled conductor can also generate an electrical current. In the core of the earth, you've got a strange setup for this: instead of moving the electrons through the conductor, you're moving the conductor itself (the molten iron) around the magnetic solid core. This electrical "current" around the magnet essentially acts like an amplifier for the magnetic field. It's something of a combination of an electromagnet and a permanent magnet. At least from my understanding.
Superheating any metal will create an electromagnetic field due to the movement of electrons. 

Take a massive amount of metal, molten and flowing with heat and pressure, and you get a significant amount of electron movement, and a resulting electromagnetic field.

This is also why the Earth's magnetic field is so inconsistent in terms of flux lines and fluctuates significantly over time. 

Most of the replies have focused on the motion of the liquid outer core around the solid inner core, which is the primary driver of the geodynamo (there's other, more exotic dynamos for things like gas giants that might depend on metal-liquid hydrogen and slushy methane mixtures). But for the inner core, many of the comments haven't addressed the original question - why does the iron not lose it's ferromagnetic properties?

The answer is that the super crazy high pressures drive phase changes in the solid iron that allow it to maintain an inherent magnetic field, although not with the same properties (note: this also depends on stuff like nickel mixed in with the iron). Iron at 335 GPa is not the same structure as ambient pressure iron, and the electronic properties get all sorts of weird. Same goes for the liquid iron of the outer core - it is still electrically conducting (the convective flow of which drives the dynamo), but the spin state of the iron changes. There's also some anisotropy in the Earth's magnetic field that may be related to the crystallization properties of the iron at the liquid-solid outer-inner interface in the core.

Source: [quantum mechanics of the Earth's core](https://phys.org/news/2017-07-quantum-mechanics-earth-core.html)

Actually, that was a pretty good question.

Magnetism is apparently an emergent effect from a large number of charged particles sharing an alignment.  Think of that famous video of simple harmonic motion taking down the bridge in Washington.  (Tacoma Narrows, I believe.)

Well, a small thing can be aligned on the scale of, well, a small thing.  But eventually other factors can be so energetic they disrupt any small form similarity.  That's your Curie limit.

However, things that are dissimilar at one scale can be highly similar at another.  Over distances where the temperature driven effects completely average out to nothing, there can be much larger similarities.  From those larger features (think a raindrop vs a state in a hurricane) you can derive the same similarities that gave you locally magnetic behavior, only of course now on a much more vast scale.

Huh!
The short answer is that 1) spinning metal has a magnetic field, 2) temperature doesn't exist in a vacuum; temperature and pressure both influence the state of matter, and 3) it's and its are different words in every language.
Very weak magnetic field multiplied by a whole lot of it, which is sufficient to move very small (by comparison) magnets but too weak to move large or very weak ferrous materials.

Also:

His, Hers, Its.

He's, She's, It's.
[deleted]
[removed]
Basically any magnetism is caused by moving electrons (or current).  

In a magnet, for example, it's caused by the electrons in the Iron atoms spinning all in the same direction.  (I'm not going to get into why they spin in the same direction but if you'd like to know feel free to ask.)  Iron loses it's magnetism because the aligned spinning electrons in the piece of iron start spinning different ways as they loose their alignment over 800C or so (or even if you hit a magnet hard enough you can screw up the alignment).  

The earths core however has currents in it (it spins) so all those electrons in the iron atoms are moving in the same direction as a total change in motion.  This rotation of the core (and therefore the iron atoms) means that the electrons in the atoms also are carried along.  This means that there is a current which causes a magnetic field!

TLDR:  The core is spinning which causes a net electric current which causes a magnetic field!  
i wonder how does it correlate to the magnetosphere?
To understand why not, you have to understand how a battery works. It's not a tank of electrons, exactly. A battery is a chemical reaction between an anode (-), cathode (+), and an electrolyte (or some form of conduit) which facilitates the exchange of electrons.

When you discharge (use) a battery, a chemical reaction is occurring, moving electrons between the two chemicals (anode and cathode). So, don't think of a battery as a vessel which just holds a pile of electricity. It's a specific transfer of energy between two chemicals.

Now, in a rechargeable battery, this reaction can be reversed by applying electricity using a charger. But, again, it's not adding anything, really. It's reversing the chemical reaction which is putting the chemicals back to their original state.

When charging, it takes time for the reaction to reverse and at the same time, heat is created because the electrons are moving back across the conduit they travelled across originally. Electrons moving over a conductor meet resistance which creates heat, no matter which direction. Much like the battery heating up when you use the phone, the battery will heat up while charging for the same reason. If you were to throw a bunch of current at the battery, it could easily cause a thermal issue. Nickel-cadmium (NiCd: pronounced Nye-Cad) batteries had a problem called thermal runaway where the battery, if charged too quickly, the heat generated could induce a self-feeding loop where the battery could heat to a point of ignition or explosion. But that's not an issue with Lead-Acid batteries (like sealed lead acid batteries like in your car, which use a liquid electrolyte, also known as a wet cell [thanks to u/gilescoreywasframed for the correction below]). Also prone are Lithium Ion (LiOn, like in your cell phone) or Lithium-Polymer (Li-PO) batteries (credit to u/incrediboy729 for the correction here), but these usually incorporate circuitry to prevent this situation.

tl;dr - it is dangerous, and modern cell phone chargers and batteries have circuitry to prevent too fast/overcharging which could lead to heat damage or fire.
This is what we're hoping to be able to do with [graphene batteries](https://www.google.ca/amp/s/phys.org/news/2016-07-fast-charging-everlasting-battery-power-graphene.amp) 

They are an example of a supercapacitor, a battery able to physically hold onto electrons. You'd be able to plug it in, fill it up and go on with your life in a matter of seconds.

This is very different from our current technologies, all of which use an electrical current to reverse a chemical reaction. As everyone is saying this process necessarily takes a while and is fairly inefficient, hence battery chargers getting hot, heat production=wasted energy. This is essentially the same difference between lightbulb types. Incandescents are very inefficient, so they get very hot, CFLs are fairly efficient so they only get warm and LEDs are very efficient so they barely produce heat at all.
In short - heat.  Cells (because the world is a collection of bodges) have chemistry that creates internal resistances, and that means heat is generated in charging much as passing current through any wire.

Fast chargers (usually rebadges of Qualcomm QuickCharge) negotiate upto 12v charging adjusting the current to reduce heat build up getting out of control.

What you want CAN however be done with supercapacitors - they're basically chemistry-free, and are constantly evolving.  Soon we'll likely be seeing them in cellphones, but they're not ready quite yet.  You can charge them to capacity very quickly.
In simple terms without getting into battery chemistry:

Power (wattage) in a conductor is resistance times current squared.  So when current is flowing in or out of a battery, it is going to create waste heat based on it's internal resistance and the current flowing in or out of it squared.  This means the battery itself gets warmer when it's in use or charging.  And since it's current squared doubling the current quadruples the heat.  If you either charge or discharge too fast, it will overheat.  The result of overheating will depend on the type of battery.  Could make the pack fail quietly, or could make it burst into flames...
A battery is made of metal and rust. One of the metals wants to give away the glue that holds it together, so it can turn to rust. The rust wants to get some glue so it can become metal again. Instead of letting them make their trade normally, by putting both in a bath of basically saltwater, we put something in their way. Now, they have to push it through a pipe to exchange it. the power they spend pushing is used by your phone to play youtube and stuff. How hard they push is called voltage, or how badly they want to trade electrons. Your battery wants to be at 0.00V. If it goes below 2.90V, we won't be able to recharge it, because one metal will be a pile of rust and another will be overgrown from all the electrons the rust gave it. 

At too high a voltage, the metals say "lol, screw this" and start doing stuff we don't want, like making hydrogen gas (explosive) or getting too hot. And we also can't recharge them after this. 

If you try to charge too quickly, you make too much heat. The types of metals in batteries really like heat because it helps them rust quickly. All this heat lights your phone on fire.
For an example of something like what you're describing, look at the Tesla Model S.  

Rapidly dumping power into a battery makes it heat up, which is bad for the chemistry.  So when supercharging, the Model S runs extra cooling to the batteries to optimize the charging rate without damage.

That takes a LOT of ventilation, that no one wants in a phone.  And it's still not instant.  (30 minutes is pretty quick.)
I don't think I read this anywhere but the electrochemical explanation behind your question is that, charging a battery faster past its rated amperage becomes a mass transfer limited reaction which will result in lithium plating, on the electrode, uneven surface charging , dendritic growth and ultimately shorting of the battery. 

Source: UCR, research lab.
Two solid reasons:

One, all conductors have a resistance, and it generates heat when you put current through- but it goes up with the SQUARE of current, e.g. 2x current =4x more heating.  8x more current=64x more heat.

Second, in a lithium battery, lithium ions- the actual atom- must physically leave the cathode into the liquid electrolyte and move across a very fine-passage porous separator to the cathode and merge with the material in the anode.  Not just the surface, it needs to penetrate fairly deep although they do try to keep the anode/cathode thin.

You're not just moving electrons, like a capacitor.  You're pumping whole atoms from one side, through liquid, to the other.  There's no physical pump, of course.  The electric field coerces them to leave and diffuse on and infuse into the anode.  That's a slow process, relatively speaking. 
I see a lot of good responses here, but didn't see one word in particular. Charging a lithium battery too quickly will result in the formation of *dendrites*, physical deformities inside the battery that look like little tentacles. These occur naturally during "normal" use and are part of the reason why no lithium-based battery is good forever, but their growth is exacerbated by very fast charging, among other things. These little tendrils inside the battery reduce how much charge it can store, reduce its ability to perform well even when charged, reduce how much longer it is good for (i.e. remaining charge cycles before the battery is useless), and are believed to increase the risk of a catastrophic failure of the battery.

One way "around" this (sort of) is the software-defined battery. A device can have multiple batteries of different chemistries that are exposed to its operating system as a single battery. The charging circuit intelligently controls charging and discharging in a way that takes advantage of these chemistries. For example, you can have a normal lithium-ion battery and a second one that is more tolerant to fast charging. The charging circuit would provide lots of power to the more tolerant battery initially to give you a usable starting charge quickly, then would slowly charge the second (presumably higher-capacity and longer-life) battery.

In short, with the types of batteries used in phones, it's bad for the battery and potentially dangerous to charge the battery quickly. We have ways of getting around it - to a point - but at some point we're probably going to need a change in battery chemistry to get the best of both worlds out of a single battery.

Edit: italicized the word "dendrites" since that was the whole reason I made this post.
[deleted]
Unfortunately, most of the responses in this thread are wrong.  It's not an issue of heat, or cabling, or wall chargers, it's an issue of our current state of the art in battery technology and charging-induced damage.

Every time you charge a battery, you damage it, permanently, irreparably.  The amount of damage you do depends almost entirely on the speed at which you're charging it, faster charging = more damage.  Different battery architectures/chemistries have different limits on the rate at which you can charge/discharge it with minimal damage, but unfortunately the chemistries which allow faster charging almost universally have a lower energy density.

You can ABSOLUTELY grab a lithium-based battery that can charge at 10C (dead to full charge in ~6 minutes) with minimal damage (eg: LiFePO4), but it has an energy density roughly half of what's in your cell phone, which means it would need to be twice the physical size/weight for the same capacity.

The high density batteries used in phones/laptops to make them smaller/lighter can really only be charged at a maximum rate of around 0.5C without causing TOO much damage, which means flat to full charge in ~2 hours minimum.  If you try to charge it any faster than that, you'll cut significantly into the battery's lifetime.  Instead of it lasting a year and a half before becoming mostly unusable, it might only last 6 months.

This works the other way as well, if you throw your "ultra fast super charger" in the closet and use an old 0.5A or even 1A charger instead, you can extend the lifetime of your battery significantly.  Reserve your fast chargers for when you actually need them, use a slower charger overnight and your battery will thank you.
Even aside from the chemical process that is the basis of slow battery charging, there is a more fundamental limitation at work. It's worth having a concept of Ohm's law as to why this is the case. Ohm's law relates voltage (volts), current (amperage), and resistance (ohms). This is [a decent picture to describe the relationship](http://imgur.com/SWwg7hP).

Essentially, resistance limits the amount of current you can put into something (a circuit or even just a wire). Higher voltage will push more current through the same resistance (this is part of how the rapid chargers work, they use 9 volts instead of 5 volts).

So, getting a lot of energy into something really quickly means (by definition) very high power (power is voltage multiplied by amperage). Unless you have a superconductor (no resistance) you need very high voltage to push sufficient current through. High voltage and consumer electronics do not play well together.

In addition to that, some power will always be dissipated in a resistive circuit (non-superconductor). The amount of power is proportional to the current squared multiplied by the resistance. The more current you try to push through, the worse it gets squared. Dissipated power shows up as heat and heat gets things hot. Lot's of heat and consumer electronics do not play well together. 
With great power comes great heat exchange.  Ever~~y~~ realize your laptop gets hot after charging for a few hours?  Imagine all of that heat exchange in a millisecond.  Your phone would be a puddle of plastic and fried circuit components.
ELI3:

You can probably lift 10lbs 100times. Might even be able to lift 100lbs 10times. But you probably cant lift 1000lbs even once.

Not a perfect analogy but the point is just because you can do something over time doesn't mean you can do it all at once.
It's too dangerous and not possible, both for the same reason: the resistance of the batteries.  This resistance limits the amount of current you can shove into the battery by increasing the voltage across the battery when current is going into it (V=IR), and it also dissipates more and more power in the form of heat the more current you shove into the battery (P=I^2 R)
> Why can't we just inject a ton of power into a phone at once to instantly charge it?

We can. Very easily. Just swap the battery. That puts a ton of energy into the phone at once and instantly gives it a full charge. It's not even dangerous.

Oh, *why* can't we swap batteries on phones anymore? Well, that's another story altogether...
Heat. Every electronic component has a max temperature before it fails. Every electronic component also has at least some kind of resistance (though it may be small). When you run a current through anything with a resistance it generates heat. When you talk about injecting a ton of power into a phone, you are really talking about a ton of current, and as a byproduct a ton of heat. If it didn't outright kill the components on the electrical path of the current, they could be walking wounded and die at anytime, or at the very least have their long term reliability greatly decreased.
Think of the opposite scenario. What if you short circuit a battery so it discharges quickly? The chemical reaction is so fast that the battery gets very hot and is damaged. In the same way, you cannot pump in a huge amount of electricity at once as the chemical reaction would be too fast, the battery would become extra hot and may even explode. 
The same reason why you can't eat one huge meal that lasts you for three days: there's a limited acceptance rate for charge. That said, you could carry a week's worth of food in a backpack, which is similar to an external battery that will charge your phone in a few hours.
You've just described a supercapacitor, something that doesn't exist yet but we're actively trying to invent.

A capacitor is excellent at absorbing a charge, but it leaks like crazy, so unless it's plugged into a constant source of electricity, it will empty out to zero very quickly.  If you stare at a motherboard or any electronics, you'd see tiny cylindrical barrel-looking things, those are capacitors.

A battery is excellent at holding a charge, but it's chemical and takes forever for the various molecules to find their twin.  Slow charge and slow discharge, but it doesn't leak much and can hold onto that charge for weeks unlike a capacitor that holds a charge for seconds.

A supercapacitor is a capacitor that doesn't leak as much.  In theory, it should be able to hold a charge for hours if not days or weeks.  A capacitor is amazingly fast at charging and discharging.  Short of a lightning bolt, most large sources of electricity can dump into a capacitor as fast as you can supply it electricity.  This is a holy-grail idea like cold fusion.  It would revolutionize handheld devices, or even Tesla cars.  Tesla, by the way, is majorly funding research into supercapacitors.
Kahonas already gave a good answer regarding the chemistry of batteries.  
However at the moment batteries aren't the limiting factor for charging. There are batteries for RC models which can be charged in 15 minutes.  
The limiting factor for phones is most often the charger:  Nearly all chargers are based on USB 2.0 which is limited to a voltage of 5V and was designed for a current of 500mA.  
Many chargers will use 5V/2A which results in 10W of power. And this isn't really much.  
A part of the 10W gets converted into heat (power losses of the cables, connectors and charging circuit).  
There are newer charging technologies, that allow the voltage to increase (Type C chargin and Quickcharge). This leads to overall more power and less power losses, resulting in a faster charge.  
Some devices can get 50W of power over USB type c
Similar to having 20 gallons of gas in your tank that will ignite over the span of 6 hours to propel you 400 miles down the road. You can't torch it all at once without a serious heat-induced destruction of the vehicle
If only we could change batteries easily. Like everyone would have two batteries, one on the charger, one in your phone and when the phone battery discharges just swap them. Fully charged battery in the phone and depleted on the charger.

And if the phone would have a tiny, secondary built in battery to keep the phone turned on just for a minute, we could replace the batteries without having to turn it off and on.


The long and the short of it is that batteries have a fairly substantial internal resistance that means cranking the amperage while charging generates far too much heat to deal with. That's the advantage of supercapacitors, they have a negligible internal resistance so they are able to charge and discharge extremely quickly. Unfortunately, they have a lower charge density than batteries meaning they are much larger for the same capacity, and they are much more expensive.
A rechargeable battery converts electrical energy into chemical energy when it is charging, in the same way that a light bulb converts electrical energy into light.

In both cases, some of the electrical energy is also converted into heat, which is dissipated into the battery or bulb's surroundings.

A proportionally large amount of a "tonne of [energy]" injected into a light bulb would be converted into heat. The bulb may not be able to dissipate this heat fast enough, and would get very hot and 'blow'. Similarly, a battery would get hot and 'fail'.

Batteries often contain nasty materials, so the consequences of this failure can be dangerous.
For the same reasons that you can't discharge all at once. Most batteries have a safe operating current, and an immediate charge or discharge would produce a current way above this limit. Higher current produces high heat, which could cause a chain reaction and ruin the battery.

When you hear about exploding E-cigarettes, it's almost always due to someone keeping a battery in their pocket with loose change. This causes a short, which can make the battery discharge a ton of stored charge and overheat. 
Since the answers here are pretty thorough, I'll just go off on a tangent: 

What you want (if you want to inject a ton of power) are capacitors. The only problem is that they just don't hold a sustainable amount of juice. Their discharge curve is measure in micro or milliseconds.
You actually can charge batteries faster by applying more energy.  Too much and they explode.  Batteries are taking electricity and converting it into chemical energy, which is way better for storage.  Then converting that back when you want electricity.  Being a chemical reaction you can have some energy converted into heat.  Too much of that heat and it can react with the chemicals and start a chain reaction that converts all the energy into heat.  Then your phone is on fire, like a Galaxy note 7.  Shorts or other things that apply too much energy at once can trigger this.

In addition applying too much energy might not destroy the battery, but might slowly damage it resulting in lost capacity over time.

So most batteries have their chargers designed to charge them at a rate that doesn't risk them catching on fire or wearing out the battery to fast... unless you still have a Note 7.
The energy per second (watts) would be enormous. Watts = voltage * current. You could choose really high voltage and get plasma arcs that melt the battery or current so high that the battery explodes, like a fuse. Or both 
Batteries are chemical in nature and will need time to charge/discharge. They don't have an infinite capacity for current and will blow up if you try this. 

What you're describing is only possible with capacitors which are purely electrical devices that hold power, but they have their own problems. Graphene capacitors look promising though.
For the record, if this question was born out of desire for convenience and not curiosity, OP can always buy a second phone battery & external charger/broken-screen phone to use for charging and swap batteries instead of plugging OP's phone in every night.
Batteries have electrical resistance, they're not perfect. Lets take a typical phone battery of 3.7v/1 amp hour with 0.5 ohms of internal resistance and apply ohms law.

If we applied 4.2 volts to this to charge and the battery was already at 3.7 volts then the battery will consume 1 amp. (0.5 volt difference  / 0.5 ohms = 1 amp)

If the battery was completely dead at 3.0 volts then it would take up to 2.4 amps at the beginning. 

The only way to speed this up is to increase the voltage which is dangerous for lithium or to decrease the resistance of the battery. Newer 'fast charging' batteries have a lower resistance and can charge/discharge quicker.
No process is perfectly efficient, and batteries are quite bad about this. What this means in power technologies is that it creates unwanted heat with whatever waste energy it produces.

With something like a phone, the battery is designed to be small yet hold a lot of power. To charge it extremely fast would cause it to heat up very fast, faster than the phone can shed the heat. The heat then would damage heat sensitive components, or even cause solder to melt. It would destroy the phone.

The fact is, we have batteries that can hold a bunch of charge and charge/discharge very fast. But they invariably get very hot in the process, and with the kind of batteries used in phones, this can denature or even ignite materials inside the battery itself.

Long story short, we COULD potentially charge a phone in a few seconds, if we could figure out how to make a battery extremely efficiently, or a phone (and battery, and hand) that could stand the extreme heat. Neither of these is likely so we're going to have slow charging phones from now till doomsday.
[removed]
There is a practical limit beyond the important problem of battery heating. Where are you going to get the power from? To charge in one second, a 2 Amp-Hour cell would need a DC power supply capable of over 7200 Amps. That current would need huge cables and connectors. The phone would weigh 20 pounds. 
This is the sole reason I miss my LG G4. I ran 2 batteries so when one was at ~10% I'd just switch it out for a fully charged one and 30secs later I'd have full charge. I'd have to do this most days as I'm a heavy user but it worked well for me except the android part. 
There's this thing called a logic level. It's the voltage chips operate at. Some operate at 5v logic, others 3.3v, etc. There's a special chip used for power management. It does things like charge batteries, distribute power, regulate voltage, etc. Since that chip also has a logic level, say, 3.3v, if you inject a ton of voltage, it will instantly fry the chip, rendering it useless.
Best case scenario, you would trip the circuit breaker, worst case scenario, you'd set the outlet and the charging cable on fire.
As for capacitors, they have a property called ESR meaning equivalent series resistance, which limits how much current they take in at a given voltage (Ohm's law; U=ESR\*I), which slows down charging. ESR also affects how much they heat when you charge them (P=I\^2\*ESR) and of course you cannot heat them too much or too quickly, which again slows down charging.     
Aside from the answer to your question, lots of people, including Elon Musk, are rapidly working towards what's called a super-capacitor, which would allow for very fast charging and slow discharge. Assuming it's even possible, inventing and selling these super-capacitors would be WILDLY profitable. Probably the most profitable invention in human history. 
Electricity is run on an alternating current (AC) which reverses voltage in the pattern of a sine wave. In the US, the frequency of this alternating is 60 Hz, which is a B flat (more of a flat B or sharp B flat, really). Sometimes interactions with surroundings, especially at high voltages like in transformers, can cause a hum at the frequency the grid is run on. In most of Europe, as an example, it's run at 50 Hz not 60 Hz, so you would hear a G (a little sharp) instead of a B flat.
In the US, alternating current (AC) oscillates at 60Hz and the buzzing you hear is generated by the second harmonic, so 120Hz. The nearest B-flat is at 116.54Hz, so it's pretty close! The next B is at 123.47Hz, so really you're hearing something that is very much in the middle of B and B-flat. If you were in Europe, where the standard is 50Hz, you'd be hearing something between G (98Hz) and G-sharp (103.83Hz).

Also, excellent ear, are you a musician?
[deleted]
Electricity oscillating doesn't produce the sound by itself; the oscillation of electricity needs to move air in order to produce a sound. Air is not influenced enough by electromagnetism at audible frequencies that we would hear any sounds from a direct interaction between the electricity and the air.

The oscillating current of AC electricity creates an oscillating magnetic field, which is especially strong in electrical transformers where the wire is wound around an iron core. The kind of iron used in transformers exhibits a phenomenon called "megnetostriction", where the crystal structure of the iron grains in the metal changes its length in the presence of a magnetic field. It is this rapid oscillating length change in the iron cores of transformers and other electromagnetic components that results in vibrations that we can hear.
[removed]
Interesting side note ... aircraft typically use 400 Hz AC generators. Sometimes you can hear the high 400 Hz hum when announcements are made over the PA. 400Hz is between G ~392 Hz and A 440 Hz. 
400 hz: https://www.youtube.com/watch?v=33qV3d3U0q4

440hz: https://www.youtube.com/watch?v=rFOl-9SNxLY
A lot of answers here are "electricity is produced at 60Hz in the united states"

but none of them are answering why this is the case,

the frequency of AC mains power is selected to give Synchronous motors attached to grid power directly good operating characteristics. that is, stuff like vacuum cleaners, washing machines, fans for air systems etc.

These are most electric motors that plug into the wall that don't have a special power distribution mechanism built in, those that do have this kind of frequency controller hardware are called Variable Frequency Drives and are often used where the speed of the drive needs to vary rapidly.

VFDs are limited to industrial use for the most part.

*****

The reason the frequency matters for Synchronous motors has to do with the physical design of alternating current motors and the geometry, but the simple explanation is that for simple synchronous motors the speed of the motor in RPMs is dependent on the supplied frequency because the physical rotation of the motor is being driven by the rotation of a magnetic field and the magnetic field is going to rotate at the frequency of the alternating current in AC Synchronous motors.

If we imagine the motor in its most simple state as a set of magnets with a single North and South pole on the rotating part of the motor (the rotor) and a single North and South pole on the stationary part of the motor (the stator).

  Our AC mains power drives the Stator magnets, and the magnets on the rotor is a fixed magnet, the rotor spins in normal operation as the poles on the stator switch positions, and the permanent magnet moves to correct its alignment.

With only one set of poles, the rotor speed will match the mains power speed, making one full rotation for each full cycle of switch in polarity.

In real motors the speed will be some multiple of the frequency because multiple poles are used as they provide better torque.

because of this 60 Hz in the US, and 50 Hz is Europe was selected because it allows motor designers to easily build motors that will operate at safe speeds and torques when connected to mains power.

EDIT: added information about how poles work in a motor

Some old analog synths use the 60hz of the electricity supply as a reference for their oscillator frequency. Needless to say, these are then tuned lower when used in Europe :/   And I once saw someone playing at a forest rave where everything was powered by a diesel generator, which didn't have a stable RPM, so some of the synths were drifting in and out of tune all the time :)  Not that it mattered...
The power in your house is called Alternating current (AC) which inverts the voltage in a sin wave at a certain frequency. Depending on where you live, its either 60 or 50 hz. 60 hz tends to sound like a b flat (this is probably your case) and 50 is like a G sharp. 
This effect especially noticeable in microwaves. See, microwaves have quite big transformers in them. Transformers typically have 2 wire coils, one called the primary and the other one called the secondary.

How this works is the alternating current in the primary coil generates a changing magnetic field that induces alternating current in the secondary coil. So wait, you are just generating current in another circuit, why is this useful? 

Here comes why this is called a transformer. Say your primary coil has 1 loop of wire, and the secondary has 2. The voltage you put across the primary is doubled on the secondary coil. If the secondary had 4 loops and the primary had 1, the voltage would be multiplied by 4. 

This way, you can generate the crazy high voltages you need to drive the magnetron to produce microwaves from 110V if you live in the US, which seems to be the case taking into account you seem to be hearing 120 Hz (the second harmonic of 60 Hz, the power line frequency).

So why does the microwave sound like this? The microwave walls are made out of metal, which reacts to magnetic fields ever so slightly. The magnetic field the transformer is producing is oscillating, making the metal oscillate, making the air oscillate, making your ear drums oscillate! 

And so, you hear the 60 Hz hum. How, I don't really know why we hear the 2nd harmonic and not just the fundamental frequency, though I'm quite interested to know. If someone could add up to this answer in the comments, I'd be very very thankful.
https://m.thisamericanlife.org/radio-archives/episode/110/Mapping
This is a very applicable segment from this American life from the 1998. Please go to act 2, mapping sound. This guy maps what notes all of the sounds he hears throughout the day make. Very interesting!
[removed]
[removed]
Is this also correlated to electromagnetic fields around the power lines? I thought I had heard that the visual spark (seen at night) and the humming was due to electrons in the air surrounding the lines being transferred from the e-magnetic field in the power line. As a result of the high voltage, an audible and visual transfer of electrons in the high lines, until after a substation. 
Not sure on the correct terminology for "electrons being transferred".
Several things.

There HAVE been many advances in cancer prevention, detection and treatment. (Though obviously there is tremendous work yet to be done.)

Often, new treatment technologies, while genuinely promising, fail to fully live up to that initial promise.

Pop science and medical news reporting often exaggerates or overstates the potential of emerging technologies. Even responsible journalism is often sensationalized to bolster reader interest.

Finally, there are MANY different forms of cancer with wildly different causes, characteristics and disease processes. It is pretty unlikely that we will ever see any single "cure" for all types of cancers.
Pointing at just one of them:
https://www.cancerresearch.org/blog/december-2013/cancer-immunotherapy-named-2013-breakthrough-of-the-year

Checkpoint inhibitors which that article talks about have had a massive impact on a wide variety of cancers.  They don't always work, but for some types of cancer they improved survival 20% or more.  That is massive for what probably averages a 1-2% per year increase in survival normally.

I work in the cancer field and a decent percent of doctors still talk a bit in awe of checkpoint inhibitors and how big of an impact they have had.

Likewise you have CAR-T linked as well.  That also for the folks it works on is revolutionary.  It took some cancers with an average survival measured in months and gave them a chance.  Again it is for a limited number of cancers mostly blood cancers (work is ongoing for solid tumors, but proving difficult though recently there has been some promise), but for the cancer types it works on it certainly is life changing for those folks (namely that they will have a life).
There are multiple reasons as to why. Each “breakthrough” has different struggles. One of the sources listed says it was able to unlock the genetic structure of a certain cancer or something to that extent. While it is a breakthrough to see how the cancer may develop or spread, it isn’t the whole picture. It’s similar to how you may have pictures of a particular engine, but that doesn’t mean you know how to build the engine or change it in a way that will make it more efficient. 

Another source stated a type of T-cell was found that can distinguish between cancer cells and normal cells and would leave the normal cells alone. That is great that we have found that type of T-cell. But without being able to control that T-cell and make it do exactly what we want or to duplicate for the benefit of others, it is hard to move past the “breakthrough” point.

Other examples may be finding a certain drug or treatment that is extremely effective against a certain kind of cancer in lab rats. That is good for knowledge of how it may be used in humans, but human physiology is very different from rodent physiology. It can take a very long time to make the transition between the two. Or after they have made the transition to human testing, they can find it’s no more effective than the other treatments we currently have. 

Also, at least in the case of the United States, it can take many years for even simple drugs to go through the multiple phases it takes to go from initial testing to FDA approval. Drugs for more complicated conditions can take longer as they may be more risky and so they start with lower doses and wait longer to see if there are any side effects. With a smaller pool of candidates (like with rare cancers) it’s harder to get a clinical trial going so that can add time as well. 

All said, many of the breakthroughs are wonderful things and we can learn a lot from them, but it’s hard to translate this new knowledge into a practical form of treatment that works reliably across multiple types of cancers and the different body types.
* Many of these breakthroughs do end up being useful but only for specific use cases. For example, you might have a treatment that works well for Acute myeloid leukemia but might not work well for more solid tumors elsewhere. 

* Some science journalists jump on pre-clinical (non-human) research and hype it up before we see what happens in humans. Obviously almost all good clinical research starts with pre-clinical discoveries in rodents, tissue cultures, and other animals (especially when doing toxicity studies for safety) but obviously not everything translates well to humans. 

* Sometimes it's a failure on the *sponsor* of a clinical trial (pharma company, university, research hospital network, etc...). Sometimes they can't perform a robust enough clinical trial or messes up data collection, or has a critical flaw in the study design that ultimately weakens the evidence (even if the study was designed to be *safe* enough to pass an ethics committee, which they are all supposed to be). 

Source: I have worked as a science journalist before and I have made mistake #2 here before, my apologies.


If you want a universal cancer treatment, you need to develop powerful technologies around genetic engineering. Small molecule (or even large molecule) treatments will not cut it as a universal treatment. Cancer is a disease of the genome and until we learn how to protect, maintain, and alter (in order to fix errors) the genome then we will never be free from cancer.
In general, believe very little you can read about in standard news outlets as far as medical breakthroughs. "Incremental progress, we'll know more in three years" is not a clickable headline. For all their good intentions, journalists rarely understand the science they try to report on.
Because the media decides what is newsworthy, not the scientists. And a "we struck a major blow to cancer today by discovering and/or creating xyz" is just the kind of story that grabs people's attention. 

The ongoing "here are the successful field trials over the last x number of months" follow up stories, however, aren't nearly as glamorous, although no less important (and some might argue even more important). But the media wants to grab your attention and draw you in, ultimately to increase viewership and subsequently sell advertising spots at the highest possible rates...and stories like the latter don't do that. 

Doesn't mean that good things aren't still happening. You just have to dig deep to find out the information.
There are two basic reasons;  


1. The media report is premature or misleading and further research shows that the treatment doesn't work as well as the media first reported.
2. The treatment does work, but once it's been reported about once, it becomes much less "news worthy".   


Honestly in my time working in science, I would say that as far as I recall I never saw an accurate news report on the topics that I studied and understood at a professional level. I don't know how often the news gets it absolutely right, but in my anecdotal experience I would guess that it's the bottom side of 50%. In some cases the scientists are at least partially to blame, by being understandably hyped about their own research. In some cases it's clear the journalists have done little research.
- Cancer is not one disease, but a multitude of diseases. Each type of cancer is very different. For example, blood cancers has better treatment outcomes than solid tumor relatively speaking. So studies that find a cure for specific types of leukemia doesn't automatically mean that they've also found a cure for solid tumor. For example, CAR-T has had great success with lymphoma, but not as effective for solid tumor.

- There have been great breakthroughs. But like I said, a breakthrough in one type of cancer doesn't mean it works for all cancers.

- Breakthroughs in cancer often means extending the patient's life by a few months or a couple years, instead of total cure. There are a few reasons for this. One of them being that treatments work best if given to patients in early stage of the disease. However, patients typically only received the expensive new treatments if they have failed more conventional treatment, and this reduces the efficacy of the new treatments. 

- Clinical trials take a looooong time. It takes years to get approval for any new drugs and treatments. Often things make it to the news because of a successful animal study. Add about 10 more years for when the drug would hit the market.

- The news isn't for you. Now this is a bit of a cynical view on the whole thing... But more often than not, the press release on early successes of a drug isn't written with the generally public in mind. They are for potential investors and existing stakeholders. This is how a company showcases that it has hit an important milestone. Clinical trials are very expensive to run and companies need to continually attract investor funding. This is one way to do so.
[removed]
You should look into drugs like trastuzumab, imatinib, or all-trans retinoic acid. These drugs have revolutionized their respective cancers. For example, Her2 positive breast cancer used to be a death sentence but now has survival rates that are basically 10-20 times higher than they were. Same for imatinib, Chronic myelogenous leukemia was basically a death sentence but the drug slows the cancer essentially to a halt and people live with the cancer well into their 80s and usually die of a heart attack or stroke. And all-trans retinoic acid essentially takes cancerous blood cells that are immature and immortal, usually killing the patient in a year, and basically pushes them passed a cell division checkpoint causing them to mature and die like they would normally as a part of their life cycle. 

Basically take any cancer survival rate from 30 years ago and compare it to today. Every single one is higher, some cancers (like the ones I listed above) went from patients dying from the cancers to the patients now dying with the cancers.
You don’t cure cancer “universally” because cancer isn’t A disease. It’s 10,000 diseases with 10,000 pathologies.

A breakthrough for one might lead to insight in a dozen others if they’re similar but you’re still left with 9,987 OTHER types of cancers that haven’t been cured.
1) Actually several of these breakthrough have reached the clinic and are now used as mainline treatment with, for some, unprecedented increases in long term survival. For example, the 2013 article talks about immune checkpoint inhibitors, which have been approved for several cancers. For examplee, in late stage metastatic melanoma, they have increased long-term survival from >2% to about 40%. Such improvement has never been achieved by a single class of drug in any cancer, ever. Child leukemia mentioned in the 2017 article now has a >95% remission rate, whereas it was absolutley deadly a few decades ago.

&#x200B;

For some other "breakthrough", the drugs are still in clinical development. It takes many years (10-15+) to develop a drug based on a biological finding in the lab.

&#x200B;

Some others have been hindered by toxicity issues or disappointing results in clinical trials. This point is important to remember: most drugs who enter clinical trial will never be used in the clinic, and there can be mulitple reasons for that.

&#x200B;

The media also has a role to play in these seemingly disappointing results, as they often fail to understand the limitations of a specific scientific report, or will use sensationalism for, you know, business reasons.

&#x200B;

2) Unfortunately, it is often difficult to discern facts from exaggeration, especially as a non-scientist, as the only way is to go back to the original research. Many articles will report a new drug that "kill cancer cells", but won't state, for example, if they also kill healthy cells. One indication of the quality of a specific research article is to look where it was published. An article published in a very low impact factor scientific journal is unlikely to revolutionize medicine. But ultimately, a new therapeutic strategy exposed in a Nature paper may very well turn out to be a total disappointment when applied to humans.

&#x200B;

3) Each and every cancer is a different disease, with different driver mutations, not only between types of cancer, but also between patients with the same type of cancer. The body  is incredibly complex and it will often react to a drug in different ways. Another layer of complexity is added by the drug resistance phenomenon. Many drugs will initially induce great results. Unfortunatley, cancer cells are genetically unstable, very heterogenous, and thus very adaptable. They will often find ways to evade the action of a certain class of drugs, and the mechanism of resistance is often very diverse according to patients. Therefore, it is extremely unlikely that a universal cure for cancer will ever be found.
The articles above about the t cell treatment are very much in use now with great succes rates. 
Clinical trials take years to complete . My dad went through this trial which has been approved as regular treatment since late last year (in Europe. its approved in the USA since much earlier).
The t cell treatment does not have a 100% cure rate but many patients who had failed all previous treatment, often with a very high tumor burden, are now in remission.
There are a couple reasons here.

The first is that coming out with a treatment takes time. Once a target is identified it can take a year or two to run the appropriate preclinical mouse and other studies and develop the drug. Then several years more for the clinical trials to run.  For the articles you posted from 2020 and 2019, not enough time has passed to make viable treatments off those discoveries.

The second is the variety of cancer. Cancer is any cell that grows in an uncontrolled way in your body. This means that any tissue type could become cancerous using any mutations that induce uncontrolled growth. This in turn creates a huge amount of biodiversity in cancer which means a single treatment is unlikely to be able to treat all cancers. As an analogy you can think about how one treatment doesn't work for every virus (flu meds won't necessarily work for covid, and hepatitis treatments won't work against ebola, etc.)

The third is that there have been some phenomenal breakthroughs in treating cancer in the last decade in a half thanks to one man in particular, James Allison, and the rise of immunotherapy. But what we consider a breakthrough is not as glamorous as what the general public may consider a breakthrough. Immunotherapy is using the body's own immune cells to fight cancer instead of treating the cancer directly with drugs. And in one of the first trials for a type of immunotherapy called a check point inhibitor, a small, single digit % of patients lived for years afterwards (so long that they stopped keeping track for the trial) and showed no relapses. These were stage 4 patients who didn't really have any chance of survival who then went on for decades now and raised families and were (we really don't like using this word for cancer, but I'm going to anyway) essentially cured. This was a major breakthrough but it only affected a small % of a single type of cancer indication, but the mantra in the industry became, raise the tail (referring to increasing that small % of long term survivors to higher % values). That is a huge breakthrough by our standards, but not anything close to a universal cure.

As far as clues to indicate whether a piece of mainstream media is hyping up or over exaggerating the impact of some new discovery, there's a checklist you can use.

\-Is the discovery they are referencing published yet (as in is there a peer reviewed paper about this breakthrough in a respectable scientific journal)?

\-What stage is this discovery in? As in has it just been performed on cells in a perti dish, is there only mouse data, or is there data in humans? Mouse data and in vitro data (data on cells in dishes instead of in whole animals) are often not repeatable in humans so claims that there's a breakthrough at that stage warrant scrutiny.

\-Is the discoverer making outlandish over grandiose claims? There was an Israeli lab a few years back that claimed to be able to cure all cancers within a year using targeted peptides (a not terribly novel technology), obviously that was too bold a claim to be accurate.

\-Is the lab in question funded by a group with a political agenda? For instance a PETA funded lab making a claim that veganism cures all cancer, or a lab in a dictatorial country making grandiose claims that the state news network can use as propaganda.

There are probably other good metrics that I'm not thinking of right now, but that is a good starting point to weeding out falsely inflated stories about breakthroughs.
[removed]
[removed]
The documentary, [Cancer: The Emperor of All Maladies](https://www.pbs.org/video/story-cancer-emperor-all-maladies-cancer-emperor-all-maladies-trailer/), goes into this quite well.  I highly recommend watching it as it was very eye opening and educational on the realities of cancer.
An important thing to remember is that most of these stories are mentioning a positive result in an experiment. Most often this means that we have found a chemical compound that has reacted in a potentially positive way to a known cancer cell or even a molecule that is known to be present in cancer cells. 

As this is medicine we need to be 100% sure we know what is happening before we can even begin clinical trials. This step can take a long time and is consistently going to be peer reviewed to ensure it is scientifically accurate. After this we can do trails and red tape before production. The whole process usually takes 20-30 years to complete. And that is assuming that the original discovery actually worked.

Sometimes the original discovery is just a piece of a larger puzzle that we might not see the end product of for another 50 years
Two of these treatments you cite are currently in use in the clinic. 

The article from 2013 discusses the use of PD-1/PD-L1 blockade which has become standard of care therapy for patients with certain disease types we know respond to those treatments, but there are also a ton of current clinical trials using these treatments in combination with others to see if we can find an even more efficacious combination.

The one from 2017 is talking about CAR-t cells of which there are many many clinical trials ongoing, the issues with these treatments are: 1. You have to be admitted to have your t-cells removed from your body, it then takes a few weeks to expand and modify them before they can be infused back. (Long process, potential for issues from leukapheresis). 2. Once they’re infused back there is a serious risk for cytokine release syndrome which can be fatal. All in all I think this treatment will ultimately be one of the first choices in the future but we need to work through the bugs and it would also be super helpful if we could use “off the shelf” T cells instead of borrowing from the patient. 

The other articles seem to be incremental laboratory work which will be used to design other experiments which ultimately could drive new treatment options...as others have pointed out, while neat to read about and definitely important discoveries...the advances these things promise are almost always years away because it takes so long to bring a treatment from the bench to the clinic and then ultimately to the public.
Hello! I work for a pharmaceutical company that makes cancer medicines and here is my take:

Often when a new compound that has potential to treat cancer occurs, the discoverers start a new company and start researching. This can be done first in cultures cells in a test tube, then one or two animals that have been engineered genetically to get the kind of cancer the drug is made to target, then the human trials (clinical trials). 
Sometimes the company falls apart, sometimes they runs out of something essential, money. Not enough people invested or the burn rate was to high. Sometimes there’s mismanagement. 
Large pharmaceutical companies will often acquire these companies at a hefty buy out at various stages of research and continue it onward. In some cases, further testing shows that the drug does not perform as well as existing drugs. Sometimes the side effects are too severe. 
In one case I can recall, a drug worked too well, and killed the cancer so quickly it caused the spleen to explode if I remember correctly (too many dead cells to filter at once). Ones a drug I shown to be to risky for human use it’s pretty much dead. 

Just my two cents.
One important factor is that many such articles are about something promising discovered *in mice*. That's the key phrase you should always look for.  Mouse-model results often don't translate to humans, and are only a first step. About a hundred more steps need to happen after that and any one of them can fail. And even the successful cases take many years to see fruition.

As a rule I write off all mouse-model articles as non-news. Those articles' only purpose is to secure grant money for future research or fill a slow news day.
Can I [introduce you to the fascinating Twitter poster](https://twitter.com/justsaysinmice) called "just says in mice."

[Here's the tweeter explaining the point behind the account](https://medium.com/@jamesheathers/in-mice-explained-77b61b598218).. But, essentially, they post pop science stories like "vitamin C cures cancer," then add the tag line "... in mice."

The idea is to point out that the journalist in question took a study in mice and then wrote an article about this being true in humans *when none of the work to prove that has been done yet, and mice testing is simply one step needed to examine whether something works.*

The most pithy description of the issue comes from the medium article I linked above:

> Reporting pre-clinical research as something that’s directly relevant to people in the here and now is like pointing at a pile of two-by-fours and a bag of tenpenny nails and calling it a cottage.

[Here's an example](https://twitter.com/justsaysinmice/status/1260565875322748929).

The original article says:

> **In studies on mice**, researchers found that the combination delayed tumor progression in multiple mouse models of colorectal cancer; in some mice, it caused disease regression. The results were published in the journal Nature Communications.

(my bold).

The title they ran it under said:

> A combo of fasting plus vitamin C is effective for hard-to-treat cancers, study shows

So of course that's misleading. Hence the tweeter added, "in mice."

I'm not saying that's always the case. But if you follow "just says in mice" for any length of time, you'll grow increasingly depressed how often mice research gets picked up with a headline that generalises it to humans as if no more research is needed.

Let's take [your first story](https://www.bbc.co.uk/news/health-51182451) just as an example.

The headline was:

> Immune discovery 'may treat all cancer'

The article body says:

> However, the research has been tested only in animals and on cells in the laboratory, and more safety checks would be needed before human trials could start.

and the scientist in question says:

> Daniel Davis, a professor of immunology at the University of Manchester, said: "At the moment, this is very basic research and not close to actual medicines for patients.

But, unfortunately, "some indications may be available for a new line of research but even if it works, it would need years of study to pay off" doesn't make a good headline.
3 things come to mind:

1) cancer is not a disease like flu, it's just a general term for a variety of symptoms. You can't "cure cancer".

2) from invention to market release a new therapy takes about 12 to 16 years and around a billion dollars. Only 1% of drugs that are invented make it to the market for various reasons.

3) journalists don't know shit about the things they write about so it's often just the case that a scientist says "this might help in fighting this particular kind of cancer" and some journalist writes "breakthrough, will cancer be healed tomorrow?"
1) Not per se. It all boils down to what the specific circumstances were for that particular drug and or treatment to be considered a “cure”. Many drugs work wonderfully against cancer and other devastating diseases when tested in cell culture or animal models, however, as soon as you try with humans they fail miserably. If the story is picked up before they do human trials they are essentially reporting a very promising drug that fizzles in humans. It is not possible to predict how well a new drug will work in humans; you need to do clinical trials in humans (tens of millions of USD and years).

The opposite is also true, there have been many cases when a drug showed no effect in animal trials but had significant effect in humans (thalidomide comes to mind). This is both good and bad. An older review about this can be found [here](https://www.bmj.com/content/334/7586/197?__cf_chl_jschl_tk__=a8572afe2d36c799cfcb3b144f7484a5a207c74e-1591672027-0-AfUFHNdFdByNFJLvVE5KJmZUKqEbvFNUsfoHMFN4EvEwWAf4M3nfmVMV3wmztvwcAOjzq-3-UPSfhqq34WU8Nt25YgGZwcNLFoSIF3QIeys0gMMwtdWQosCeE-4XOEVBm9645YEbyWgm5rnbhokRnY4gyd3HF2nH2BsWYPscgo7wnnaTmgHatrhF-_STCNKSVusPfPdLuN2OBPlmJm2qCuBgA6vgH0CRZcf0uEIyX0X0mReKCzrtuTgxuhWxqPLzlD8D6xqN3fkD0eD5n9ZYqg6CfOgVKCbQoHw7E7VOpD5G) .


2) As a simplified rule of thumb any study that is [blinded](https://en.m.wikipedia.org/wiki/Blinded_experiment) presents more accurate data than those that are not. This is a gross simplification, but one that makes life easier. Another indicator for the reliability of the results from a study is the stage of [drug development ](https://www.fda.gov/patients/learn-about-drug-and-device-approvals/drug-development-process)  the study was conducted, e.g. if someone claims they cured a disease in animal models vs claim of stopping progression of disease in humans. If both studies were scientifically rigorous, then the human study has far superior value.

3) Cancer itself. As was explained by others, there are many types of cancer. Each one is different and manifests differently in different patients. Some cancers can be grouped together by their disease development, location in the body, or effects; members of such groups may be susceptible to the same treatment, but different groups may not have any commonalities. In fact, there are treatments that work against one type of cancer but promote other types. Think of it like this (super simplified example): when you have a headache, drinking a glass of cold water often helps. If the headache is caused by anything other than slight dehydration, e.g. you bumping your head on the cabinet, drinking water will not help at all. Similarly, treating cancer requires that you know exactly what the cause is and how to treat that specific cause with a targeted drug. 

Now, having said that I have to mention that radiation works equally well on all cancers that are localized (non-metastatic) and that have clear physical representations. However, we seldom tolerate radiation therapy well since our healthy cells and tissue suffer as much damage as the cancerous cells and tissue.
[removed]
Some cancers are already treatable with immunotherapy. That is a field with a lot of breakthroughs. The problem is some of the immunotherapy drugs have vicious side-effects on other parts of your body so its going to take some time to refine them. Source: my S/O is a Cardiologist.
These are all such loaded questions. Each one is incredibly nuanced and difficult to answer in any sort of generalized manner. 

>Is it exaggeration or misunderstanding on the part of the scientists about the drugs’ effectiveness, or something else? It makes me skeptical about new developments and the validity of the media’s excitement. It can seem as though the media is using people’s hopes for a cure to get revenue.

A lot of trials end up failing because the clinical trial pipeline is designed to test safety first, then efficacy, then effectiveness. What we care about in terms of curing cancer is effectiveness. So companies can spend years in the Phase I and II trials looking at safety and efficacy profiles and making sure that their drug passes through these phases. Phase III concerns effectiveness, which is a larger scale study in patients under scenarios more closely approximating the real world. Contrast this with efficacy, which is whether the drug works in a smaller population of patients under highly controlled conditions. Drugs can fail effectiveness trials and thus be worthless. 

Science in general is also about hyping up results. It's simply the culture. Once a scientist makes a discovery, it is to their benefit to hype it up because that's what generates additional funding and that's how you publish. Obviously pharmaceutical companies are less interested in academic publishing (although there has been a recent trend towards increased funding for this) but they still hype up their drugs. 

>While I know there have been great strides in the past few decades, how can we discern what is legitimate and what is superficial when we see these stories?

Only time will tell. When people say something that sounds too good to be true ("I've cured cancer!") always take it with a grain of salt. If you do that, you'll never be disappointed. And statistically speaking, you'll be right most of the time. The only surefire way is to dig through the scientific data (which isn't always readily available) with a critical eye. 

>What are the major hurdles to actually “curing” cancer universally?

The issue is at the heart of what cancer is. Cancer is a disease of your own cells that start to grow in an uncontrolled manner. It's not a disease that you can acquire from other people (except for specific cases like HPV-caused cancers) and thus any therapy that kills cancer cells will almost inevitably kill your own cells. The idea behind most chemoradiation is to kill the cancer cells before the treatment kills you. More modern treatment options like many of the immunotherapies target markers that are only expressed by cancer cells so they reduce the non-specific targeting of your own, healthy cells. These therapies are still being developed and refined and so far, haven't yielded a universal cure.
Scientist to journalist (dumbing a discovery down as much as they can).

Journalist: "Could it potentially... cure cancer?

Scientist: "mmmmaybe, but there's loads of research to be done first, and that could take a decade or more"

Newspaper headline: MIRACLE CANCER CURE!!!

Scientist panics and tries to put the real story out, while the public goes nuts over the story, repeating it wherever they generally gossip.

The scientist finally talks to someone credible, and puts out the real story five years late.

Karen: "Pffft, what would scientists know?"
A lot of extensive and expensive research is also done ex vivo (outside of the human body, ie in laboratories) only to find that they don’t do well in actual patients. The First Cell by Azra Raza talks about this (and alot more!) at length if you want a good read!
The way funding for research changed years ago. Now 'popular' research has a high chance of funding. Also there is pressure on researchers to communicate successes. So we are hearing about potential avenues sooner in the process. Years ago, the same research was never heard of until it actually panned out.
I disagree with your premise. A good friend of mine is being treated for Hodgkin's Lymphoma at the moment.  In the 1950s, the 5-year survival rate for him was 30%.  Today, it is 86%, and it's not unreasonable to think he'll outlive me. 

Don't think that just because the cure isn't one little pill that we haven't made huge strides.
The problem is that there any many types of cancer. A breakthrough in one form of cancer may not be applicable for another form of cancer. In addition, while a cancer of a certain organ be be classified by that organ, there are different cancers that could be affecting that organ and such a breakthrough might only be applicable to one type of cancer. 

Cancers also don't always respond to their treatments, which complicates matters even further.
Cancer isn't a disease, it's hundreds of different, if related, diseases. 

Progress often comes gradually so we don't notice until we think about how mortality rates have changed over a few decades. For example, when I was a kid, leukemia was a near-certain death sentence for almost all of those diagnosed. Now the average 5 year survival rate is ~63%, with many living much longer. Over time these numbers tend to slowly climb.
1. Science “media” is basically just clickbait trash that way under-reads and over-hypes every paper they report on. 
2. Find papers and studies published in actual, peer-reviewed publications. They’re not exactly page-turners though— so be on the lookout for reputable curators and science communicators who do the homework and report honest facts. 
3. “Cancer” is not one disease process— it’s hundreds of different ones... so curing it universally is about as simple as curing death itself.
[removed]
You will never cure cancer in a way that it will not even happen again.  Cancer is a disease caused when cells divide uncontrollably and spread into surrounding tissues.  All sorts of things cause this and you are probably having a tiny bit of it somewhere in your body right now.  Most of the time it never progresses to the point that it becomes a problem, sometimes, and particularly with exposure to certain things that encourage it you will have some get to the point it causes a detectable change - that is when something is diagnosed as cancer.

Someone may eventually find a treatment that handles most cancers effectively, bit it will always exist on a small scale.

The media loves clickbait, and researchers love to label things promising, that is always part of it.  Some are effective some are not, some cannot be reasonably mass produced, some have side effects more terrible than the cancer, some while novel are just nor more effective than existing treatments on average.
One thing a lot of people aren’t mentioning is that drug development can take over a decade. When you hear about big discoveries in the lab, it can take 10+ years, sometimes more, to develop and verify an actual pharmaceutical.
One thing that I dont see mentioned here is time. Anticancer drugs typically take more than 10 years to go from concept to FDA approval. Take PD-1 inhibitors for example: the first published papers on PD-1 inhibitors for cancer date back to 2001. The first clinically approved PD-1 inhibitor was approved in 2014.

You may see a news story about a cancer being cured in a petri dish somewhere, but you would have to wait over 10 years to find out if it can actually make it to the clinic.
[removed]
Pop media loves to report on preclinical studies, and then seems to lose interest as drugs move through FDA clinical trial process, in which most drugs will fail during the early stages. Many do get to market though and there are breakthrough drugs every year that have continued to change the paradigm of cancer treatment and increase survival. Phase 3 trials and on market drugs are usually for very specific indications and often nuanced efficacy improvements like "increases progression free survival of metastatic colon cancer after failure of first round of therapy over typical second line treatments", which is not as flashy for pop media as "amazing new treatment for colon cancer!"
The 2017 article is talking about CAR-T-cell therapy against some forms of leukemia. These therapies was approved by the FDA (and has been approved in many European countries since) in 2017-2018 with the names Kymriah and Yescarta. There is several reasons it's not used in plenty. The cost of one treatment is ~300000$. And it is usually "last line defense", after other treatments have failed.

The reason it's so expensive is every "batch" has to be tailored using the patients own T-cells. So you can't just make a bunch and keep it on ice/storage. 

Research is being made into making CAR-T-cells that anyone can take.
You might want to read/watch a book/documentary called "Cancer : the emperor of all maladies", which is a biography of cancer, how it was discovered and how the treatments improved. The "early years" (which are actually up until the 1980's) are actually rather gruesome. But what this documentary well illustrated IMHO was the "russian dolls" nature of cancer(s). Every single time one major breakthrough was found, from the first chemotherapies to the most recent immunology-based ones, while it did cure some cases it mostly uncovered how complicated the disease can be.

One particular part I remember is when genetic sequencing was first used on some cancerous cells. The first result was "yay, we found one gene that is defective in tumor cells, we're much closer to a cure!". And then when more tumors from other types of cancer were also sequenced, it appeared that for these it wasn't just one gene, but two, 10, or even 50.
There are T-Cell therapies approved for use by the FDA. Their use is gaining steam. They are for specific cancers though. The problem with cancer is there is no cure-all yet. 


Yet.


There are amazing things being accomplished in cancer treatment. A lot of it is just clinical trials until proved efficacious.
All great comments - I'll just add that progress has been made in some types of cancer but often underappreciated is a key element - cooperation on protocols.

There is a type of kidney cancer called a Wilms' tumor. Once the mortality was in the high 90% range, but over 50 years of systematic work - it has now inverted the statistic so survival is much better in the lower stage disease. A key part of this success if the cooperation, sharing of results and uses of protocols. There are about 500 cases in the US each year, yet they have made huge progress because nearly every patient is part of the research study effort.

 [https://en.wikipedia.org/wiki/National\_Wilms\_Tumor\_Study\_Group](https://en.wikipedia.org/wiki/National_Wilms_Tumor_Study_Group) 

 [https://en.wikipedia.org/wiki/Wilms%27\_tumor](https://en.wikipedia.org/wiki/Wilms%27_tumor)
Numerous cancers have gone from practically 5% survival rates to +50% due to many of the breakthroughs you hear. Some cancers are over 90% curable these days.

Unfortunately there are SO many types of cancer that there will never be a suitable blanket cure for all cancer. You may cure breasts cancer but the treatment may not work for brain cancer.
So there are a lot of factors that go into this the main two I see are this. The level of understanding of journalists and translation from in vitro (in a test tube) to in vivo (in an animal model).

Most journalists sensationalize results either one for more views/clicks or purely out of scientific ignorance. In a lot of cases they have no more science background than your average college graduate. There’s also the invitation of research bias when they interview lead researchers. Researchers of course will be very proud of any breakthrough because it generally takes years of work and a little luck. This can lead to some exaggeration from excitement when they speak on a more personal level.

The difference between in vitro and in vivo can be massive, especially working with cancer cells that are already working differently than healthy cells. Whatever fueled the breakthrough whether chemical drug, protein manipulation, or immune modification could potentially still be toxic even deadly on an organism level. There’s also the possibility that the breakthrough simply won’t do anything at the organism level due to metabolism and clearance of the product before it can make any meaningful physiological difference. Finally we come to funding and time. Millions of dollars and several years most likely went into the in vitro studies, it will take millions more and hopefully a little less time(still years though)to get through in vivo studies, and then millions more and at least 5 years to get to actual drug trials. In a perfect world those funding hurdles wouldn’t matter, but that’s not where we live.
Lots of good answers so I'll just highlight a point on funding.   


It's super easy to get excited about what happens in cell culture/tissue/mice/etc. But while they're gross simplifications of what *might* happen in a person, I'd argue that it's important to sensationalize to some extent.   


Why? Because funding. Even if grant reviewers don't buy the sensationalized media, the people influencing that grant money availability - like congress or their constituents - *will*  be affected by the media. I believe it's better to cast the results in a positive light so the larger community can benefit for (a) continuing and expanding research funds and (b) so that 0.00001% chance of a treatment making it to FDA-approval improves that much more - not because of poor regulation but because of good science, good statistics, and good compatibility with existing treatments.
Many have provided good and solid points as to why this occurs but another important factor is risk mitigation. People like to throw the "companies don't want to find a cure because it stops revenue" argument when trying to discourage medical research companies but another thing they seem to omit is that sometimes it's just not worth the fallout when compared to the success rate. 

Ive seen case studies where a new treatment is made available to a select few patients but the results aren't up to the standards that the developing company was expecting. For example 30% of patients recovered while 70% of patients did not, and in those 70% variables that are generally not seen with the underlying illness were present. 

So yes, 30% survived but the risk of having to fight those who did not is not worth the hassle.
None of these links go to the actual peer-reviewed papers written by the scientists.  This is journalism hyping up science to increase their audience.  I tell people don't trust anything scientific or medically related you hear or read from the media.  Ask a real scientist or physician if you want to get an accurate assessment of the field.
It's not just cancer. It's what the medical industrial complex promotes and the media cooperate in. A breakthrough headline, a little razzmatazz about cures, and then at the very end, well, it's in stage x of some protocol, and will be in clinical use in 10 or 20 years. Then you never hear of it again, but you do remember all the fine people who brought you the good news.
One thing I haven't seen mentioned yet is money. Some of these new treatments aren't worth investingn in because either the form of cancer they treat is rare and therefore developing a cure wouldn't make for a solid return on development. Or they won't make enough money to appease their shareholders.
A lot of people have stated the science behind "cancer" and that it's not a singular thing.

I'd also like to point out the flip-side and comment on the "reporting" of things.

Media, whether it be News Media Channels, Online News Websites, thrive on clicks, views, and ratings. They are a revenue based business like any other business in America. While they try to attempt integrity with their reporting, they almost always carry some sort of bias, either towards a view that will attract specific audiences (MSNBC, Fox News towards their respected demographics for example) or they will try to sensationalize or use other tactics to get you to click. So maybe what actually happened in the world is a minor-breakthrough but a news media might sensationalize it to get you to click and read the article... because that's what's good for THEIR revenue even if they are slightly misinforming you of the bigger facts.

Just a perspective. Not the end-all-be-all of this debate, but I think what I stated about Media defiantly holds true for a lot of things.
Things that kill cancer in a petri dish but are not registered for use in humans:

1. Bleach
2. Bullets
3. Uranium
4. Plutonium
5. Just about anything

As to why we don't have a universal treatment: Cancer refers to a plethora of mutations that derail the normal cell cycle, not just one. We have an ever evolving arsenal of cancer drugs for specific mutations, but even when you have these very specific drugs available, targeting them is not always easy.

Think of a cancer as of a tree. Every new case starts with a mutation - the trunk. It then gets more and more mutations - the branches. We target the mutations we think are best to stop the growth. But if we weren't somewhat lucky and we managed to hit that one mutation common to all cells in that instance of cancer - cut the trunk, there will be new branches sprouting and the cancer will come back.
Because the pharmaceutical companies don't want a cure. They will make far more money on a treatment(i.e. a battery of drugs you will have to buy and take for a long time) than they would for a cure(i.e. a one time treatment that would eradicate said cancers).
Are you an oncologist or cancer researcher? No? That's probably why you don't hear of them again. People in those fields do and it becomes part of their norm. There's no reason for the general public to hear of it again. It's no longer exciting. Btw, don't confuse what scientists say and what the media says- it often goes like this: 
Scientist: so this will contribute important knowledge to the field of breast cancer in patients who failed first line treatment
Media: MAJOR BREAKTHROUGH FOR BREAST CANCER PATIENTS, NEW HOPE FOR DYING PATIENTS

Understand that even major breakthroughs are still tiny incremental steps. Many drugs that took thousands of these steps to get approved may only increase survival by an average of few years or months compared to current treatment with some types of cancer.
 
But before you get depressed- we've made some huge leaps. Immunotherapy, gene therapy, etc- these things will change/are changing modern oncology and there's a lot of room to grow there. Cancer is enormously complex for many reasons, not the least of which is that we're trying to take out our own cells without killing ourselves. I don't see a cure for cancer in my lifetime, but perhaps a steady state of constant surveillance to keep your cancer in check, much like some of the less deadly types are now. But probably not anytime soon, not for all types.
[removed]
[removed]
TLDR:

A lot of experiments look extremely promising in labs under controlled conditions. And once larger scale testing starts being performed the flaws start appearing. A “miracle” cure that works in an isolated scenario suddenly doesn’t work in live subjects.   Sometimes in clinical trials the side effects of a drug prove to be more fatal than other methods of treatment, or it has significant side effects that weren’t shown prior to clinical studies.

In some ways medical science is a bit of a “throw it at the wall and see what sticks”.  As soon as you do a clinical trial you’re throwing it at the wall, and no number of promising pre clinical results can accurate predict exactly how a drug will react to humans with cancer.
[removed]
In addition to what many others have said, there can also be a very large gap between when something is first discovered and when it gets to the market. Results must be verified and repeated, go through animal and human trials. It can cost millions of dollars to create the first batch of a drug so scientists have to both prove that it is safe, works well enough for it to sell, and be able to produce it cheaply.

For example we have know for a few years now that psilocybin (the active ingredient in magic mushrooms) can help treat depression and PTSD, but (as far as I know) no one has gotten a prescription for it yet. This is in part because we have yet to find a cost efficient way reproducing psilocybin in large quantities.
[removed]
As a patient who has benefited from a breakthrough chemotherapy medication, I can tell you from first-hand experience, they are out there in the world and having enormously positive impacts in the cases where they apply. The medication I’m on has turned my diagnosis of chronic myeloid leukemia from a likely death-sentence to a moderate but manageable inconvenience.

There are a lot of reasons it feels like we aren’t making progress with these breakthroughs, even though we very much are. The biggest one is that cancer isn’t a single disease, but rather a category of diseases. There are a ton of different cancers that affect the body in different ways. My particular type of leukemia cells are fortunately caused by a single, simple mutation, which allows the therapy to work. However, solid tumors like breast cancer, and even other leukemias, are much more complex.

Another big factor is that the announcements of these breakthroughs tend to be over-simplified by news articles. Breakthroughs in medical research often come years if not decades before that breakthrough can be used to create actual therapies. 

Plus, there is no guarantee that the breakthroughs written about will be viable in actual treatments. For instance, if you see a breakthrough on a study in mice (or really any other non-human animal), there is a distinct possibility that the finding won’t even apply to the human body. Even if a treatment does work, there may be side-effects and complications that require it to be pulled from the market or limited to treating a very small number of conditions. I was on one drug for several years that actually got pulled by the FDA because it caused severe heart issues in many patients (yikes!!)

But with all that, I’m not the only person I know of who has survived and thrived thanks to ever-improving cancer treatments. More and more cancers are becoming chronic conditions rather than lethal ones, if not being cured outright.
In all subjects, medicine, humanities, STEM, to really get a clear picture you have to read the research. The media will never give you a clear picture, and you might have varying success with orgs of experts that do outreach.

I think the problem is expecting education to come easily.
Newspapers need sensational pieces for clicks

Researchers need sensational news about their research for continuing grant money and funding

The grad students working for those researchers need the sensational pieces for kick-starting their careers

You, the reader or the cancer patient have no role in this dynamic except clicking those links to those sensational news, so you do your job and don't get anything else :D
On a sidenote Media really likes to link peoples work to something relatable like cancer that people feel is important, almost everyone has a relative that died from cancer. I myself got an article in the local newspaper that implied that my work would cured cancer when in reality I had been working on tools that could be used in cancer research.
The media. They love to overstate things. A good example of this occurred at my university where a researcher made a breakthrough in treating infertility. The media jumped on board and made it out to be a cure all for infertility. The researcher was almost immediately drowned in emails, phone calls, letters etc asking for her to cure them of their infertility. This got so bad that she ended up changing her name and moving states. The breakthrough had been in a very specific condition and only in mouse. The same occurs with cancer breakthroughs. While these kinds of articles are great for generating interest and getting funding, the public must be careful to not over react and let the researchers do their work or else you could end up stop the work.
Yeah same thing come to my mind too. Some years ago i watched a very informative video which showed the use of CRISPER in cancer treatments, can be used to kill virus and was very useful in many other ways too but now that there is Covid-19 going on i didn't hear anything related to CRISPER being used to treat Covid-19 patents only heard its being used for fast testing corona positive.
A basic answer would be that the report says "this has the potential to change cancer treatment" but the way this is reported back to the general public is without the part where it also says "this depends on a great deal more research." That research is done, with less publicity or fanfare, and the results prove less promising at a larger scale. This is less likely to make it to the general public.
I think a lot of the problem stems from how they’re reported. A group of scientists will write a report saying they’ve found some treatment that’s been proven to be say, 70% effective on one type of cancer in mice, then the news reports “genius scientists find cure for cancer!”. Everyone gets excited then when it never shows up people blame the scientists.
Aside from all the good answers here, nobody has mentioned money.

Researchers need to produce results or they won't get more funding.  So they are incentivized to report any kind of finding to get press coverage for their team and institute to retain current and attract new funding.  Funding is limited, so people will throw money after whoever is getting results, and they will also pull money from teams that aren't showing any progress.
A lot of what we hear about in the public is misreporting from news outlets.  The Daily Mail is famous for its causes/cures cancer headlines.  If a paper suggests a marginal benefit in a petridish of one ingredient also found in chocolate and the headline is "chocolate cures cancer".
reason number 0 : Science journalism is a joke.

The cycle usually goes:  Pretty exciting research is due to come out -> University PR department gets involved and writes a media summary (already overstating the results) -> journalists write news based on the PR text, maybe with an interview with the researcher -> Clickbait title is chosen in order to generate clicks -> Pop science websites that live out of Facebook clicks make it into even more outrageous claims and clickbait.

It is a whole wheel that academics don't really participate into very often, but are probably not fighting against it either.
1, they are often sensationalized by news media that doesn't understand how to report on science in a nuanced way.

2, sometimes researchers themselves will draw conclusions (that get reported on) that aren't supported by the data.

3, the above two things happen for lots of science, and cancer being a popular topic, it tends to get reported on a lot.

4, often these headlines are things like "such and such kills cancer cells in culture!" And treating cultured cells with something to kill them is very, very different from giving it to a live human and having it kill just the cancer. I mean, I can spray some bleach into a flask of HT-1080's (a human fibrosarcoma line) or HeLa's and kill them, and you could probably find some gullible sucker out there and tell them "hey, bleach kills cancer cells in culture" and they might start going "well bleach kills cancer cells, why don't we use bleach for chemo?"

5, these things are often discovered by academic researchers, and turning an academic discovery into an FDA (or equivalent) approved therapeutic product takes years and years of work on everything from determining whether it's actually effective by use of double blinded placebo controlled studies (or whatever is most ethical, I don't know how ethical it is to give cancer patients a placebo, I'm not really involved in the clinical side of therapeutic development, sorry), determining whether it's safe, developing a shelf stable formulation, developing a manufacturing process that creates a potent and pure drug substance, and much, much more, and there is very much no guarantee that the candidate ever makes it that far down the pipeline.

There's probably other reasons too, but these are the main ones I could think of from my perspective in biotech.
It's not just cancer research. There are often a ton of cool inventions or discoveries which never seem to get traction in a timely fashion.

https://www.independent.co.uk/life-style/food-and-drink/edible-water-eating-ooho-skipping-rocks-lab-no-packaging-plastic-pollution-world-h20-a7682711.html
>Is it exaggeration or misunderstanding on the part of the scientists about the drugs’ effectiveness, or something else?

Most of the exaggeration and misunderstanding comes from the media and their interpretation of the scientific papers. 

Journalists often have neither the academic background to understand research papers nor to they have the incentive to accurately report what the research paper states. 

In your examples, you are just citing news sources and blog sites. Those are not scientists and therefore not scientists saying anything. These are *people that are saying scientists said something*. 

To know what the scientists were actually saying, you need to look at the source. The scientifically published paper that they are all citing. 

In those, you probably won't find the bold claims anywhere that the news reporters come up with.
Researchers over-speculate as to the importance of their findings in order to have a chance for more funding, peer review is not sufficiently probing, very few results are independently verified, many breakthroughs are diagnostic rather than an intervention.
To add to the other comments: What you mentioned absolutely SHOULD make you skeptical of new advances reported about as ‘breakthroughs’ or ‘milestones’. Often these fail to hold up due to reproducibility or scalability, the latter being a huge obstacle in the adoption of new technologies. 

And even if an advancement holds up in further studies, the way to mainstream adoption is long and not as exciting to report about.

There is a general tendency in science reporting to overstate things. Often the reported findings and predictions are even much different than what the authors write in the original study.
It may take decades, and hundreds of millions of dollars or even billions In Investments before a discovery is commercialized into a product. 

Typically then, even though the task was monumental, news will rarely report on these commercialized products when they are available because the TV networks want $$ for pharma TV ads. They aren’t going to provide free advertising !
Often, there are promising drugs or treatments that are very effective in mice or against cancer cells in a petri dish. Many such treatments don't work in humans because of the differences between humans and rodents, and/or side effects (most compounds which kill cancer cells are dangerous to healthy cells as well.

Scientists will usually fully disclose all such caveats and limitations when publishing their findings. The press will often omit these pesky details and latch onto the most promising aspects of the study. As the story goes mainstream, the headlines get even more sensationalized.
The real answer is becuase science/medicine journalism sucks AND the emphasis in science of being first.

People are yearning for new tools, when the results are in tissue culture they are at best preliminary.  People (non scientists) focus on the ability of drug X or treatment Y to kill cancer cells.  Killing cancer cells is easy ( he'll even ultraviolet light and sanitizer will work) Selectively killing them in a 3D model is much much harder.

For some cancers we have done a really good job, for others the progress has been non existent. The emphasis of biologicals will hopefully move us away from the big three ( burn it, cut it, poison it) but the path is slow.

Although it is not written for 5 years olds, get a copy of " the emperor of all maladies".  It does a very good job of explaining where we started, how we progressed ( shudder!) and a little about where we are hoping to going.
Former zookeeper here: I’ve only personally worked with lesser apes, but from my observations they either squat to use their urine to scent mark, or just pee from their perch. No hands involved. Ring tailed lemurs (not an ape, but a tangentially related fun fact) will pee on their tails, then flick the scent at each other. 

The rest of the great apes have much much smaller penis to body size ratios than humans do. Gorillas are only about the size of your pinky. Also unlike humans, most apes have a penis bone, which means their flaccid penis is much less “floppy.” So humans have a much different penis anatomy than other apes, which could lead to the behavioral difference of holding it while you pee.
Chimpanzees caregiver here, I have never seen any use their hand to pee. Most of the time they just pee off a ledge.  I have seen one pee into a bottle before but he didn’t use his hands he just brought the bottle to the stream.
Human male penis size relative to body size is VASTLY greater than other apes (and almost every other mammal). This is likely due to a number of reasons, although it’s important to understand that physical evolutionary traits are not being governed by some conscious force trying to get optimal results, so it could just be a random genetic mutational accident that we have since compensated for (as long as it doesn’t seriously inhibit reproduction then a trot won’t be selected against).

First, is that we have a very low degree of sexual dimorphism compared to other apes (and many mammals in general). A male gorilla with a penis the same proportions as their body compared to humans would potentially cause a lot of pain from ripping and tearing the relatively much smaller female gorillas, for example (especially considering they don’t generally give the female any time to “warm up” and get the natural lubricant flowing).

Second, female sexual pleasure is an important part of human mating, and encourages more frequent reproduction opportunities compared to animals that don’t have this trait. Other traits that contribute to this effect in humans are frequent menstrual cycles concealed estrus. Instead of a yearly period of “heat,” human females are available for insemination at regular, frequent intervals. It’s apparent in other species when the female is fertile, but human females do not exhibit engorged and discolored labia, nor do they exhibit strong pheromones during their period of receptivity to being impregnated. All of this contributes to increased mating opportunities. It also contributes to a confusion about paternal parentage, which seriously reduces the amount of babies killed by dominant males that only want females to spend their time and energy caring for their children (since any of the kids given birth to by a female they have mated with could be theirs), which would otherwise be common, and is still frequently seen when males from outside a group take over that group (they can safely kill all the kids because they definitely aren’t theirs). But that’s not relevant to the current discussion; I just wanted to point out how these things aren’t only related to the same root causes as increased penis size, and how they may not really be related to penis size at all. Some of these factors are also present in other primates, not just us.

Third is the phenomenon of a sperm arms race, due to the aforementioned incentive for frequent copulation of females with multiple males in the same general time period. When multiple males inseminate a female around the same time, the one that gets their sperm the furthest in has a better chance. Ejaculation velocity and sperm “swimming” speed are also important, but sperm can be relatively slow to get going through the first parts of the vagina, so those things being equal, a longer penis equals enhanced reproductive capabilities. Gorillas, for example, typically only have one male attempting copulation within their harem at any one time, so there is no need for sperm competition.

If you look outside apes to other primates, there are a bunch of monkeys that have similar penis ratios, likely for similar reasons, so we’re not exclusive with this particular trait. We do have very small testicles relative to our body weight, compared with other apes, but I haven’t studied the reasons for this as much so I’ll just leave it at that, except to say that chimpanzees probably have even more sperm competition than humans ever did, but they don’t have larger penises, and that’s probably because their testes are over 33% the size of their brains, while ours are a paltry 3%. This is because their solution to this sperm competition is to use their massive testicles to copulate with females many times a day, every day, whereas if humans attempt this, their sperm count drops drastically after only two ejaculations a day.

There are a variety of other factors to consider, such as our tendency towards monogamy and how it clashes with some of these other theories. We could have moved closer to monogamy relatively recently, but conflicting with that is the fact that Humans have remarkably smooth penises, which is generally found in monogamous species. Polygynous species usually have odd proportions such as huge heads that act as a plug, or bumps and ridges and extreme curvatures, etc. Human males also are quite a bit bigger than females, just not as much as some other species.

Edit: I meant to respond to the other comment about this, not the main post.
[removed]
[removed]
[removed]
[removed]
Follow-up questions: When did we start doing that? Since we designated holes to piss in, or before? I mean I would suspect the only way to have any idea would be from a well-preserved, old-ass cave painting of a dude draining his pisser lizard, right? Would that have been a thing back then? Like, early piss porn or something?
Biological anthropologist here 👋🏼 this is an excellent question! While a lot of the answers are (for the most part) plausible, I noticed there’s an absence of citation. I know this is an informal platform but some peer reviewed journals would be nice to refer to! 

While my interests do align with reproductive ecology, I don’t have enough experience with this topic to give a thorough explanation but I think if you also posted this on r/Anthropology you’d get some insight as well
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
Bloodsucking insects will become engorged after feeding. Mosquitos, bed bugs, lice, ticks, fleas, ticks (arachnid), are visibly larger after feeding.

Perhaps more relevant is the Fat Body.

Insects have an organ called the Fat Body. Lipids (fats) are stored here in adipocytes in the form of triglycerides (same way mammals store fat, essentially). These lipids are consumed during periods of high energy demand (like when flying), and are replenished in periods of food abundance. 

Some insects have been shown to increase the size of the Fat Body in the winter, as a mechanism to enhance survival. Other insects (like house flies) don't seem to be able to store extra fat. 

edited to add some sources: 

Study on fat storage in Culex pipiens: https://www.ncbi.nlm.nih.gov/pubmed/2769712/

review paper on the insect Fat Body:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3075550/
It depends a lot on the insect in question.

The most extreme example I can think of is the [honeypot ant](https://en.wikipedia.org/wiki/Honeypot_ant). Some members of these colonies become living food storage for the rest, hanging in place and taking in or giving out their stored reserves as needed. 
[removed]
[removed]
Many insects have a part of their lifecycle dedicated to collecting and storing energy and another part where they can't collect energy, and can only live off of their stored reserves. Lots of species of moth spend their larval stage eating, then form a cocoon and emerge as an adult without a digestive system at all. Their adult stage is dedicated to mating and laying eggs, which will hatch into larvae and repeat the cycle.
Not sure whether you're still interested in yet another person telling you, but here I go:

Insects have an organ called the fat body, which is located on the ventral side of the abdomen. This is a kind of functional equivalent of the liver and is also used (as the name suggests) to store fat as an energy reserve.

In our experiments, we could always very clearly see the difference between fat bodies of starved vs non-starved locusts.

Fat isn't only stored in the fat body though. While that's the main site of storage, you can also see fat in all body cavities.

The reason why an insect doesn't get literally fat is because they are often filled with air sacs, filling the body cavities. When the organs grow (because of fat storage, production of eggs,...), these air sacs simply deflate and become smaller.

Finally, there is a large difference between insects, and logically also a large difference in the time you can starve an insect. Our locusts could easily make it two weeks without any food. Several of them would make it over a month.
I work at an Insectarium. The answer is a resounding YES. We take very good care of our arthropod guests and some of them don't get the amount of exercise they would in the wild. Currently we have an obese praying mantis, tailless whipscorpion, and centipede, just off the top of my head. 

All arthropods have areas of membrane between their sclerites (the panels of exoskeleton) and when they have so much fat body (insects equivalent to our fat storage, it also does other things) that their sclerites aren't anywhere near touching anymore and their semens of membrane are stretched thin... they are fat. I'm pretty sure we have some that would be in the obese range if there was a scale for insects.
https://www.youtube.com/watch?v=VqMijWK9ZOc

This video (particularly around 10:30) shows the fat on a mantis very clearly.

https://www.youtube.com/watch?v=pMbQ4-mrsZM

This one shows a different mantis with much less fat, though still has some. I would say that both of these videos are fairly useful for understanding insect anatomy. Good visual aids.

Point being, insects do store fat within their exoskeleton.

As for how long an insect can go on its reserves, that largely depends on the species. Some insects, like cockroaches, can live for multiple months without eating anything at all. Other insects must eat much more frequently.
[removed]
There seem to be two main ways insects store food - by storing the liquid in their digestive system, or by incorporating fat into their system. The liquid method seems very popular in social insects, particularly ants and wasps, because it can be regurgitated at a moment notice to share with nestmates. 

[I remember reading](https://thesmallermajority.com/2013/09/12/mozambique-diary-the-fat-coneheads-of-gorongosa/) an post on fat katydids - true fat stored in fat cells. Fat is not as clearly visible in insects as it is vertebrates, but some species that have to undergo hibernation or periods of starvation do use it to their advantage, and they definitely are heavier and fatter than when without their fat stores - insect exoskeletons are a lot more flexible than people give them credit for.
Nematode biologist here (not an insect, but may be informative nonetheless):

Even very simple animals, like a 1,000-celled nematode (roundworm), store fat.  In roundworms, many of the proteins that synthesize fat are similar to those in humans.  Also, some conditions, like low food availability or high population density, cause worms to enter hibernation, and these animals store more fat to compensate.  So yes, very simple animals can store fat and use it as an energy source in a similar way to mammals.
There are several answers suggesting that insects store fat in the same way humans store long chain hydrocarbons.  The insect process is different.  Please read the attached article, specifically the part about vitellogenin in insects.

http://www.sciencedirect.com/topics/neuroscience/vitellogenin
[removed]
They definitely can get fat. If anyone here keeps reptiles you might know about gut loading, when you essentially keep inverts and fatten them up before you feed them to the reptile to give them more nutrients in their diet. 
I had a praying mantis. Got pretty fat. Was a happy mantis. But only the abdomen swells. They get to their normal weight some days later. Takes a lot less time then for humans and doesnt really harm them.
They can eat nearly their one body weight at once. 
Acutally had 2 after another and the guy that followed at an adult argentinian wood roach which is about 4-5 cm in lengh while hanging upside down in a timespan of pretty much exactly 24 hours. He wasnt even fully adult at this stage and  about 6-7 cm big. 

[removed]
During a cockroach dissection in university biology labs I recall noticing how much fat they had inside their bodies. It's just packed inside the rigid shell, although presumably if they get fat enough they moult to increase the available space.
Insects have limited capacity to store energy due to there size, so they basically have to be constantly eating in order to stay alive.  Their surface-to-volume ratio is much higher than that of a human so heat dissipation will be much faster and also their metabolic rate will be much higher due to their size (look up metabolic scaling - interesting fact, every living creature typically has the same number of heart beats in a life time it's just smaller creatures tend to have a faster heart beat hence why insects live for a matter of days and elephants closer to a hundred years).  
First, I'm required to point out that the Sahara, contrary to popular belief, is mostly *not* covered in sand dunes. [Here's a map](https://upload.wikimedia.org/wikipedia/commons/c/c1/Saharan_topographic_elements_map.png) of all the dune fields (in yellow) in the Sahara. Most of the Sahara looks something like [this](https://commons.wikimedia.org/wiki/File:Algeria_desert_park.JPG)- a rock-strewn sandy soil with a hard crust ("desert pavement"), like what you see in the Mars rover photos but with scattered bushes. The dunes covered places that look like that, so imagine a rocky soil a few meters thick at the bottom of the dunes. Then the groundwater level is usually somewhere above the old ground level, so imagine that it's soaking wet and muddy. That's what it's like down there. The dunes are not like glaciers- they don't rub rock formations smooth once they're buried. They mostly preserve it whole. (For an extreme example of this, see the [camel thorn trees of Namibia](https://www.nationalgeographic.com/photography/photo-of-the-day/2011/5/camel-thorn-trees-namibia/) which were buried centuries ago and only recently uncovered as the dune kept migrating.)

Another thing to consider is where all that sand came from. You get sand dunes when the environment is producing more new sand grains faster than it can stabilize them into rock. The Sahara has so many dune fields because when the climate was wetter about 6000-10000 years ago, there were massive lakes covering what is now desert. When these lakes dried up, their sandy bottoms provided an ample source of sand to make dunes (and an ample source of nutrients in the form of wind blown dust to feed [the Amazon rainforest](https://www.nasa.gov/content/goddard/nasa-satellite-reveals-how-much-saharan-dust-feeds-amazon-s-plants/)). [Here's a map](https://imgur.com/a/EJ45gPx) (snipped from [this paper](https://www.researchgate.net/publication/258066270_Dynamics_of_Green_Sahara_Periods_and_Their_Role_in_Hominin_Evolution)) of all the huge lakes and alluvial fans (in blue and gray) that used to cover the Sahara. Notice how many of them are in the same parts of the desert that now have dune fields in that earlier image? In many places, the current dunes are directly over the old lake bed, so the bottom of the dunes is exactly what you would imagine a dried up lake to be like. See [this radar image](https://imgur.com/a/0fHqobo) from [an earlier askscience](https://www.reddit.com/r/askscience/comments/1xjnz0/if_all_the_sand_in_the_saharan_desert_were_to_be/) question. The top of the gray bar is the top of the dunes, and the red line is the bottom. It's so flat because it's an old lake bed. There probably aren't mountain ranges or other huge topographical features buried under the sand.

EDIT: Thanks to /u/RenascentMan for correcting me on the GPR image


[removed]
[removed]
Hope this thread helps a bit:

https://www.reddit.com/r/askscience/comments/1xjnz0/if_all_the_sand_in_the_saharan_desert_were_to_be/
Apply enough pressure to sand and you're getting packed sand that behaves as a very brittle rock. Apply more, you get sandstone. Apply even more pressure and you're getting harder sandstone, closer to granite, Pile up enough sand and the sand on the bottom gets that pressure. So instead of a "bowl filled with sand", a desert is a set of strata of sand/sandstone of different hardness.

Tectonics, wind, erosion mix that up; expose hardened rock, break it up into gravel, pile sand up creating mountainous dunes that get rocky core then blow the loose sand away; shifting plates lift deeper layers onto the surface, so things aren't smooth and uniform, "same depth - same hardness", but more chaotic - but you can be pretty damn sure 500m down it's all solid rock.
The surficial layer of bedrock will show the progressive effects of surface exposure followed by burial in a hot environment.

Back when those rocks were exposed, they underwent chemical and mechanical weathering in hot climatic conditions with varying amounts of humidity. 

The mechanical weathering consists in fracturation from unloading, and thermal expansion from day/night cycles, and grain plucking through eolian and fluviatile processes.

The chemical weathering consists in re-equilibration of mineral assemblages to surface conditions, mostly the conversion of ferro-magnesian minerals and feldspar to various types of clay minerals  and oxydes. Carbonates undergo some degree of dissolution. These chemical transformations might result in volumetric changes which make the rock crumbly.

Burial progressively brings these weathered rocks at depth, under a thickening layer of sediments. Remember that chemical weathering? The water involved in those processes is now loaded with dissolved minerals and works it's way down into the sediments and the weathered/fractured bedrock, where it will precipitate mostly carbonates (calcrete) but sometimes even silica (silcrete) which cements grains and rock fragments together.  

All in all, a history of exposure and burial in a hot arid environment such as the Sahara leaves a deep and recognizable mark on the underlying rocks.

EDIT: TLDR ... Cracked, broken up in chunks, and mostly chemically transformed to clays and carbonates to various degrees by exposure to water and changes in temperature.
Excellent relevant answer from a few months back - https://www.reddit.com/r/askscience/comments/8eu5ua/how_deep_or_shallow_can_the_sand_be_in_a_desert
[removed]
Not sure what prompted your question, but at least a portion of the bedrock in the sahara shows [polishing and scratching from glacial activity](http://the-earth-story.com/post/82180880097/glaciation-in-the-sahara-440-million-years-ago). This dates from the geologic era when the Sahara was part of Gondwana Land and there was a polar ice cap there (similar to Antarctic is today

First of all: sahara desert is a mistake because the name 'Sahara' is derived from a dialectal Arabic word for "desert", ṣaḥra (صحرا /ˈsˤaħra/). It's like you say "what's the bottom of the desert desert like". It's suffice to say "what is the bottom of the Sahara like".
Content Delivery Networks (CDN).  Multiple servers around the country cache the content, closest geographical or fastest is the one that serves you so not everyone is pulling from the same server.  It's not hard to forecast bandwidth usage since it is just simple data and in general most CDNs are not run near capacity so there is room for these spikes.  
Many people are mentioning CDNs and that is the correct answer. However, to address your question, it is possible for a site to spin up their own servers from a cloud service company to handle sharp increases in load. CDNs are very good at delivering static content but they wouldn't be able to help if the spike were due to a huge influx of user registrations or ticket purchases.
[removed]
~~Consent~~ Content delivery network (edit: was on mobile and didn't notice the auto correct). Looks like they use Level3 for this. Essentially they are paying someone else to deal with the massive bandwidth spike among other benefits. 

http://www.prnewswire.com/news-releases/hbo-streams-game-of-thrones-season-7-using-level-3s-cdn-300488213.html


I work for a company that handles the infrastructure for a large streaming platform in Australia. CDN's are great at handling static files (pictures, videos, etc) but the majority of the workload come from things like API calls that can't be cached or change on a per-user, per-session basis:

- Can the user play this video file?
- Are they authenticated?
- What is the DRM key that is used to decrypt the potentially encrypted fragments?

All these can't be cached to the same degree as video files. The newest GoT season started with a spectacular failure of our largest cable provider's online platform - which was due to the fact that the authentication service couldn't handle the load. So all the video files were there, all the DRM keys available, but because no one could prove who they were, there was no playback.
[removed]
The process is generally known as "playout" (see the WIkipedia article).

CDNs are a major part of the last step. But there is a whole massive video processing infrastructure to even get to that step from the creator supplying the content. Content acquisition systems fetch the content from wherever it is generated (for GoT it is reasonably simple but for a complex live broadcast video will be being acquired from many places over many different technologies). Then there are the transcoding servers. And don't forget ad-insertion. And eventually some streaming servers. All before you get to the CDNs.

These are really big engineering projects -- each step involves large server farms, built around massive, fast storage.

And sending the bits would be useless without the operational management, quality assurance and fault and performance management systems to make the whole lot work reliably.

I don't know about HBO, but many broadcasters outsource playout to specialist companies you have never heard of. For example, my employer handles playout for a large European TV broadcaster.
Worth noting, the HBO Now streaming service uses MLB (yup, Major League Baseball) Advanced Media for providing the backend infrastructure.  MLB Advanced also handles ESPN's product, WWE, PGA, World Cup, the NCAA Basketball Tournament and obviously MLB.  IIRC there are a few other high profile media outlets that use them as well.  I believe they work out of CNBC's old plant in Ft. Lee, NJ. It's quite an operation and has really been a leader in the nuts and bolts of delivering streaming products and are very good at what they do. It also makes the league an obscene amount of money, somewhere around $650M/yr.
[removed]
[removed]
They use content delivery networks (CDNs).  A content delivery network is a service that specializes in distributed networks and servers that decentralize content delivery and bandwidth load.  

An early player in CDNs is Akamai.  When I worked on Target's online bill presentment and payment service in 2000, they used Akamai to host some of the site content.  
The magic word is CDN, content delivery network. There's a few large companies who supply servers and bandwidth for exactly this purpose. The best-known is probably Akamai. Essentially their business model is to have a LOT of servers and bandwidth available at all times, and to sell that to many companies. Since no one company will have high bandwidth demands all the time the sum of spikes from different companies evens out for them somewhat. A big data pipe you (as company) can rent by the minute, so to speak, if you don't need it all the time.
CDN is the right answer, but I wanted to mention that in the days of Napster and Kazaa we had peer to peer networks capable of streaming massive amounts of data quickly to the edge of the network with supply growing immediately and automatically on demand. There were even some good attempts to commercialize this but Hollywood wasn't ready to buy into online distribution just yet. In the meantime CDN's we're growing to be able to service the needs of their clients and bandwidth prices came down so sharply that CDN's still own the market. I still think there's a great deal of untapped potential in p2p to be able to handle huge spikes in demand without adding much bandwidth cost for the distributor. 

FWIW, my HBOnordic crapped out all day yesterday and made GOT unwatchable until today. 
> Generally how do massive amounts of data get transferred from one source over shortly periods?

In most cases the service provider (such as HBO/ random website) relies on 3rd party cloud (internet) services that have a massive data centers across the country / world. What pretty much happens is when the data centers detects a massive increase of request it automatically clones your data and distributes your data across multiple servers. So pretty much you go from having 5 servers that are hosting your data to 500 servers. 

Amazon (AWS) is the number one provider of this kind of service. They figured since they need a massive network to run their business, they'll lease out their "extra space" and make some extra coin from it. 

*I could be wrong, however I know someone out there will politely correct me lol. 
[removed]
Lots of people talking about CDN and that's correct.

They forget (or try to simplify) that the video is behind an authentication system and that should not be behind a CDN. A CDN only serve static content because cache (have several copy of the same content, and keep it synced) across several geographical regions.

However when it comes to a dynamic content (like authentication) that involves a logic to be processed the architecture changes and we talk about horizontal scaling. Most cloud services (AWS, GCP, etc) offer the ability to create clusters of an application that are managed with geographical locations, conceptually, in the same way of CDN. The difference is that they offer the same code that will process a request, not the same static content itself.

I work with GCP instead of AWS and they have a "simple" page that explain the concept: https://cloud.google.com/container-engine/
AWS require much more work on the company side but can achieve the same results, i personally prefer the approach of Google that tries to make everything nearly completely automatic in terms of management.
[removed]
[removed]
As someone who is intimately familiar with this problem, pushing bits of video is not the real challenge (as others have said CDNs were made to handle that). 

The real challenge with a title like Game of Thrones is having your services weather the first minute of traffic. Service traffic shoots up exponentially the second a title like GoT appears on a service which can cause all sorts of problems to an unoptimized architecture which is why you always heard about these services going down during events like GoT.
[removed]
Content Delivery Network (CDN) is used to cache content near to the consumer.  Quality CDN's have edge locations in every major metro area.  

The CDN will prevent most traffic from going back to the origin servers. That being said, things like session data and authentication token data must go back to origin.  A media company can do predictive scaling for a campaign or event to scale up prior to the load.  Or they can use a PAAS like Google App Engine that scales in miliseconds and not have to worry about scaling.
[This is an example of how Netflix does it](https://openconnect.netflix.com/en/delivery-options/) but it could be the same for HBO.

Basically once someone requests a video the ISP will keep a copy at their local station.  Then whenever the next person in your town wants to watch the same video the ISP can just give you that copy, instead of re-downloading it.  So the original HBO server only needs to give the video out to "1000 towns", and they can each give it out to "1000 homes", etc.
[removed]
CDNs are a key part of this as everyone mentions. But, the short answer is "they don't", IBM does.  
HBO VoD is [delivered by Clearleap](http://www.businesswire.com/news/home/20130325005159/en/Clearleap-Enables-HBO-Offer-Expanded-Content-Offerings)  which is part of [IBM Cloud Video](http://clearleap.com/).

[removed]
So everyone appears to be mentioning CDNs, and some people are mentioning scalability in cloud services, but noone appears to be mentioning Peer to Peer (P2P). Netflix in particular is a big player in P2P networks. You get the website and personal information from Netflix servers on AWS, but when a popular show comes on (eg. Stranger Things), the servers relieve load by allowing their users watching Stranger Things to download from other users that also recently watched/are watching the same episode. Your computer caches the video (temporarily downloads it as you're watching it) and when another Netflix user requests this information, your computer responds by giving them the part of the episode they asked for.

I'm not sure if HBO does something similar, but this practice is very good at being scalable. The more users watch the same episode at the same time, the more 'servers' (other users' computers) your computer has to request information from.
Pediatric patients are fitted with eyewear based on prescriptions from mostly just the autofractors as jaimemaidana pointed out. This gives a really good estimation of the corrective lens prescription. A ballpark or rough estimation of the prescription.

Once someone learns his/her abc’s or sometimes shapes a phoropter may be used. The device that sits in front of your face, and you are asked if one or two, or a or b looks better as you are looking at an eye chart. This allows for an even better prescription to be determined. The phoropter may be used by itself or it may be used to fine tune the prescription that the autorefractor read, so you get the best possible vision.

It’s not that an adult’s autorefratcor generated prescription couldn’t be used, but your doctor wants your eyewear to have the best chance for giving you the best possible sight.
Streak retinoscopy is an objective way to get a prescription.  [Here is a video showing it in action](https://youtu.be/kAreDffuVCQ). In short, you put lenses up until you perfectly focus a light beam on the eye.  That gives the objective prescription.

When the doctor is asking you "1 or 2" they are doing a subjective way of finding your prescription.  It is you saying what you like better.  

A doctor can use streak retinoscopy to get any ones prescriptions.  However, if the patient is able to speak, it is good to confirm, and tweak, if needed, the prescription.  A subjective prescription may not give the best vision, but it is what the patient likes the most.
I am usually a lurker but I had to reply to dispell some mis-information in this thread. 

As it has been brought up by others in this thread, retinoscopy (streak) is the definitive (currently) way optometrists and ophthalmologists calculate prescriptions in cases where verbal communication is not possible, i.e. babies, infants, young children, mentally challenged or elderly patients. 

Retinoscopy works by shinning a retinscope into a patient's eye and observing the reflex of light that is reflected back. This system is built on the optics found in the eye, no different to the optics of a camera lens. There are two movements we expect to see, they are an against or with movement. An against movement means you need a negative lens correct (myopia) and a with movement means you need a positive lens to correct (hyperopia). You can also calculate astigmastism with retinoscopy, and this is where it becomes difficult. 

In the hands of a capable operator, retinoscopy is far more accurate than any modern autorefractor (for now) as operators have trained many years to use a retinoscope. The problem with current auto-refractors is that they do not account for accommodation in the eye; think when you switch between looking at something far away like the horizon and then looking at your phone quickly. Do you sometimes notice it takes a microsecond or two to adjust? Thats your accommodation working, your eye lens physically changes shape in order for you to see your phone clearly; think chaning focal lenghts on a camera zoom. Accommodation breaks down around 40years old progressively until around 60 and this is why people need reading glasses. I digress. 

Auto-refractors are still a very important screening tool in helping aid reaching your final prescription, however the results are not generally completely reliable. 

I hope that helps.

Source: OD. 

There are two ways that I know of, one being retinoscopy, and other being teller cards.  
Retinoscopy basically measures how the light fractures in your eyes, while the teller cards are more or less cards with things that looks either blurry or sharp, not 100% clear on that.
[removed]
Doctors/optometrists fit children with these prescriptions simply because there isn't a good way of doing it subjectively. Speaking as a pre-registration optometrist in England I will try and explain why this is the case as best i can.

Subjective results are generally better because we must control a process we call "accommodation"  i.e. the ability young individuals have to focus for near. Optics 101 also shows that you can use this additional focusing power to focus through some prescriptions, for example "long sighted" or plus prescriptions, this causes long term issues in patients because this requires effort and causes eyestrain. In children the ability to accommodate is very high. Trust me when I say it is much easier to relax this accommodation in someone who a.) Has less ability to do it and b.) Can understand subjectively when a lens makes no difference to their sight. These are both adult qualities, as a result we must do it differently for children. 

The way we generally do it is by relaxing the accommodation with drops called cycloplegics, these drugs then have side effects which then make subjective testing EVEN harder. For example it makes your pupils larger which can then cause glare which could be mistaken for blur. We must then use objective methods to determine a rough estimate of the prescription because the patient cannot operate subjectively, it is by no means more accurate. 

BUT that's not to say it is going to damage your child's sight, we tend to give these prescriptions simply to relax the child's eye muscles and allow for NORMAL ocular development. Normal ocular development shows that children tend to be slightly long sighted and become more short sighted until they have no prescription, this is normal because the larger your eye the more short sighted you become. It's obviously normal for your eyes to become larger as the body gets bigger. This is why we only correct gross long sightedness (we rarely intervene with short sightedness because glasses may not help much, specialist CLs are generally used for these children).

 As you can probably deduce now, it does not matter if we leave the child slightly long sighted because they have a large amount of accommodation, so the fact our prescriptions will be slightly off when obtained objectively (which they will be) will not be an issue. It's all about reducing the amount of long sightedness to a point which will not cause the patient to accommodate TOO much, which will then cause eyestrain and even lazy eyes (accommodation also stimulates the eyes to turn in as well). Lazy eyes are the worst case scenario in children undergoing ocular development, because the brain is still malleable or "plastic". Therefore, it can develop to IGNORE the visual information being sent by the lazy eye (therefore, the brain is avoiding double vision and blur) and cause very reduced vision in that eye in later life. 

In my experience, I generally prescribe long sighted prescriptions causing problems (for example lazy eyes) then use further tests to monitor whether the prescription has removed the lazy eye at future visits. I refer short sighted/ different prescriptions in each eye (anisometropia) to specialists to fit contact lenses etc. Some children also need patching to remove this dominance issue which is undertaken by other specialists. 

TLDR: we generally prescribe children to relax their "accommodation" to the point where it doesn't cause them any issues (lazy eyes/eyestrain). The long sighted child will see fine regardless of if a prescription is in place or not because their ability to accommodate through long sighted prescriptions is so good. Objective methods are not superior but are our only alternative. Prescribing for short sighted children or children with prescriptions which are different in each eye are more complex and require contact lens intervention. We prescribe for those children to slow the progression of short sightedness or to make the images formed by each eye more equal. Again, having the exact right prescription may not be possible for these children without subjective methods but we definitely allow the children's ocular development to be less affected by their incorrect prescription. The goal in prescribing for children is usually not to let them see clearly but to improve their ocular development.

Phew, that's hard to explain in layman's terms.

technically, they don't NEED feedback from you---optometrists have to adjust lenses for stroke patients, etc who are entirely non-verbal with some frequency. they are able to find your prescription by using a Maddox Rod (gives the patient a focal point at near, far, and at extreme points of the peripheral gaze) and by measuring the reflexes in your optic nerves. your optometrist talks to YOU because it's less awkward/more polite and so that they can find out what your PREFERENCES are---some folks prefer to wear a maximum corrective lens with their necessary magnification or refraction while others see/walk/drive better with a lighter correction---also, if you're being fitted for bifocals/progressives, it is much easier on the optometrist to figure out segment height, etc, if you can communicate what you're comfortable with---that said, they won't always be made perfectly the first time so you may need to go in and give additional feedback. because of the negative impact bifocals and progressives can have on patients with balance/mobility issues, patients with these concerns (especially the non-verbal patients) may be prescribed separate entire pairs of glasses so as to avoid visual distortion and discomfort. 
Question. 

Special education teacher here- I have a student (14yo male) who is nonverbal and has extreme difficulty expressing (ASD). He squints when looking further than a few feet. All his close relatives wear glasses. I assume he could benefit from glasses but everyone just says "you can't do a vision test on someone who can't express". This thread seems to suggest otherwise? 

Is there anyone that could help him?
Yeah as soon as they can it seems like they do something similar even on very small children. They fit my 2yo for glasses by asking her to label different pictures of things as they got smaller and smaller, and a few other things. They made it fun and helped her feel comfortable
To make a long concept short, adults need to read and see more clearly. Its easy to give babies the ability to see with the current technology at hand, but to perfect the prescription they need a person's input.
Because babies don't really need highly precise eyesight. They don't need to drive, read street signs, read anything. They don't have to worry about eye strain from sitting in front of a computer all day. They don't need to do much at all. 
There's a subjective refraction which usually gets you to see 20/20 or at least close. 
This is what they do on a baby. 
Afterwards you participate in an objective refraction which is more so for comfort than anything else. 
You guys are using a lot of terms I'm not too familiar with, so excuse the ignorance. 

These days, when I go get my eyes checked, the very first thing I do is stare into this machine that has a hot air balloon as the pic, and it makes it sharper and basically spits out my prescription. What machine is that? Is that the retinoscope or something else? 

It'd be awesome if that was the only step for prescriptions unless I have other concerns about the eye.
What I haven't seen mentioned here yet is the slew of accommodative  and vergence disorders that can actually be worsened if someone were to take their autorefracted/retinoscopy prescription and walk out the door. Cutting plus or adding prism to convergence insufficiency patients (as high as 13% of US general population) is the perfect example of how glasses prescriptions must have subjective input to be completely best corrected. 
Babies only need a rough prescription to get enough input to their brains so that their eyes and brain develop properly. This can be determined by retinoscopy (a technique that utilizes optics performed by a doctor). But adults have much better visual acuity and can determine the difference of a quarter diopter so the doctor asks which is better to get the exact prescription. But even then the doctor determines what the patient can actually handle by showing those lenses in free space because large changes in prescriptions can be rejected by patients. 

TLDR; ask your optometrist or ophthalmologist not reddit. 
The instrument that measures the exact shape of the eye is accurate, but the adult brain is able to correct for somewhat distorted images. If you had better eyesight in the past, the brain learned how to tweak the image to get rid of it correctly.

Once an adult switches to an eyewear, the brain starts forgetting to do things without eyewear, and you become dependent on it. It is a tradeoff between having to wear the eyewear all the time, or going along with what the brain is able to already deal with.

Baby brains have zero practice, so even if they would be able to answer, it would be identical to the instrumental readings.
[removed]
[removed]
[removed]
[removed]
Bill & Melinda Gates Foundation article covers it pretty well 

https://www.gatesnotes.com/Health/How-to-respond-to-COVID-19

“There are two reasons that COVID-19 is such a threat. First, it can kill healthy adults in addition to elderly people with existing health problems. The data so far suggests that the virus has a case fatality risk around 1%; this rate would make it several times more severe than typical seasonal influenza and would put it somewhere between the 1957 influenza pandemic (0.6%) and the 1918 influenza pandemic (2%).

Second, COVID-19 is transmitted quite efficiently. The average infected person spreads the disease to two or three others. That’s an exponential rate of increase. There is also strong evidence that it can be transmitted by people who are just mildly ill or not even showing symptoms yet. This means COVID-19 will be much harder to contain than Middle East Respiratory Syndrome or Severe Acute Respiratory Syndrome (SARS), which were only spread by those showing symptoms and were much less efficiently transmitted. In fact, COVID-19 has already caused 10 times as many cases as SARS in just a quarter of the time”
The two main metrics are CFR (Case Fatality Rate - the percentage of infected people who will die), and R0, which measures the spread of the disease. R0=1 when an infected person infects one other person on average. This gets more complicated as more of the population is infected, so you might see Reff (effective R0) used instead.

The CFR in Wuhan/Hubei province is about 4% but about 1% in the rest of China (according to the official statistics). The difference is likely due to treatment availability and the heads-up the rest of the country got. That’s compared to about 0.01% for seasonal flu, about 2.5% for Spanish flu and ~10% for MERS. 
(EDIT: 10% for SARS, 20-60% for MERS, thanks for the correction.)

R0 is normally about 1.3 for this kind of virus, however the long incubation time means the infection rate is probably higher here. This is why the massive quarantine in China has been effective, and why there’s the potential for a huge impact on countries that can’t or won’t impose the same level on containment.
This is a quickly moving pandemic - our knowledge of the virus is shallow and its potential to evolve with time is greater than 0. That said, the initial outbreak in Hubei province allows for a detailed breakdown of mortality by age and sex. The following comes from [a paper in the China CCDC weekly](http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51) that evaluated 72,000 case studies, along with about [56,000 case studies from the World Health Organization](https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/#ref-2) (WHO). The full integration and interpretation of these two sources can be found [here](https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/). A partial summary follows.

By age:

|Age|Death Rate|
|:-|:-|
|80+ years|14.8%|
|70-79 years|8.0 %|
|60-69 years|3.6%|
|50-59 years|1.3%|
|40-49 years|0.4%|
|30-39 years|0.2%|
|20-29 years|0.2%|
|10-19 years|0.2%|
|0-9 years|no known fatalities|

&#x200B;

And by sex:

|Sex|Death Rate|
|:-|:-|
|Male|2.8%|
|Female|1.7%|

&#x200B;

It is unclear why men and women are affected, but an intriguing possibility is an X-linked gene that creates the ACE-2 receptor, which is exploited by some coronaviruses to enter cells. Females are XX, while males are XY, meaning they only have one copy of the gene and a subpopulation may be more vulnerable. Primary research can be found [here](https://www.nature.com/articles/s41421-020-0147-1) and [here](https://www.nature.com/articles/s41368-020-0074-x), with a more accessible summary [here](https://www.the-scientist.com/news-opinion/why-some-covid-19-cases-are-worse-than-others-67160) This matches a similar pattern with the 2003 SARS outbreak, where 13% of females died while 22% of men did.

The origins of the virus are unknown, but genetically it is [very close to a similar virus in pangolins](https://www.nature.com/articles/d41586-020-00548-w), which unfortunately are poached for traditional medicinal purposes. While it is not definitely known where the virus comes from originally, it is most likely an episode of zoonosis - where a virus spreads to a new species from another. Many human pandemics have their roots in animal transfer, including influenza (chickens), Ebola (chimpanzees), HVI/AIDS (chimpanzees again), measles (cattle), among hundreds of others. If you are interested in the history of these kinds of disease species jumps, I recommend [Spillover](https://www.amazon.com/Spillover-Animal-Infections-Human-Pandemic/dp/0393346617) by David Quammen.

My background is as an archaeologist, and I've been researching the emergence of epidemics for the past few years (no pubs on the topic though, sorry). If you are interested in this too, I recommend William McNeill's [Plagues and Peoples](https://www.amazon.com/Plagues-Peoples-William-H-McNeill/dp/0385121229). Historically, diseases seem to have gone through an initial high mortality phase, followed by a more contagious phase. This is only an hypothesis - and by a non-specialist at that - but I wonder if the SARS to COVID-19 infections follow that pattern. The difference being that a modern medical system can isolate the more lethal first step (SARS). Unfortunately, for more contagious diseases (like COVID-19), only massive containment measures are likely to be effective. Not all governments are capable (or willing) to do this.

Lastly, some may read the low infection rates for young people as a reason not to worry. As a parent of an infant, I take some solace on them. But keep in mind that even if you get a mild infection, you may spread it to someone you care about who will have a much harder time with it. While masks (N95 or greater) may be effective at preventing contamination, the most effective measures will be simply hygiene like frequent showers and washing your hands thoroughly. WHO recommendations for hygiene habits to reduce disease transmission can be found [here](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public).

Lastly, I caution agains the [normalcy bias](https://en.wikipedia.org/wiki/Normalcy_bias). It is easy to assume that this will blow over like so many other supposed crises have. But the only comparable disease in the modern era is the [1918 influenza epidemic](https://en.wikipedia.org/wiki/Spanish_flu) that infected 500 million, and killed 40-50 million. If the numbers from Hubei are representative of the infection (and keep in mind - that is with extraordinary containment measures), then a 2.8% mortality infecting 500 million today would be 14 million people. It is unknown how international travel through airports and crowded cities of 10+ million will factor into this pandemic - we don't have a historical precedent for that. We also don't have a precedent for how pandemics interact with modern medical systems. There are a lot of unknowns. Wash you hands.

I hope this helps. Stay safe everyone.

ps. sorry for the weird text changes, wrote this in markdown first, which did not turn out well
I've been checking in on [this site](https://ncov.r6.no/) daily just for a quick stats update on the outbreak. It has comparisons to both the swine and SARS outbreaks on graphs 2 and 3. Like the author of the site says, many things have changed with each virus outbreak, such as detection methods and when information was spread about them, and so I wouldn't use this as a bulletproof analysis, but it's nice as a graphical representation.
The fatality rate is presently a dynamic function. You can't divide by the number of infected, as they may yet die. But you also can't divide by the number of cured because those infected may yet survive. The point is, the only correct way of calculating the fatality rate is to wait until the dynamic effects wear off, and we reach the steady state infection rate. 

Professional virologists have access to some impressive, highly sophisticated modeling software which considers the limitations I mentioned above and beyond. They seem to be going with 1 - 3% HIGH end mortality rates. I consider these estimates to be conservative, but nonetheless within the realm of possibility.

Another factor to consider is the novelty of the virus. Due to containment efforts, and the aforementioned dynamic response, the current healthcare system is overwhelmed. This superficially adds to the mortality rate as we can expect persons to have reasonable access to healthcare during steady state.

So in conclusion, the most reliable "back of envelope" calculation you and I can make will be a gross estimate. But it can be done. Deceased / Cured will give an extremely conservative estimate during early dynamic states. Consider week one had a few deceased and no cured. This method leads to a mortality rate of infinity percent! As time goes on, it will approach a more stable mortality rate. In contrast Deceased / Infected could potentially be completely erroneous. Overall, it lacks meaning during a dynamic state for reasons mentioned previously. It doesn't add much at steady state either as a greater percentage of deaths will occur during the dynamic response and overall the rate of change regarding infected people will decrease leading the rate to continue artificially inflating with time. Best someone like you and I could do is wait another month for more data. Take a 1 - 2 month window and calculate the Deceased / Cured ratio within that period. It will still have dynamic effects or second order effects, so it will remain conservative. But it will isolate a window with 3 - 4 full cycles of infection, death, or recovery.

An interesting project for someone to take up would be to plot the daily *moving average* D / C curves over varying window lengths. Say 3 days, 1 week, 2 weeks, 1 month etc. It will start at infinity as we mentioned, and should decay to some steady value over time. Regression techniques could then give fair estimates of steady state rates ... that is until the bell gets rung again somehow.

TLDR: We're seeing the *dynamic* response of a novel virus. Mortality rates will be relatively high at first, but should decay to a steady state. 1 - 3% rates are conservative, as they are derived using dynamic data. Expect it to go down.
[removed]
[removed]
Based on numbers alone COVID-19 has killed about 3,000 people so far, whereas regular flu killed over 500,000 people last year globally. What makes COVID-19 more dangerous, however, is the spread rate and the current lack of a vaccine. We are very good at developing a vaccine for the new strain of flu every year, so countries with access to good medical systems see relatively few deaths from flu each year, and it's usually the elderly or already infirm who die. The problem is, we don't have a vaccine for COVID-19, so the spread can't be contained via those means. This means that developed and undeveloped countries are almost on a level playing field when it comes to spread rate. The only option so far is to isolate people who catch it. Problem with that is it has a near 12 day incubation period, during which a carrier is contagious, but without symptoms. That leaves a carrier as a kind of walking bioweapon who is completely unaware of it. If we can develop a vaccine and distribute it, the spread can be contained much more efficiently, but until such a time, we have limited options. Hence why it's so potentially dangerous if it starts spreading out of control. We're not looking at an end of the world scenario, but we are potentially looking at millions to tens of millions of deaths at worst.
[The Guardian just put out a nice article about Covid-19 illness.](https://www.bbc.com/news/health-51674743). The article dwells on the "it's hard and a bit too early to tell for sure" aspect, though, and less on how it compares to influenza.
I read something in passing that mentioned that some people can become infected with the virus but never get sick enough to even develop symptoms and essentially just become vectors... Can anyone confirm? Also I know that in China, people who may have the disease are being turned away from hospitals if their symptoms aren't very severe and since this disease can manifest as essentially the seasonal flu, would it be somewhat accurate to say that the current numbers reported for total infected and estimated CFR are inaccurate?
This article ...

https://www.mdpi.com/2077-0383/9/2/523

... an analysis mostly done using informatics techniques, and therefore hypothesis generating (i.e. not hypothesis testing) suggests a much higher case fatality rate in the range of 5% or higher.

This would be a rather bad thing.  Along with that, the R0 was estimated to be between 2 and 3 ... roughly the same as influenza.

So ... this is a cool new analysis by competent people, using available (and therefore suspect) data.  The real answer is that no one knows yet.
From here: https://www.mdpi.com/2077-0383/9/2/523

>Our cCFR estimates of 5.3% and 8.4% indicate that the severity of COVID-19 is not as high as that of other diseases caused by coronaviruses, including severe acute respiratory syndrome (SARS), which had an estimated CFR of 17% in Hong Kong \[9,10,20\], and Middle East respiratory syndrome, which had an estimated CFR of 20% in South Korea \[21\]. Nonetheless, considering the overall magnitude of the ongoing epidemic, a 5%–8% risk of death is by no means insignificant. In addition to quantifying the overall risk of death, future research must identify groups at risk of death (e.g., the elderly and people with underlying comorbidities) \[22,23\]. Moreover, considering that about 9% of all infected individuals are ascertained and reported \[24\], the infection fatality risk (IFR), i.e., the risk of death among all infected individuals, would be on the order of 0.5% to 0.8%.

Takeaways:

- 5 - 8% mortality rate among those confirmed to be infected.
- This mortality rate is only around 30 - 40% that of MERS and SARS.
- Expected 9% of actual infections get reported, because many are mild.
- **Actual mortality rate amongst all infected is 0.5 - 0.8%.**


Comparable toral mortality rate for seasonal influenza is about **0.13%** based on CDC numbers. https://www.cdc.gov/flu/about/burden/index.html

So, 4- 6 x worse than influenza based on just this study.
How deadly isn't quite the right answer you're looking for. You want to know three numbers about a virus:

1. How long does it take to become contagious or show symptoms after being initially exposed to it?
2. How often does a person exposed to the virus die from the virus?
3. How easily does the virus spread from one person or surface to the next?

These three things working together as a system can determine how LETHAL a virus will be within a community of people.

So how do you make things tougher for the virus to cause damage to a community of people?

1. Quarantine people who are infected for as long as whatever answer you have to question #1. Keep them isolated so they can be monitored for at least that long for symptoms. This prevents them from spreading the virus to others.
2. The answer to #2 may seem counter-intuitive. The more people a virus kills the more difficult it is to find new hosts to infect. Most deadly viruses only kill SOME of the people they infect, this way the virus is able to find new uninfected hosts more easily. A virus that kills 100% of the hosts it infects will quickly burn itself out. This is why a virus that only kills 50% of the hosts it infects is in some ways just as scary as a virus that kills 100% of its hosts. Knowing the answer to #2 gives you an idea how far the virus can spread each week if preventative steps are not taken.
3. The answer to #3 can give you an idea of what types of barrier protection will be needed to prevent YOU becoming infected, as well as the community you live in. Does the virus break down easily under sunlight or warm temperatures? Will washing hands be enough? Should people start wearing facemasks to prevent the virus from spreading if you cough or sneeze? (Facemasks worn in public are to prevent you from infecting others, not the other way around. They provide little to no protection otherwise aside from barrier protection against accidental droplet exposure.) Is the virus particularly hardy and able to survive on surfaces for weeks or longer? Is bleach enough to clean a surface that has the virus on it?

You find out the answer to those three questions and you'll know how deadly a virus is.
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
[removed]
How deadly is this coronavirus?

We don’t yet know. However, signs suggest that many people may have had mild cases of the virus and recovered without special treatment.

Very early in this epidemic, it looked like about 20% of cases were severe. People developed acute respiratory distress syndrome (ARDS), which causes tiny air sacs in the lungs to fill with fluid, crowding out air so that not enough oxygen can reach the bloodstream.

As of February 25, 2020, the reported confirmed cases and deaths in China suggest the mortality rate is roughly 3%. It is important to remember that early on in an epidemic there is a “tip of the iceberg” phenomenon where we overestimate more severe cases and mild or asymptomatic cases go unrecognized, so the mortality seems higher than the reality. That may be happening when we speak of up to 3% mortality. By contrast, SARS had a mortality rate of around 10%; the MERS mortality rate is closer to 30% to 40%. There appear to be many more COVID-19 cases confirmed than there were with SARS and MERS.

Source: https://www.health.harvard.edu/blog/as-coronavirus-spreads-many-questions-and-some-answers-2020022719004#q14
[removed]
[removed]
[removed]
The mortality rate is only established once the number of infections is known.  

Of the 82,500 covid 19 infections recorded:
66,000 were infected but didn’t get significant symptoms (80%)
12,375 were infected and got cold-like symptoms (15%)
1,325 got flu symptoms and recovered (1.6%)
2,800 died (3.4%)


By contrast - Global flu last year: 
Approx 27 million people contract flu viruses in the last year.
Approx 22.6 million got infected but didn’t get significant symptoms (82%)
Approx 4.3 million people reported cold-like or flu symptoms and recovered (15.7%)
Approx 635,000 people died (2.3%)

Side by side (so far):
No symptoms:
18/19 flu (82%) vs covid 19 (80%)

Cold/flu symptoms and recover:
18/19 flu (15.7%) vs covid 19 (16.6%)

Death:
18/19 flu (2.3%) vs covid (3.4%)

Roughly 1% higher chance of getting symptoms with covid 19 and 1% higher chance of dying from it than any other flu.
The fatality rate in the press is low because the disease originated in China where the data is at best unreliable and at worst completely fabricated. This website is tracking the virus using only the most reliable data (excluding China and Iran)

> The CFR calculation has been updated and now is based on cases from countries who scored at least 50 out of 100 on the 2019 Global Health Security Index's measure of their ability to detect and report emerging epidemics

https://www.cassandracapital.net/covid-scenerio-tracker

https://i.imgur.com/blZ3f6D.png
[removed]
[removed]
SARS-CoV2, which CAUSES covid-19.......is an offshoot of SARS with a more efficient binding method on the ACE2 receptor than SARS.  


Also, Influenzia is a totally different type of virus family......nothing to do with Coronavirii.
[removed]
Cave systems can be present deep underground in sedimentary rocks under the right conditions.  One way is to have a layer of limestone at the surface long enough to form a karst (cave) topography, then subsequently subside and be buried by thousands of feet of sediment.  The caves will still exist, but have no connection to the surface.  One place this occurs is some parts of Texas, where those deep caves can be a significant drilling hazard in oil and gas exploration.
Like all these comments, one of the best examples is the Naica’s Selenite Crystal Cave in Mexico. This cave is home to the biggest known crystal selenite in the world. Single selenite crystals that are larger than telephone poles. Scientists theorize that the cave formed 26 million years ago when a nearby volcano forced mineral rich water into the limestone. For pictures of this cave, a thousand of feet below the surface, [National Geographic Article](https://www.google.com/amp/s/api.nationalgeographic.com/distribution/public/amp/news/2017/02/crystal-caves-mine-microbes-mexico-boston-aaas-aliens-science)
Oh yes, and boy can they be interesting. In South Africa geological conditions caused a rise in one area. I don’t remember the exact details, but there is an excellent book, [Cradle of Life: The Story of the Magaliesberg and the Cradle of Humankind](https://www.amazon.ca/Cradle-Life-Story-Magaliesberg-Humankind-ebook/dp/B07VR1K9TC/ref=sr_1_3?keywords=Cradle+of+humankind&qid=1587360885&sr=8-3). The result was a variety of cave systems. The entrances to these cave opened and closed at various times (rock slides, etc.) and in the late Nineteenth, early Twentieth centuries the economy is SA needed lots of lime for construction. Many caves were opened by blasting, including the one where Australopithecus Sediba was found my Matthew Berger.

Most of these caves were created by water flow eroding for dissolving the earth, and there will be caves that have never opened to the surface.
In Israel there are several sealed karst caves that were discovered in quarries. Some of them contain beautiful stalactites  and one or two contain unique organisms that have evolved in isolation for millions of years.  
[https://en.wikipedia.org/wiki/Ayalon\_Cave](https://en.wikipedia.org/wiki/Ayalon_Cave)  
 [https://en.wikipedia.org/wiki/Avshalom\_Cave](https://en.wikipedia.org/wiki/Avshalom_Cave)
Yes, there can be caves only connected to the surface by tiny cracks. The Nullabor in Australia has these. 

Fun fact: caves move up. Every now and then, part of the ceiling collapses to the floor. The cave just moved up. 

Not so fun fact: when they get close enough to the surface, a large area can collapse, leaving a "well" tens of metres deep and tens of metres across. So if you're driving across the Nullabor, stick to the tracks. Otherwise, you might go all Thelma and Louise.
Remember that the Earth's crust is only a small fraction if its total radius. The caves you are imagining are probably restricted to only the very upper parts of the continental crust. Most of the Earth comprises a solid mantle, liquid outer core and solid inner core. By virtue of the immense pressure, large voids would be improbable.

That's not to say there aren't spectacular cave systems in the upper crust! For example karst terranes (loosely speaking, areas of limestone with areas dissolved by water over time) produce spectacular caves, many of which probably haven't been discovered. Or lava tubes, where runny lavas flowed through but emptied, leaving behind subsurface cave systems.
I think a lot of these answers are not quite answering OPs question

OP, almost all major caves are relatively superficial within the crust, even the deepest caves are still in the top 2-5% of the crust. They're almost always formed as a result of water and, beneath a certain depth, you stop encountering flowing water, which makes caves rare 

You will find gas pockets or surface caves that have since been subducted underground, but you won't get "journey to the centre of the earth" style mega caves deep, deep underground.
[removed]
There are lots of caves formed by gas getting trapped in cooling rock that have no entrance. Maybe you’d count water carving out caverns. Not sure if that counts since the water had a way in. But I think the deepest known cave is only just over 7k feet deep, which isn’t all that deep really.
I asked a similar question a few years ago and got some pretty interesting answers if you want to take a look.  I know it’s not exactly the same question but the information is still relevant to what you’re looking for

https://www.reddit.com/r/askscience/comments/6b82qe/are_there_ways_to_find_caves_with_no_real/?utm_source=share&utm_medium=ios_app&utm_name=iossmf
There are known lava tube caves with no known entrances. 1000 metres of a such a cave in Iceland have been measured with no end in sight. The measurements were done using ground penetrating radar. Magnetometers can sometimes be used as well but proved unsuccessful in this case.
Hang Son Doong is the name it was discovered in 1991 and is the biggest cave system in the world. The cave has it's own ecosystem, creates it's own weather system and produces it's own clouds. You can find YouTube videos about it. One guy flew a drone in it. The video is only a minute long but you should see it.
There have been statistical analyses based on cave size and number of openings that indicate the vast majority of all caves (I want to say around 90% or so) have no entrances and thus are likely to never be found. There’s an article about entrance-less caves in the Encyclopedia of Caves
Depends on what you mean by 'pretty solid'. On short time scales, most of the Earth (except for the outer core, acts like an elastic solid. So, think seismic waves. On geologic time scales, most of the earth's interior (including the mantle) acts like a very viscous fluid. So even if you managed to create a void in the deep hot parts of the Earth, they would very quickly (on a geological time scale) disappear.
There are definitely voids in the crust that are not connected to the surface. There have been a few found by mining operations, like the Pulpí Geode. 

https://en.m.wikipedia.org/wiki/Pulp%C3%AD_Geode

This is about 11 cubic meters, which while not huge, was found by chance. Who knows what else is there, but there are definitely examples.

Here’s another couple found at the Niaca Mine, also in Mexico. 

https://en.m.wikipedia.org/wiki/Naica_Mine
A well driller in my Karst topography county told me it was very common for them to be drilling a well, and the rig just drops ten feet all of sudden. We're in an area that is full of springs, caves, seeps,  I can locate two dozen caves, and I don't know half of them. Some have sinkhole plains stretching for miles.

Although some springs have a tiny opening, one can follow a line of sinkholes for a quarter mile that feeds the spring, often sinkholes match up with a large room below in caves that one can crawl into. Presumably such springs may be fed from larger rooms or chambers. 

The driller's job is complicated by these drops as the water in such caves is generally not safe to drink. Caves within 150 - 200 feet of the surface around here communicate with surface water.  They have to drill down below a layer of rock that is less permeable, and line the hole with steel casing from that layer to the surface. Generally 400 feet or more. 

Karst areas are unique. They have limestone rock that is easily dissolved, topography that lends itself to cave formation. There is little reason for caves to form in, say, granite or basalt. Some volcanic areas can form lava tubes but these are rare. Igneous rock isn't going to form caves easily.
Void space deep in the Earth is limited by the compressive strength of rock.   Strong rocks could have maybe 250 MPa strength to take a high end number.  Compare that to the weight of the rock above you at density\*gravity\*depth.  Where these two stresses are equal, your cave collapses.  So 250 MPa / ( 9.8 m/s\^2 \* 2000 kg/m\^3) is about 10 km or 6 mi depth.  Again, that's basically assuming that the entire Earth's crust is made of Fe ore deposit or similar.
I used to work for a small construction company in Texas. We did backyard additions to people’s homes, lots of patio work. In the contract we specifically stated that if we start excavation and find out there’s some huge cave underneath your property, we will not be held liable for filling the hole.

The truth is that this is actually a common clause in many construction contracts. I’ve learned in becoming an architect that it is more common for this to happen than a layperson might realize. Not to say it’s super common, but it is within the realm of possibility that you could have a huge cave on your property and not know. I’m sure you’re going to get more science based answers than this, that’s just my two cents from a construction perspective.
Hoping someone can answer this:

There are caves that are SUPER deep, like 7 thousand feet deep. But that’s only just more than a mile, and the distance to the center of the earth is nearly 4,000 miles. 

Is it theorized or is it scientifically possible for there to be caves deep, DEEP underground? Like perhaps 1-2 thousand miles below us?
3 things if you want to contextualize *Journey to the Center of the Earth* with reality:

1.  It would be unlikely to have breathable air without major ventilation to the surface.  Various geological processes and leaks would likely mean pure CO2, hydrogen, natural gas, etc.  And of course filling with water is pretty common.
2.  Although shallow caves can be "cool", ambient geological temp increases about 25C for every km overall.  So you're limited to about half a km before it's unsurvivably hot.
3.  The ambient pressure becomes tremendous.  However, it's different that being underwater, as many types of rock can support weight and keep pressure out of a cavity.  However, still, if a cavity didn't collapse, it does tend to fill a cavity with gas or water under tremendous pressure.  A huge cavity like some deep underground sea or Blackreach or whatever, the weight of rock over a large vault like that would likely just collapse.
I went to college in western Virginia  --  western Virginia and most of West Virginia were major karst (cave) areas.  I belonged to the National Speleological Society and to my school's local chapter.   Most all the caves we explored had natural entrances -- sometimes several entrances.   But there were times when we were "ridge walking" (hunting for new caves) and discovered "evidence" of a cave.  This was usually a small hole in the rocks (too small to crawl into) that air or water was coming out of or going into.  So,  with the landowners permission, we'd sometimes try to blast the opening larger with dynamite.  (Dynamite wasn't very controlled back then - early 70's.)  I don't think we ever succeeded,  but it was fun trying.   Apparently, other cavers had some success with this method.
I'm not sure about the sealed off entrance part, but that's basically what you are trying to drill for when sinking a bore hole. A big underground river or flooded cave system. Also saw a very cool video of some cave divers diving in a deep underground cave and you can see at one point where the pipe for a bore hole has been drilled into the cave. I think the chances of them hitting a diveable cave is quite small
Depends on the area. I can tell you Kentucky and Tennessee and even parts of Alabama are VERY cave heavy. So yes, at any given time you might be on top of some sort of underground cavern system. Its why sinkholes can develop.
There's one in Tenneesee you can visit:   [https://www.rubyfalls.com/](https://www.rubyfalls.com/) 

A lot of people are underwhelmed because the actual waterfall isn't impressive but I thought being in the middle of a mountain was pretty cool.
[removed]
Cave systems only exist in areas with limestone, although I guess we should include lava. So no, the entire earth is not riddled with caves.   
There are caves with no entrance large enough for a man or animal to enter, but large and extensive enough to walk or swim through. But surface water is what forms caves, so technically they all have an "entrance" that allowed water through, just not large enough for a person. Even though we can't enter, we know these caves are there due to dye tracing, or the occasional accidental manmade opening. A particular cave in Kentucky comes to mind. There was no entrance large enough for a man untill the limestone quarry.  Now one can rappel about halfway down the quarry wall to the manmade entrance of a cave opened by the quarry. (Lower down the wall a spring comes out of the wall through a very narrow crack, more on that latter.)  This cave was formed as water seeped through small cracks and faults up on top of the ridge, flowed along fault lines made ever larger as the slightly acidic water dissolved the limestone. This water now forms a stream through the cave. As one walks along the cave passage the water flows through the cave till it disappears down a narrow fissure in the floor too small for a person. This water comes out as a spring far below through the wall of the quarry.   
So the original "too small" entrances were faults on top of the ridge that gathered rain water and a spring that exited through a small crack on it's way to the river. A cave unknown till man quarried an entrance.
ooh ooh! This has nothing to do with my scientific specialty, but on my off time I used to cave in West Virginia all the time.

Many of the mountains in that area are Karst, which basically means the rocks are slightly water soluble. That means that water flows through cracks in the ground, eventually hollowing out underwater rivers in much the same way that a normal river erodes a path for itself.

My dad and I used to do cave digs in Germany Valley, in Huntington county. We'd find promising-looking mini-caves and go into them until they get too small for humans to pass and we'd dig them out with hammer drills and crowbars and shovels. Once or twice we got lucky "broke through" and discovered a massive underground network of passages hidden away. Miles of passage, caverns unmeasured by man. Sometimes we'd even link them up with other known cave networks. I think that memorial day cave has linked up with at least half a dozen other cave networks and entrances.
since sink holes open up all the time I would guess in some areas for sure, especially in areas with lots of limestone, like South Dakota wind caves, though that cave does have an entrance, the water will definitely dissolve openings underground. Also the fact that we drill into the ground to get oil, that oil most have pooled up somewhere.
Really? I didn't know that. Have you seen the cave system in Vietnam? The Vietnamese used it during the war to train and hide troops. They kept it a secret for years. Some of the biggest spaces i have seen in a cave.  a river runs through the middle of it for miles.
TBH, no one knows. There could be an actual Journey to the Center of the Earth world down there full of lizard people. 

The furthest humanity has drilled was 7.6 Miles and it seems at that depth and pressure rock turns to hot silly putty and destroy's drill bits. We don't know what's past that depth. And the crust is generally about 45 miles deep in most places. Under that could be amusement parks for Drow for all we know.

There could be caves or pockets under our feet and we'd never know.
[removed]
there is getting so much gas up, expanding because of less pressure that it will firm giant hollow spaces. 

then with cracks by Earth shifts, that gas can escape, leaving empty caves.

it's one of the highest risk while drilling for oil, to hit a gas pocket.
The earth isnt really all that solid at all. The earth's crust an exteamly thin layer in comparison to the liquid and plasma flowing in different cores of the earth. The thin layer on top is at the mercy of the current of the mantel pushing around the central masses of crust, our continents, thus making things like fissures that then fracture out into the crust creating these cavities called caves. So yes. Their is almost near infinite possibilities when it comes to the formation of caves and crevices in the earth's crust. Like huge under ground lakes that have created large pockets in the crust with no entry or exit
It gets caught in the thin layer of mucus lining the inside surfaces of your lungs. The lungs are also lined with tiny hairs called cilia that beat in a coordinated fashion to slowly push the mucus up and out of your lungs as new, fresh mucus is produced to take its place. The old, dirty mucus reaches the top of your airway where you may cough it out, but healthy people usually swallow it continually. It is then cleared through your digestive system, which (unlike the lungs) is quite robust to dirt and bacteria and such.
Cilia are like small fingers that can carry some particles upwards out of your breathing tract.  However, if you've ever seen lungs of smokers after death, you can see that many things we breathe stay in our lungs.
Usually you have many airway defense mechanisms. First the hairs in your nose filter out the majority of larger particles. Smaller ones that reach your trachea and bronchioles are trapped by ciliated (hair like cells) and goblet cells which produce sticky mucus. This is gradually shifted upward and you spit or swallow it out. The few particles that get through into your deep lung, or alveoli, are dealt with by macrophage immune cells which essentially eat the particles. On top of this actions such as coughing and sneezing cause a huge expiration which can shift particles too.
If you have ‘healthy’ lungs, most dust that you breathe in, get trapped in the trachea, bronchi & brochioles.  Those parts of the Respiratory system have epithelial glands lining them that produce mucous continually& the cilia present help to sweep that mucous up & into the back of your throat.  Most ‘regular’ dirt & dust particles are handled by your body this way.  That said all ‘dust’ particles aren’t the same. Some types of airborn particles have different morphological shapes, such as asbestos, which embed within the tissue itself & will never be expelled.  Cigarettes, & also marijuana, when smoked & inhaled have plant-based substances that turn into a sticky tar-like substance that covers the cilia & blocking the mucous-generating cells, eventually killing them & causing a chronic inflammatory process that causes permanent scarring of the bronchi & bronchioles.  That is called a ‘Chronic Bronchitis’.  The scarring shrinks as it goes on - as all scars do, & causes narrowing of the airways.  The narrowing causes difficulty breathing, because your body, when healthy, will dilate the bronchi & bronchioles in conditions where you need more oxygen, like running, or if you’re sick.  Emphysema is the result of chronic narrowing of the airways, making the lungs unable to expel the expired carbon dioxide (what you get after breathing in & using up oxygen).  The trapped carbon dioxide dilates the alveoli because of the increased pressure.  The Carbon Dioxide also takes up space in the lungs that should have oxygen in them- rendering those parts of the killings useless.  So, it matters what types of ‘dust’ or other matter you are inhaling into your lungs.  Even campfire smoke is harmful to your lungs, as are cleaning agents & many chemicals used in manufacturing. 
     Your Gastrointestinal (GI) Tract (from your mouth to your rectum) is contained of differentiated specialized epithelial cells.  Your skin is an example of a specialized epithelial tissue, but the GI Tract is not the same.  Your GI tract is specialized to breakdown food, to absorb the food, & then excrete & defecate the waste products.  The GI tract includes the liver, pancreas & the gallbladder, that help to breakdown & regulate what products get absorbed, how much gets absorbed.  Your body, with the help of hormones & other organs, help to make all the substances your body needs, such as more hormones, sugar, insulin, bone, blood, an immune system, & everything else.  Different types of nutrients are absorbed in different parts of the small bowel, in fact, if a person loses a part of their small bowel, they run the risk of having a mal-absorption syndrome, where certain types of food can’t be absorbed.  
   Sorry for the ‘lecture’, but wanted to get some things straight. I don’t want people to be mis-informed.  This may seem long, but obviously this is a health-Science where new findings are being discovered continually, & there are textbooks, classes & specialists in all these areas.  If you have any concerns, it would well be worth the effort to consult with a specialist, rather than to hope to get ‘good’ info from a social website.
[deleted]
[removed]
As folks say, coughing mucus or absorption by macrophages

Why doesn't asbestos work this way then??

https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/full_width/public/thumbnails/image/Crocidolite_UICC2.jpg

Asbestos is a mineral the body cannot chemically break down.  Some of its particle sizes are VERY small and make it into the alveoli where there's not a high pressure and flow rate from a cough to move them out.  

But the real problem is they're an insidious shape.  Well there's 6 distinct types of asbestos shapes.  The worst ones are corkscrews and have little barbs/hooks so they tangle and pierce tissue.  In fact the point can be so tiny yet stiff that it's smaller than a cell and can pierce an individual cell's wall, which is believed to be part of its exceptional carcinogenic potential.
[removed]
Wonder what happens to workers who accidentally breath concrete dust. I imagine once it gets in the moist lungs it does what it is supposed to do, harden up into a solid. Scares me just being around someone mixing up a bag of concrete.
[removed]
[removed]
So I inhaled some small glass beads years ago. Walking with a jar, dropped it, and caught it again but reacted to forcefully and dashed my face while inhaling a surprised gasp. In theory, if some went into deeper portions of my lungs, would they still be there?
Depends on the size of the dust.

At first there is a special layer in your nose and your trachea which collect the bigger parts of the dust. Smaller dust goes down to the alveolar and will be collect by special immune cells makrophages. Smallest dust particles like pm2.5 will go directly into your blood.
[removed]
[removed]
[removed]
